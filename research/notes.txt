E. B. Lee and L. Markus, Foundations of Optimal Control Theory.York, Wiley, 1967.
M. Athans and P. Falb, Optimal Control: An Introduction to the Theory
and Its Applications. New York, McGraw-Hill, 1966.

control grammaian: a square matrix
you can take the eigenvectors of it, sorted by eigenvalue magnitude
the larger the eigenvalue magnitude, the more controllable directions in state-space

biggest eigenvalue eigenvectors, most bang-for-buck u to controllable direction
seems related to minimizing u

the goal is to pull out the states x0 and x1 from c, not to simplify

https://stackoverflow.com/questions/67513799/efficiently-zero-elements-of-numpy-array-using-a-boolean-mask
https://numpy.org/doc/stable/reference/generated/numpy.unique.html
https://www.google.com/search?q=numpy+meshgrid+unique&sxsrf=ALiCzsagUMJsAFWsM7dW8HkyXdRlnwxjpg%3A1657325093441&ei=JcbIYr61GrSJptQPrNassA4&ved=0ahUKEwj-2-KLwer4AhW0hIkEHSwrC-YQ4dUDCA4&uact=5&oq=numpy+meshgrid+unique&gs_lcp=Cgdnd3Mtd2l6EAMyCAgAEB4QDxAWMgUIABCGAzIFCAAQhgM6BwgAEEcQsAM6BAgjECc6BAgAEE
M6BQgAEJECOhEILhCABBCxAxCDARDHARDRAzoLCAAQgAQQsQMQgwE6CAguELEDEIMBOggIABCABBCxAzoKCAAQsQMQgwEQQzoLCAAQsQMQgwEQkQI6EAgAEIAEEIcCELEDEIMBEBQ6CAgAELEDEJECOgoIABCABBCHAhAUOgUIABCABDoGCAAQHhAWSgQIQRgASgQIRhgAUN4HWMEbYJMcaANwAXgAgAGOAYgBjBCSAQUxMC4xMJgBAKABAcgBCMABAQ&sclient=gws-wiz
https://www.google.com/search?q=arctan+vs+arctan2&sxsrf=ALiCzsad93kJ6w5XFyxtPo60xxG2yTygjA:1657325980625&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiCl-iyxOr4AhUoBTQIHX4eCGsQ_AUoAXoECAEQAw&biw=1337&bih=769&dpr=1
https://www.google.com/imgres?imgurl=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2Fa%2Fad%2FAtan2definition.svg%2F1200px-Atan2definition.svg.png&imgrefurl=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAtan2&tbnid=xeznMjlFq0ix0M&vet=12ahUKEwi7gdvBxOr4AhVRGs0KHTClCnwQMygCegUIARDEAQ..i&docid=jkCJ2q9P0pl7TM&w=1200&h=1028&q=arctan%20vs%20arctan2&ved=2ahUKEwi7gdvBxOr4AhVRGs0KHTClCnwQMygCegUIARDEAQ
https://en.wikipedia.org/wiki/Atan2
https://www.google.com/search?q=tan+of+sum&sxsrf=ALiCzsZamNBJvdzxcecMLmDIslLCm8VZLQ:1657326153376&tbm=isch&source=iu&ictx=1&vet=1&fir=IlbDXKKW8F-ywM%252Cr1t2yaerf4UAvM%252C_%253BuBY8SiVJATahIM%252CSb_TidArUMQb2M%252C_%253BM1WpEgpN7-1-5M%252CgOL58tiFiTQpoM%252C_%253BZf4uxxOWZak8BM%252COfnZcIXs1yyzSM%252C_%253BpN4J1p_LbYdeRM%252Cid3NXHI2JXISxM%252C_%253BYEhoghIFr2C0rM%252CGZJrwJC47lYNvM%252C_%253B--KWHNJ8KyeVpM%252CKqCvCr5bI-IZEM%252C_%253B4bqZ4oI75XseGM%252C-WhakXe_xh4TWM%252C_%253BjErIK5_39IIuFM%252CIquR0pfKx-XiVM%252C_%253B3mrPXrFWR_jmmM%252CJ_df47F9xogO4M%252C_%253B56PYcoGrd9PIqM%252CnwgMbwFJ-iVpCM%252C_%253BvymHKhqOz3Ls_M%252C5FFZsWAJhqhaQM%252C_%253BQDdfAd9sd1S2DM%252C6fX_5fSiAQriuM%252C_%253BBBcQqvS06JAJmM%252CL6VEnPMaTyzWDM%252C_&usg=AI4_-kR1pCbUS6gHHTRh4WOM7GXuhrQYYQ&sa=X&ved=2ahUKEwjsmJiFxer4AhXpK0QIHalVCHQQ9QF6BAgFEAE#imgrc=IlbDXKKW8F-ywM
https://stackoverflow.com/questions/59990370/doing-conditional-operations-using-meshgrids-in-numpy-and-merging-them
https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html
https://numpy.org/doc/stable/reference/generated/numpy.select.html
https://numpy.org/doc/stable/reference/generated/numpy.where.html


https://www.overleaf.com/project/62b5c6c39d656f008886d0f0
https://github.com/fwilliams/point-cloud-utils#deduplicating-point-clouds-and-meshes
https://github.com/cvg/pcdmeshing
http://www.open3d.org/docs/release/getting_started.html
https://auth.launchpad.nasa.gov/logout?SAMLRequest=jZJbT4NAEIX%2fCtl3boVS2ACxkTRBq1Zr%2buCLWXYXSgK7yOw29d%2fLRU1MmsbXk3NmzpeZGEjbdHgrK6nVC%2f%2fQHJSRZwl6D8LC94uSmYTxyPSjcmlGQcRMHroud72SBaGLjAPvoZYiQQvLQUYOoHkuQBGhBslZLExnZTrhqxNiP8D%2b0gpW3hsysmFLLYiakkelOsC2TbQ6Wg3Rgh47wixBgFiVPNnN1A0Zt1IAH%2bfqXmBJoAYsSMsBK4r364ctHipgOpuwFtBxWpc1Z8g4t40APJFeT3e9VJLKBqXxhNLP0eshAsD7EQWlI8o3SQnuJRjCSrAH%2f6mmHGzVa1CxPe9K48dhdp4ZG9m35Aqna7mTUjOznKwXaP9dmX4S4d0IRn87xvbcI43n39hzGE%2bcC8bP6f3jcr177vfKOZDzU3fnOjRbF1G%2bKarE8aLduUqS2L4Q%2fBH%2ffFr6BQ%3d%3d&Signature=lfbVo%2fnhMMLnP9n2lFtF0DIwxRzT6nRSLrHGWdvXN53hN%2fm7%2fjP3P%2fBraa6JTEFjMLUU49VyylnfaAw4buaMmftNSS7pFW4ovk3mg2Git00tNmUXS3TfNrVRIj9CCTQcIQF3ZoNV4FSjJrhd14dWl%2bsWjrrZK536nWxPeS70YvCyZR1Jqfo%2be0WBishsQHcWET3blbBerKrTgwSlO15ww4MNkNHi218Vm6DpmkfMVRGryrsp%2bbOI2RVLn6cTrhux%2feHxFND9BFTHx2GesJDzSPT2T2dnq6cOIxGKmPdXm4MdCp2ProwDhAKPAx89lbMiMS0Dwe%2bGn9%2bSP5dD3twCJQ%3d%3d&SigAlg=http%3a%2f%2fwww.w3.org%2f2001%2f04%2fxmldsig-more%23rsa-sha256&client-request-id=c2e92ade-2e83-4fe0-6f1a-008006000015
https://realpython.com/python-virtual-environments-a-primer/
https://docs.rstudio.com/resources/install-python-source/
https://pip.pypa.io/en/stable/installation/
https://stackoverflow.com/questions/1534210/use-different-python-version-with-virtualenv
https://stackoverflow.com/questions/49478573/pip3-install-not-working-no-module-named-pip-vendor-pkg-resources
https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html
https://teams.microsoft.com/_#/conversations/19:9834c96b9c0246b4a37e3e56475a8a55@thread.v2?ctx=chat
https://trunk.arc.nasa.gov/bitbucket/projects/VIPERGDS/repos/viper_verve/pull-requests/353/diff#bundles/gov.nasa.arc.verve.robot.ros.viper/src/gov/nasa/arc/verve/robot/ros/viper/commandmonitor/commands/MonitoredStereoTrigger.java
https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html
https://stackoverflow.com/questions/18608349/updated-apply-vectorized-function-on-each-cell-to-interpolate-grid
https://realpython.com/python-lambda/
https://wellsr.com/python/3-ways-to-calculate-python-execution-time/#:~:text=The%20first%20method%20to%20find,returns%20the%20current%20system%20time.
https://www.google.com/search?q=numpmy+vectorize&sxsrf=ALiCzsZEZ1gHVimE6W7FwdfzljglW4IxfA%3A1657296882814&ei=8lfIYrybMYGIptQP5vyciAM&ved=0ahUKEwi83PL_1-n4AhUBhIkEHWY-BzEQ4dUDCA4&uact=5&oq=numpmy+vectorize&gs_lcp=Cgdnd3Mtd2l6EAMyBwgjELACECcyBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA0yBAgAEA06BwgAEEcQsAM6BAgjECc6BAgAEEM6BQgAEJECOhEILhCABBCxAxCDARDHARDRAzoLCAAQgAQQsQMQgwE6BwguENQCEEM6CggAELEDEIMBEEM6EAgAEIAEEIcCELEDEIMBEBQ6BwgAELEDEAo6BAgAEAo6BAguEEM6CggAELEDEIMBEAo6CggAELEDEIMBEA06BggAEB4QDUoECEEYAEoECEYYAFDJCliNIGDvIWgEcAF4AIABoAGIAeMPkgEEMi4xNZgBAKABAcgBA8ABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/70327194/run-time-optimization-in-python-for-loop-and-meshgrid-with-numpy
https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html
https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/
https://www.youtube.com/watch?v=KOlQWNGAAbM
https://stackoverflow.com/questions/57640827/best-way-of-combining-meshgrid-with-matrix-multiplication-in-function
https://numpy.org/doc/stable/reference/generated/numpy.einsum.html
https://stackoverflow.com/questions/49776481/use-meshgrid-with-two-different-vectors
https://stackoverflow.com/questions/3337301/numpy-matrix-to-array
https://www.google.com/search?q=np+horzcat&oq=np+horzcat&aqs=chrome..69i57.2843j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/50440837/color-lookup-with-a-matplotlib-colormap
https://pyqtgraph.readthedocs.io/en/latest/3dgraphics/glscatterplotitem.html
https://pyqtgraph.readthedocs.io/en/latest/3dgraphics/gllineplotitem.html
https://matplotlib.org/stable/tutorials/colors/colormaps.html
https://docs.google.com/spreadsheets/d/1MG4PqBMiFMIlG9CeMuyddmCjwkTpC23EEIAEkpXbL8A/edit#gid=2038720634
https://stackoverflow.com/questions/40423999/pyqtgraph-where-to-find-signal-for-key-preses
https://outlook.office365.com/mail/inbox/id/AAQkADAyZDgxZDI4LTg1ZDctNDEyYi1hMzA3LTZhMDg3Y2IxZjE1ZgAQAPVXYPO79EZHl%2Fs0ch3RSoQ%3D
https://www.google.com/search?q=GLViewWidget+keypress+pyqtgraph&sxsrf=ALiCzsZaoK1e2Jipihm41c_pqqHUvbV2xA%3A1657310744258&ei=GI7IYu2uD_OhkPIPrcuY2A8&ved=0ahUKEwjtmsbRi-r4AhXzEEQIHa0lBvsQ4dUDCA4&uact=5&oq=GLViewWidget+keypress+pyqtgraph&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABOgcIABBHELADOgUIIRCrAkoECEEYAEoECEYYAFDPA1iKD2CtEGgBcAF4AIAB0QGIAZMOkgEFMC40LjWYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://www.google.com/search?q=glscatterPlotItem+hide&oq=glscatterPlotItem+hide&aqs=chrome..69i57.2106j0j7&sourceid=chrome&ie=UTF-8
https://calendar.google.com/calendar/u/0/r/week?pli=1
https://calendar.google.com/calendar/u/1/r

https://stackoverflow.com/questions/46257759/spherical-coordinates-plot-in-pyqtgraph
https://github.com/LucasF27/pyQtgraph_test
https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html
https://stackoverflow.com/questions/38698277/plot-normal-distribution-in-3d
https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html
https://en.wikipedia.org/wiki/Multivariate_normal_distribution
https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html
https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html
https://stackoverflow.com/questions/45058690/when-using-scipy-stats-multivariate-normal-pdf-having-the-erroroperands-could-n
https://docs.scipy.org/doc//scipy-1.4.1/reference/generated/scipy.stats.multivariate_normal.html
https://stackoverflow.com/questions/29042182/creating-a-4d-matrix-full-of-zeros-in-python-numpy
https://www.google.com/search?q=multivariate_normal+not+yet+returned&oq=multivariate_normal+not+yet+returned&aqs=chrome..69i57.2065j0j7&sourceid=chrome&ie=UTF-8
https://www.tutorialspoint.com/matplotlib/matplotlib_3d_contour_plot.htm
https://numpy.org/doc/stable/reference/generated/numpy.sum.html
https://numpy.org/doc/stable/reference/generated/numpy.sum.html
https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib
https://stackoverflow.com/questions/51645694/how-to-plot-a-perfectly-smooth-sphere-in-python-using-matplotlib
https://www.esp8266.com/viewtopic.php?p=77659
https://www.askpython.com/python/normal-distribution
https://www.geeksforgeeks.org/visualizing-the-bivariate-gaussian-distribution-in-python/
https://stackoverflow.com/questions/63564845/contour-plots-for-multivariate-gaussian
https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html
https://github.com/search?l=Python&q=GLSurfacePlotItem&type=Code
https://stackoverflow.com/questions/61076696/pyqtgraph-opengl-how-to-create-a-sphere-between-two-coordinates
https://www.google.com/search?q=python+normal+distribution&oq=python+normal+distribution&aqs=chrome..69i57j69i60l3.3266j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/20228546/how-do-i-apply-some-function-to-a-python-meshgrid
https://pyqtgraph.readthedocs.io/en/latest/colormap.html
https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object
https://www.google.com/search?q=pyqtgraph+slider+scatter+plot&oq=pyqtgraph+slider+scatter+plot&aqs=chrome..69i57.4859j0j7&sourceid=chrome&ie=UTF-8
https://github.com/sioan/softXRayDataAndControls/tree/a1ab1ecd86cdc55478bdbb1b4b5fbc861f3eae0b/myAnalysisTools/pyqtgraphExamples
https://github.com/cyan-at/gradschool/blob/main/231/research/distribution0.py
https://stackoverflow.com/questions/15415455/plotting-probability-density-function-by-sample-with-matplotlib
https://stackoverflow.com/questions/38082936/best-way-to-pass-repeated-parameter-to-a-numpy-vectorized-function
https://www.mail-archive.com/pyqtgraph@googlegroups.com/msg00520.html
https://gitlab.tue.mpg.de/elacosse/vviewer/-/blob/d37275d6fac3d1e15f678a74a6d27e2f75f0e71d/vviewer/pyqtgraph/colormap.py
https://pyqtgraph.readthedocs.io/en/pyqtgraph-0.12.1/colormap.html
https://www.programcreek.com/python/example/94476/pyqtgraph.ColorMap
https://doc.qt.io/qtforpython-5/PySide2/QtGui/QColor.html#PySide2.QtGui.PySide2.QtGui.QColor.getRgbF


show that F matrix is non-singular (fresnels) are non-singular
what does c look like in matrix form for A2 != I?

###############################################

loss plotting:
[0] epoch
[0] y1, psi, hjb
[1] y2, rho, plank pde
[2] rho0, initial
[3] rhoT, terminal

###############################################

control policy
v1, v2, v3 => u = B^(-1)v
v = gradient of psi

test:
x1, x2, x3, t, y1 (psi), y2 (rho)

dy1 / x1
dy1 / x2
dy1 / x3

import tensorflow as tf
tf.compat.v1.disable_eager_execution()

test = np.genfromtxt('test.dat')
psi = test[:, 4]
x1 = test[:, 0]
x2 = test[:, 1]
x3 = test[:, 2]
tf.gradients(psi, x1)


v2 = tf.gradients(psi, x2)
v3 = tf.gradients(psi, x3)

###############################################

https://www.tensorflow.org/api_docs/python/tf/gradients
https://stackoverflow.com/questions/41822308/how-tf-gradients-work-in-tensorflow
https://www.tensorflow.org/api_docs/python/tf/gradients
https://stackoverflow.com/questions/65950732/tf-tape-gradient-returns-none-for-my-numerical-function-model
https://www.tensorflow.org/guide/autodiff#getting_a_gradient_of_none
https://www.geeksforgeeks.org/how-to-convert-numpy-array-to-tensor/
https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow
https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy
https://www.google.com/search?q=tensorflow+InternalError%3A+Attempting+to+perform+BLAS+operation+using+StreamExecutor+without+BLAS+support+%5BOp%3AMatMul%5D&sxsrf=ALiCzsamTmVRqpWIRDeqRPhZUwPmsJ1YRQ%3A1660858274079&ei=oq_-Yp6sBPuuptQP5Pqx4A4&ved=0ahUKEwiehcubq9H5AhV7l4kEHWR9DOwQ4dUDCA4&uact=5&oq=tensorflow+InternalError%3A+Attempting+to+perform+BLAS+operation+using+StreamExecutor+without+BLAS+support+%5BOp%3AMatMul%5D&gs_lcp=Cgdnd3Mtd2l6EANKBAhBGABKBAhGGABQAFjgDWDZDmgAcAB4AIABAIgBAJIBAJgBAKABAcABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/35226428/how-do-i-get-the-gradient-of-the-loss-at-a-tensorflow-variable
https://numpy.org/doc/stable/reference/generated/numpy.gradient.html
http://localhost:8888/notebooks/optimality_conditions-Copy1.ipynb
http://localhost:8889/notebooks/EulerOMT_plots.ipynb
https://www.tensorflow.org/guide/variable
https://towardsdatascience.com/mastering-tensorflow-variables-in-5-easy-step-5ba8062a1756
https://www.tensorflow.org/api_docs/python/tf/Tensor

https://www.geeksforgeeks.org/how-to-convert-numpy-array-to-tensor/
https://www.tensorflow.org/guide/autodiff#getting_a_gradient_of_none
https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow
https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy
https://www.google.com/search?q=tensorflow+InternalError%3A+Attempting+to+perform+BLAS+operation+using+StreamExecutor+without+BLAS+support+%5BOp%3AMatMul%5D&sxsrf=ALiCzsamTmVRqpWIRDeqRPhZUwPmsJ1YRQ%3A1660858274079&ei=oq_-Yp6sBPuuptQP5Pqx4A4&ved=0ahUKEwiehcubq9H5AhV7l4kEHWR9DOwQ4dUDCA4&uact=5&oq=tensorflow+InternalError%3A+Attempting+to+perform+BLAS+operation+using+StreamExecutor+without+BLAS+support+%5BOp%3AMatMul%5D&gs_lcp=Cgdnd3Mtd2l6EANKBAhBGABKBAhGGABQAFjgDWDZDmgAcAB4AIABAIgBAJIBAJgBAKABAcABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/35226428/how-do-i-get-the-gradient-of-the-loss-at-a-tensorflow-variable
https://numpy.org/doc/stable/reference/generated/numpy.gradient.html
https://www.tensorflow.org/guide/variable
https://towardsdatascience.com/mastering-tensorflow-variables-in-5-easy-step-5ba8062a1756
https://www.tensorflow.org/api_docs/python/tf/Tensor

https://numpy.org/doc/stable/reference/generated/numpy.hstack.html
http://localhost:8889/tree
http://localhost:8889/notebooks/EulerOMT_plots.ipynb
http://localhost:8889/notebooks/EulerOMT_plots.ipynb
https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html
https://stackoverflow.com/questions/56099302/how-to-zero-out-all-values-of-matrix-2d-array-except-top-n-values-using-numpy
https://www.google.com/search?q=scipy+griddata+3d&sxsrf=ALiCzsYbvhqFKmTZsI4hWGqaifx5E1Um0w%3A1661205372352&ei=fPsDY8qMFaTckPIP186H6AM&ved=0ahUKEwjKuviguNv5AhUkLkQIHVfnAT0Q4dUDCA4&uact=5&oq=scipy+griddata+3d&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QBzIFCAAQhgMyBQgAEIYDMgUIABCGAzoHCCMQsAIQJzoGCAAQHhANOggIABAeEA0QBUoECEEYAEoECEYYAFAAWOoFYK4HaABwAHgAgAGkAYgByAaSAQMwLjaYAQCgAQHAAQE&sclient=gws-wiz
https://stackoverflow.com/questions/47235152/how-to-interpolate-3d-using-pythons-griddata
https://stackoverflow.com/questions/29037424/using-numpy-trapz-to-calculate-a-pdf
https://www.google.com/search?q=numpy+trapz+normalize&oq=numpy+trapz+normalize&aqs=chrome..69i57.2635j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/11615664/multivariate-normal-density-in-python
https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html
https://stackoverflow.com/questions/22071987/generate-random-array-of-floats-between-a-range
https://stackoverflow.com/questions/12402045/mesh-grid-functions-in-python-meshgrid-mgrid-ogrid-ndgrid
https://numpy.org/doc/stable/reference/generated/numpy.ogrid.html
https://anaconda.org/anaconda/pyqtgraph
https://anaconda.org/conda-forge/pyopengl
https://anaconda.org/fastchan/qt

https://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-column
https://stackoverflow.com/questions/60724146/after-upgraded-tensorflow2-1-i-got-runtimeerror-tf-placeholder-is-not-compat
https://stackoverflow.com/questions/72301866/how-to-replace-tf-placeholder-in-eager-execution
https://trunk.arc.nasa.gov/jira/browse/VIPERMOS-879
https://deepxde.readthedocs.io/en/latest/user/installation.html
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html
https://numpy.org/doc/stable/reference/generated/numpy.hstack.html
http://localhost:8889/tree
http://localhost:8889/notebooks/EulerOMT_plots.ipynb
https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html
https://stackoverflow.com/questions/56099302/how-to-zero-out-all-values-of-matrix-2d-array-except-top-n-values-using-numpy
https://www.google.com/search?q=scipy+griddata+3d&sxsrf=ALiCzsYbvhqFKmTZsI4hWGqaifx5E1Um0w%3A1661205372352&ei=fPsDY8qMFaTckPIP186H6AM&ved=0ahUKEwjKuviguNv5AhUkLkQIHVfnAT0Q4dUDCA4&uact=5&oq=scipy+griddata+3d&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QBzIFCAAQhgMyBQgAEIYDMgUIABCGAzoHCCMQsAIQJzoGCAAQHhANOggIABAeEA0QBUoECEEYAEoECEYYAFAAWOoFYK4HaABwAHgAgAGkAYgByAaSAQMwLjaYAQCgAQHAAQE&sclient=gws-wiz
https://stackoverflow.com/questions/47235152/how-to-interpolate-3d-using-pythons-griddata
https://stackoverflow.com/questions/29037424/using-numpy-trapz-to-calculate-a-pdf
https://www.google.com/search?q=numpy+trapz+normalize&oq=numpy+trapz+normalize&aqs=chrome..69i57.2635j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/11615664/multivariate-normal-density-in-python
https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html
https://stackoverflow.com/questions/22071987/generate-random-array-of-floats-between-a-range
https://stackoverflow.com/questions/12402045/mesh-grid-functions-in-python-meshgrid-mgrid-ogrid-ndgrid
https://numpy.org/doc/stable/reference/generated/numpy.ogrid.html
https://anaconda.org/anaconda/pyqtgraph
https://anaconda.org/conda-forge/pyopengl
https://anaconda.org/fastchan/qt
https://stackoverflow.com/questions/23359886/selecting-rows-in-numpy-ndarray-based-on-the-value-of-two-columns
https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions
https://stackoverflow.com/questions/26545051/is-there-a-way-to-delete-created-variables-functions-etc-from-the-memory-of-th
https://online-voice-recorder.com/
https://www.google.com/search?q=numpy+gradient+vs+tensorflow+gradient&oq=numpy+gradient+vs+tensorflow+gradient&aqs=chrome..69i57.5348j0j7&sourceid=chrome&ie=UTF-8
https://www.tensorflow.org/guide/autodiff
https://www.toptal.com/python/gradient-descent-in-tensorflow
https://blog.finxter.com/np-gradient/
https://trvrm.github.io/using-tensorflow-to-compute-gradients.html
https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html
https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html
https://www.google.com/search?q=tensorflow+take+gradient+model&sxsrf=ALiCzsZMgrwVnLqmJyGc_3QvitFPOyG8ig%3A1661283985084&ei=kS4FY7bkBJuoptQPxrO7sAI&ved=0ahUKEwj28LSO3d35AhUblIkEHcbZDiYQ4dUDCA8&uact=5&oq=tensorflow+take+gradient+model&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsANKBAhBGABKBAhGGABQzSVY0Chg2yloA3AAeAGAAaUCiAGDBpIBAzItM5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/35226428/how-do-i-get-the-gradient-of-the-loss-at-a-tensorflow-variable
https://stackoverflow.com/questions/59145221/how-to-compute-gradient-of-output-wrt-input-in-tensorflow-2-0
https://www.google.com/search?q=deepxde+save+model&sxsrf=ALiCzsay03iuHedMEG3e_h937a4e2VK-PA%3A1661284123016&ei=Gy8FY8pE0oim1A-a3ongBQ&ved=0ahUKEwiKvpfQ3d35AhVShIkEHRpvAlwQ4dUDCA8&uact=5&oq=deepxde+save+model&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsAM6BwgAELADEEM6BggAEB4QBzoICAAQHhAIEAc6CAgAEB4QBxAFOgYIABAeEAg6BQgAEIYDOggIABAeEAgQDUoECEEYAEoECEYYAFCQA1iGFGCNGmgCcAF4AIAB_AGIAeEMkgEDMi03mAEAoAEByAEKwAEB&sclient=gws-wiz
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html#module-deepxde.model
https://www.google.com/search?q=deepxde+%27numpy.ndarray%27+object+has+no+attribute+%27_id%27&oq=deepxde+%27numpy.ndarray%27+object+has+no+attribute+%27_id%27&aqs=chrome..69i57.1878j0j7&sourceid=chrome&ie=UTF-8
https://fixexception.com/tensorflow/numpy-is-only-available-when-eager-execution-is-enabled/
https://www.google.com/search?q=deepxde+gradient&oq=deepxde+gradient&aqs=chrome..69i57.2760j0j7&sourceid=chrome&ie=UTF-8
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html#module-deepxde.model
https://www.google.com/search?q=deepxde.gradients.jacobian&oq=deepxde.gradients.jacobian&aqs=chrome..69i57.192j0j7&sourceid=chrome&ie=UTF-8
https://github.com/lululxvi/deepxde/issues/377
https://github.com/tensorflow/tensorflow/issues/18304


##############################################################

q_statepenalty_gain = 0.5, T_t = 200, 2.0 -> 0.0

nonaxissym_4/
axissymmetric_6/

q_statepenalty_gain = 0.5, T_t = 5.0, 2.0 -> 0.0

run_-2.000_2.000__5.000__2000__0.001__1_1_2__0.500__2.000_1.000__0.000_1.000__12000_1000__50000/
run_-2.000_2.000__5.000__2000__0.001__3_2_1__0.500__2.000_1.000__0.000_1.000__12000_1000__50000/

q_statepenalty_gain = 0.5, T_t = 5.0, 5.0 -> 0.0

run_-2.000_2.000__5.000__2000__0.001__3_2_1__0.500__5.000_1.000__0.000_1.000__12000_1000__50000
run_-2.000_2.000__5.000__2000__0.001__1_1_2__0.500__5.000_1.000__0.000_1.000__12000_1000__50000

##############################################################

non-axissymmetric uncontrolled pdf:

then validate axissymmetric specific case on above code


########################################################################

(1) training error/residual plots (see attached),
(2) optimally controlled univariate state marginals superimposed with
optimally controlled (i.e., closed loop) sample paths as three
subfigures of the same figure
(3) optimal control tau1, tau2, tau3 versus time sample paths as three
subfigures of the same figure

run_-2.000_2.000__5.000__2000__0.001__1_1_2__0.500__2.000_1.000__0.000_1.000__12000_1000__50000/

16k:

time=2k
initial=1k
samples_between_initial_and_final=12k
final=1k

########################################################################


i've been training with tensorflow v1

using tensorflow v2 fails because of libdevice error???
but now it runs
  maybe this is why:  %env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda
but is very, very slow? 2 epoch / minute

maybe this is why v1 is faster: 
Enable just-in-time compilation with XLA.


@tf.function(jit_compile=config.xla_jit)
def train_step(inputs, targets, auxiliary_vars):
    # inputs and targets are np.ndarray and automatically converted to Tensor.
    with tf.GradientTape() as tape:
        losses = outputs_losses_train(inputs, targets, auxiliary_vars)[1]
        total_loss = tf.math.reduce_sum(losses)

self.train_state.y_pred_test, self.train_state.loss_test = self._outputs_losses(
    False,
    self.train_state.X_test,
    self.train_state.y_test,
    self.train_state.test_aux_vars,
)

#########################################################

2022-08-24 17:37:21.671109: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x562d3ec2f240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-08-24 17:37:21.671135: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA RTX A2000 Laptop GPU, Compute Capability 8.6
2022-08-24 17:37:21.702060: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-08-24 17:37:22.296729: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-08-24 17:37:22.297851: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2022-08-24 17:37:22.297860: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas
2022-08-24 17:37:22.297914: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-08-24 17:37:22.302210: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-08-24 17:37:22.303268: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:640] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-08-24 17:37:24.311800: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-08-24 17:37:24.314325: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Step      Train loss                                  Test loss                                   Test metric
0         [1.33e+01, 1.20e-01, 1.47e-01, 1.00e+00]    [1.33e+01, 1.20e-01, 1.47e-01, 1.00e+00]    []  
2022-08-24 17:37:25.662765: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-08-24 17:37:36.285751: E tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instrution is taking > 1s:

  %dot.51 = f32[128000,70]{1,0} dot(f32[128000,70]{1,0} %constant.263, f32[70,70]{1,0} %constant.395), lhs_contracting_dims={1}, rhs_contracting_dims={1}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-24 17:37:43.947684: E tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instrution is taking > 2s:

  %dot.17 = f32[128000,70]{1,0} dot(f32[128000,70]{1,0} %constant.363, f32[70,70]{1,0} %constant.321), lhs_contracting_dims={1}, rhs_contracting_dims={1}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.

#########################################################

https://deepxde.readthedocs.io/en/latest/_modules/deepxde/gradients.html#jacobian
https://github.com/lululxvi/deepxde/blob/master/deepxde/backend/set_default_backend.py
https://www.google.com/search?q=tensorflow+model+gradient&sxsrf=ALiCzsa3aiffvv-oWa4Aqr80xM00LUJvNA%3A1661310552985&ei=WJYFY4LZO4qr0PEPjbSPsAU&ved=0ahUKEwiC5fyKwN75AhWKFTQIHQ3aA1YQ4dUDCA8&uact=5&oq=tensorflow+model+gradient&gs_lcp=Cgdnd3Mtd2l6EAMyCAgAEB4QCBAHMgQIABAeMgYIABAeEAgyBQgAEIYDMgUIABCGAzoHCAAQRxCwAzoGCAAQHhAHOgoIABAeEAgQBxAKOgUIABCABDoICAAQHhAIEAo6BAgAEA1KBAhBGABKBAhGGABQ_hJYvTVg3TZoBnABeACAAXOIAfIJkgEDOS40mAEAoAEByAEIwAEB&sclient=gws-wiz
https://www.tensorflow.org/api_docs/python/tf/test/compute_gradient
https://www.tensorflow.org/guide/autodiff
https://www.google.com/search?q=%22deepxde%22+WARNING%3Atensorflow%3AGradients+do+not+exist+for+variables+%5B%27Variable%3A0%27%5D+when+minimizing+the+loss.&sxsrf=ALiCzsZ_-SNy5zTNdoLSB4FArclt9RkzsQ%3A1661311385340&ei=mZkFY-a6FOuaptQPhcOKmAc&ved=0ahUKEwim6--Xw975AhVrjYkEHYWhAnMQ4dUDCA8&uact=5&oq=%22deepxde%22+WARNING%3Atensorflow%3AGradients+do+not+exist+for+variables+%5B%27Variable%3A0%27%5D+when+minimizing+the+loss.&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsANKBAhBGABKBAhGGABQmQtY1RxguR5oAnABeACAAVSIAfIBkgEBM5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/68614547/tensorflow-libdevice-not-found-why-is-it-not-found-in-the-searched-path
https://stackoverflow.com/questions/68614547/tensorflow-libdevice-not-found-why-is-it-not-found-in-the-searched-path
https://stackoverflow.com/questions/62670495/using-xla-in-tensorflow-libdevice-10-bc-internaler
https://towardsdatascience.com/accelerate-your-training-and-inference-running-on-tensorflow-896aa963aa70
https://www.google.com/search?q=deepxde+train+model&oq=deepxde+train+model&aqs=chrome..69i57.3338j0j7&sourceid=chrome&ie=UTF-8
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html#Model.predict
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/callbacks.html#Callback.on_epoch_end
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html#Model.predict
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/callbacks.html#CallbackList.on_epoch_end
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/callbacks.html#EarlyStopping
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/callbacks.html#Callback.on_epoch_end
https://deepxde.readthedocs.io/en/latest/modules/deepxde.data.html?highlight=TimePDE#deepxde.data.pde.TimePDE
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/data/pde.html#TimePDE
https://github.com/lululxvi/deepxde/issues/377
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html
http://fischerp.cs.illinois.edu/tam470/refs/notes_time1c.pdf
https://www.tensorflow.org/api_docs/python/tf/compat/v1/gradients
https://www.tensorflow.org/api_docs/python/tf/compat/v1
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/gradients.html#jacobian
https://github.com/lululxvi/deepxde/blob/master/examples/function/dataset.py
https://www.google.com/search?sxsrf=ALiCzsZIowiN3nCUm_PU4xQRBcmMbn14Zw:1661318728876&q=%22conda%22+tensorflow+INTERNAL:+libdevice+not+found+at+./libdevice.10.bc&sa=X&ved=2ahUKEwim1sXF3t75AhV_jIkEHU-lCFoQ7xYoAHoECAEQNg&biw=1247&bih=939&dpr=1
https://www.google.com/search?q=jupyter+XLA_FLAGS%3D--xla_gpu_cuda_data_dir%3D%2Fpath%2Fto%2Fcuda&sxsrf=ALiCzsZQIaynGWofJcF6yzMx9gzvT2tgLw%3A1661319292367&ei=fLgFY8OMFt6IptQP4pOR-AU&ved=0ahUKEwjDup7S4N75AhVehIkEHeJJBF8Q4dUDCA4&uact=5&oq=jupyter+XLA_FLAGS%3D--xla_gpu_cuda_data_dir%3D%2Fpath%2Fto%2Fcuda&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEKIEMgUIABCiBDIFCAAQogRKBAhBGABKBAhGGABQAFicEGD5EWgAcAB4AIABfYgBgQeSAQMxLjeYAQCgAQHAAQE&sclient=gws-wiz
https://github.com/google/jax/issues/989
https://stackoverflow.com/questions/72107119/how-to-use-system-gpu-in-jupyter-notebook
https://www.pythonfixing.com/2022/05/fixed-tensorflow-libdevice-not-found.html
https://stackoverflow.com/questions/37890898/how-to-set-env-variable-in-jupyter-notebook
https://www.tensorflow.org/guide/advanced_autodiff
https://stackoverflow.com/questions/59590766/how-do-i-get-the-gradient-of-a-keras-model-with-respect-to-its-inputs
https://stackoverflow.com/questions/55066710/computing-gradients-wrt-model-inputs-in-tensorflow-eager-mode
https://stackoverflow.com/questions/57529831/solved-how-to-combine-tf-gradients-with-tf-data-dataset-and-keras-models
https://stackoverflow.com/questions/62325764/calculating-the-derivates-of-the-output-with-respect-to-input-for-a-give-time-st
https://www.tensorflow.org/api_docs/python/tf/GradientTape
https://www.tutorialexample.com/fix-tf-gradienttape-attributeerror-refvariable-object-has-no-attribute-_id-error-tensorflow-tutorial/
https://stackoverflow.com/questions/51440135/tensorflow-stop-training-when-losses-reach-a-defined-value
https://www.tensorflow.org/api_docs/python/tf/compat/v1/gradients
https://stackoverflow.com/questions/58888215/sess-run-and-eval-in-tensorflow-programming
https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/gradients.html

https://www.google.com/search?q=tf+v1+session+gradient&oq=tf+v1+session+gradient&aqs=chrome..69i57.4203j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=tensorflow+train+with+gradienttape&oq=tensorflow+train+with+gradienttape&aqs=chrome..69i57.11482j0j4&sourceid=chrome&ie=UTF-8
https://pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/
https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch
https://deepxde.readthedocs.io/en/latest/search.html?q=speed&check_keywords=yes&area=default#
https://deepxde.readthedocs.io/en/latest/user/installation.html?highlight=speed#which-backend-should-i-choose
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html?highlight=compile#deepxde.model.Model.compile
https://www.google.com/search?q=or+modifying+%24PATH+can+be+used+to+set+the+location+of+ptxas&sxsrf=ALiCzsarY71FmutQi-_Qvl4TsJp1IQ_6IA%3A1661379568515&ei=8KMGY4yPH-bZkPIPqauo0Ao&ved=0ahUKEwiMx5KYweD5AhXmLEQIHakVCqoQ4dUDCA4&uact=5&oq=or+modifying+%24PATH+can+be+used+to+set+the+location+of+ptxas&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEKIEOgcIABBHELADSgQIQRgASgQIRhgAUNEDWNEDYL0KaANwAXgAgAGqAYgBqgGSAQMwLjGYAQCgAQKgAQHIAQjAAQE&sclient=gws-wiz
https://stackoverflow.com/questions/46064433/cuda-home-path-for-tensorflow
https://groups.google.com/a/tensorflow.org/g/testing/c/4aPvoX21D9g
https://stackoverflow.com/questions/53429896/how-do-i-disable-tensorflows-eager-execution

https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
https://majianglin2003.medium.com/pytorch-gradients-8ed224a6b54d
https://discuss.pytorch.org/t/gradient-between-two-different-tensor/68410/16

https://en.wikipedia.org/wiki/Power_iteration
https://www.google.com/search?q=power+iteration+geometry&tbm=isch&ved=2ahUKEwjXuYfR5MH5AhWKADQIHdlbAqIQ2-cCegQIABAA#imgrc=IcPPr_-27Gj8sM
https://en.wikipedia.org/wiki/Quadratic_form
https://www.overleaf.com/project/62b5c6c39d656f008886d0f0
https://www.google.com/search?q=proximal+recursion+github&sxsrf=ALiCzsY8MkWmpDXb7hk2wcXXlrv86WYdZA%3A1660337310782&ei=nrz2Yu-oL_GaptQPzcC5sA4&ved=0ahUKEwjv7_a8lsL5AhVxjYkEHU1gDuYQ4dUDCA4&uact=5&oq=proximal+recursion+github&gs_lcp=Cgdnd3Mtd2l6EANKBAhBGAFKBAhGGABQnwVY0gpgogtoAXAAeAGAAfUBiAGPBZIBBTUuMC4xmAEAoAEBwAEB&sclient=gws-wiz

#########################################################

# # gradients = tape.gradient(tf.convert_to_tensor(preds), inp_tensor)

# # print(gradients)

# print("")
# print("test output")
# print(test_timesorted[:, 4:6])

# print("")
# print("model.train_state.X_test")
# x_test_sorted = model.train_state.X_test[model.train_state.X_test[:, 3].argsort()]
# print(x_test_sorted, x_test_sorted.shape)

# tf.gradients(preds, inp)

# inp_tf = tf.Variable(inp, dtype=tf.float32)
# preds_tf1 = model.predict(inp_tf)
# print(preds_tf1)

# with tf.GradientTape() as tape:
#     preds_tf2 = model.predict(inp_tf)
    
# print(preds_tf2)

# grads = tape.gradient(preds_tf2, inp_tf)
# print(grads)

# psi_tf = tf.Variable(test_timesorted[:, 0:2], dtype=tf.float32)
# x1_x2_x3_tf = tf.Variable(test_timesorted[:, 0:3], dtype=tf.float32)

# gradient = dde.gradients.jacobian(preds_tf, inp_tf)
# print(gradient)

https://stackoverflow.com/questions/59922755/how-to-compute-the-gradient-of-only-one-output-unit

#########################################################

2022-08-29 12:26:54.086402: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:640] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas



https://numpy.org/doc/stable/reference/generated/numpy.ediff1d.html
https://www.fast.ai/2022/08/25/jupyter-git/
https://www.mathworks.com/help/matlab/ref/gradient.html#bvhp8_m
https://www.google.com/search?q=speeding+up+tensorflow+v2+training+%22xla%22&sxsrf=ALiCzsZPE7uBuYPGdKjc6NBpoWriCdAj4A%3A1661654949458&ei=pdcKY7vSG8KL0PEPm5WsqA8&ved=0ahUKEwj7uIGIw-j5AhXCBTQIHZsKC_UQ4dUDCA4&uact=5&oq=speeding+up+tensorflow+v2+training+%22xla%22&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgATIFCCEQoAEyBQghEKABMgUIIRCrAjIFCCEQqwI6BwgAEEcQsAM6CAghEB4QFhAdSgQIQRgASgQIRhgAUJ0JWJkQYL8RaAJwAXgAgAGQAYgB5ASSAQMwLjWYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://www.sicara.fr/blog-technique/tensorflow-tutorial-training-time#:~:text=To%20optimize%20training%20speed%2C%20you,are%20smarter%20tools%20out%20there.
https://www.kdnuggets.com/2020/03/tensorflow-optimizing-training-time-performance.html
https://numpy.org/doc/stable/reference/generated/numpy.gradient.html
https://github.com/ami-iit/lie-group-controllers
https://medium.com/sicara/tensorflow-2-0-tutorial-optimizing-training-time-performance-ba9418a8c288
https://blog.seeso.io/a-simple-guide-to-speed-up-your-training-in-tensorflow-2-8386e6411be4
https://deepxde.readthedocs.io/en/latest/search.html?q=config&check_keywords=yes&area=default#
https://deepxde.readthedocs.io/en/latest/user/installation.html?highlight=configure
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html?highlight=config#module-deepxde.config
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html?highlight=config#deepxde.config.default_float
https://www.google.com/maps/place/%E5%A4%A7%E6%9D%89%E8%B0%B7/@34.2110449,136.1507057,15z/data=!3m1!4b1!4m5!3m4!1s0x600683406724a5fb:0x7202ad7a09b76b5f!8m2!3d34.2110455!4d136.1507057?hl=en
https://github.com/ainfosec/FISSURE
https://pianochord.io/chord/A-flat/Ab-lydian
https://www.scientific-ml.com/software
https://github.com/pettni/smooth
https://pettni.github.io/smooth/
https://www.cs.cmu.edu/~kmcrane/Projects/LieGroupIntegrators/paper.pdf
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.5190&rep=rep1&type=pdf
https://drum.lib.umd.edu/bitstream/handle/1903/5567/PhD_94-9.pdf?sequence=1&isAllowed=y
https://stackoverflow.com/questions/59922755/how-to-compute-the-gradient-of-only-one-output-unit
https://www.google.com/search?q=tensorflow+gradient+vs+numerical+gradient&biw=1290&bih=741&sxsrf=ALiCzsbOaIyW9vM2m8fKD3FUUlizBbpWHA%3A1661649533475&ei=fcIKY-_HHLmGptQPwOWB4Ag&ved=0ahUKEwiv-rvxruj5AhU5g4kEHcByAIwQ4dUDCA4&uact=5&oq=tensorflow+gradient+vs+numerical+gradient&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEKIEMgUIABCiBDoHCAAQRxCwA0oECEEYAEoECEYYAFCVCVieEWD4EmgCcAF4AIABdIgB_waSAQM1LjSYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1
https://arxiv.org/pdf/2007.11898.pdf
https://github.com/UZ-SLAMLab/ORB_SLAM3
https://arxiv.org/pdf/1502.00956.pdf
https://www.researchgate.net/figure/Semantic-understanding-allows-humans-to-predict-changes-in-the-environment-at-different_fig4_304163768
https://www.researchgate.net/figure/The-front-end-and-back-end-of-the-SLAM-process_fig4_263057565
https://vnav.mit.edu/material/23-SLAM1-formulationsAndSparsity-notes.pdf
https://stackoverflow.com/questions/39870642/matplotlib-how-to-plot-a-high-resolution-graph
https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html
https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html
https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html
https://deepxde.readthedocs.io/en/latest/demos/pinn_forward/eulerbeam.html?highlight=.jacobian#implementation
https://stackoverflow.com/questions/56072634/tf-2-0-runtimeerror-gradienttape-gradient-can-only-be-called-once-on-non-pers
https://stackoverflow.com/questions/62453756/how-to-move-jupyter-notebook-cells-up-down-using-keyboard-shortcut#:~:text=Now%2C%20when%20you%20are%20in,Edit%20%2D%3E%20Move%20Cells%20Down%20.&text=That%20works!
https://www.mathworks.com/help/symbolic/sym.gradient.html
https://www.mathworks.com/matlabcentral/answers/462287-how-to-import-a-3d-python-numpy-array-into-matlab
https://stackoverflow.com/questions/10997254/converting-numpy-arrays-to-matlab-and-vice-versa
http://www.cds.caltech.edu/~murray/courses/cds101/fa08/pdf/L2-2_lyapunov.pdf
https://www.egr.msu.edu/~khalil/NonlinearSystems/Sample/Lect_8.pdf
https://www.researchgate.net/publication/324578263_Asymptotic_Controllability_and_Lyapunov-like_Functions_Determined_by_Lie_Brackets
http://localwww.math.unipd.it/~marson/TS16/Slides/Rampazzo.pdf
https://web.stanford.edu/class/ee363/lectures/lyap.pdf
https://arxiv.org/abs/1608.02712
https://www.quora.com/How-does-TensorFlow-calculate-gradients
https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html
https://github.com/lululxvi/deepxde/blob/master/examples/pinn_forward/diffusion_reaction.py
https://github.com/lululxvi/deepxde/blob/770e7c1f703682633fe182a6de984987fd579afa/deepxde/gradients.py
https://deepxde.readthedocs.io/en/latest/demos/pinn_forward.html#time-dependent-pdes
https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html
https://stackoverflow.com/questions/22071987/generate-random-array-of-floats-between-a-range
file:///home/cyan3/Desktop/Schrodinger__factor_for_EulerOMT.pdf
https://matlab.mathworks.com/
https://codeutility.org/python-3-x-how-to-compute-gradient-of-output-wrt-input-in-tensorflow-2-0-stack-overflow/
http://localhost:8888/notebooks/pinn_tensorflowv2.ipynb#
https://www.youtube.com/watch?v=y8HbErj_6j8
https://stackoverflow.com/questions/59922755/how-to-compute-the-gradient-of-only-one-output-unit
https://www.google.com/search?q=tensorflow+gradient+vs+numerical+gradient&biw=1290&bih=741&sxsrf=ALiCzsbOaIyW9vM2m8fKD3FUUlizBbpWHA%3A1661649533475&ei=fcIKY-_HHLmGptQPwOWB4Ag&ved=0ahUKEwiv-rvxruj5AhU5g4kEHcByAIwQ4dUDCA4&uact=5&oq=tensorflow+gradient+vs+numerical+gradient&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEKIEMgUIABCiBDoHCAAQRxCwA0oECEEYAEoECEYYAFCVCVieEWD4EmgCcAF4AIABdIgB_waSAQM1LjSYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1
https://arxiv.org/pdf/2007.11898.pdf
https://github.com/UZ-SLAMLab/ORB_SLAM3
https://arxiv.org/pdf/1502.00956.pdf
https://www.researchgate.net/figure/Semantic-understanding-allows-humans-to-predict-changes-in-the-environment-at-different_fig4_304163768
https://www.researchgate.net/figure/The-front-end-and-back-end-of-the-SLAM-process_fig4_263057565
https://vnav.mit.edu/material/23-SLAM1-formulationsAndSparsity-notes.pdf
https://stackoverflow.com/questions/39870642/matplotlib-how-to-plot-a-high-resolution-graph
https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html
https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html
https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html
https://deepxde.readthedocs.io/en/latest/demos/pinn_forward/eulerbeam.html?highlight=.jacobian#implementation
https://stackoverflow.com/questions/56072634/tf-2-0-runtimeerror-gradienttape-gradient-can-only-be-called-once-on-non-pers
https://stackoverflow.com/questions/62453756/how-to-move-jupyter-notebook-cells-up-down-using-keyboard-shortcut#:~:text=Now%2C%20when%20you%20are%20in,Edit%20%2D%3E%20Move%20Cells%20Down%20.&text=That%20works!
https://www.mathworks.com/help/symbolic/sym.gradient.html
https://www.mathworks.com/matlabcentral/answers/462287-how-to-import-a-3d-python-numpy-array-into-matlab
https://stackoverflow.com/questions/10997254/converting-numpy-arrays-to-matlab-and-vice-versa
http://www.cds.caltech.edu/~murray/courses/cds101/fa08/pdf/L2-2_lyapunov.pdf
https://www.egr.msu.edu/~khalil/NonlinearSystems/Sample/Lect_8.pdf
https://www.researchgate.net/publication/324578263_Asymptotic_Controllability_and_Lyapunov-like_Functions_Determined_by_Lie_Brackets
http://localwww.math.unipd.it/~marson/TS16/Slides/Rampazzo.pdf
https://web.stanford.edu/class/ee363/lectures/lyap.pdf
https://arxiv.org/abs/1608.02712
https://www.quora.com/How-does-TensorFlow-calculate-gradients
https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html
https://github.com/lululxvi/deepxde/blob/master/examples/pinn_forward/diffusion_reaction.py
https://github.com/lululxvi/deepxde/blob/770e7c1f703682633fe182a6de984987fd579afa/deepxde/gradients.py
https://deepxde.readthedocs.io/en/latest/demos/pinn_forward.html#time-dependent-pdes
https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html
https://stackoverflow.com/questions/22071987/generate-random-array-of-floats-between-a-range
file:///home/cyan3/Desktop/Schrodinger__factor_for_EulerOMT.pdf
https://codeutility.org/python-3-x-how-to-compute-gradient-of-output-wrt-input-in-tensorflow-2-0-stack-overflow/
https://www.youtube.com/watch?v=y8HbErj_6j8

#########################################################

restored_psi
tf.Tensor([-0.066 -0.066 -0.065 ...  0.04   0.04   0.04 ], shape=(16000,), dtype=float32)

test output
[-0.066 -0.066 -0.066 ...  0.04   0.04   0.04 ]

restored_vopt_xt
[[ 0.036  0.018 -0.     0.028]
 [ 0.036  0.018 -0.     0.028]
 [ 0.047  0.016  0.003  0.002]
 ...
 [ 0.018  0.031  0.024 -0.025]
 [ 0.018  0.031  0.024 -0.025]
 [ 0.018  0.031  0.024 -0.025]]


 restored_psi
tf.Tensor([-0.066 -0.066 -0.065 ...  0.04   0.04   0.04 ], shape=(16000,), dtype=float32)

test output
[-0.066 -0.066 -0.066 ...  0.04   0.04   0.04 ]

restored_vopt_xt
[[ 0.036  0.018 -0.     0.028]
 [ 0.036  0.018 -0.     0.028]
 [ 0.047  0.016  0.003  0.002]
 ...
 [ 0.018  0.031  0.024 -0.025]
 [ 0.018  0.031  0.024 -0.025]
 [ 0.018  0.031  0.024 -0.025]]

#########################################################

print(np.max(t0_V1))
print(np.max(t0_V2))
print(np.max(t0_V3))

print(np.max(mid_V1))
print(np.max(mid_V2))
print(np.max(mid_V3))

print(np.max(t5_V1))
print(np.max(t5_V2))
print(np.max(t5_V3))

# the histogram of the data
b = 4
n, bins, patches = plt.hist(t0_V1.reshape(-1), bins=b, density=True, facecolor='r', alpha=0.5)
n, bins, patches = plt.hist(t0_V2.reshape(-1), bins=b, density=True, facecolor='r', alpha=0.5)
n, bins, patches = plt.hist(t0_V3.reshape(-1), bins=b, density=True, facecolor='r', alpha=0.5)

n, bins, patches = plt.hist(mid_V1.reshape(-1), bins=b, density=True, facecolor='g', alpha=0.5)
n, bins, patches = plt.hist(mid_V2.reshape(-1), bins=b, density=True, facecolor='g', alpha=0.5)
n, bins, patches = plt.hist(mid_V3.reshape(-1), bins=b, density=True, facecolor='g', alpha=0.5)

n, bins, patches = plt.hist(t5_V1.reshape(-1), bins=b, density=True, facecolor='c', alpha=0.5)
n, bins, patches = plt.hist(t5_V2.reshape(-1), bins=b, density=True, facecolor='c', alpha=0.5)
n, bins, patches = plt.hist(t5_V3.reshape(-1), bins=b, density=True, facecolor='c', alpha=0.5)

plt.xlabel('v')
plt.ylabel('counts')
plt.title('vs: %.2f, %.2f, %.2f --- %.2f, %.2f, %.2f --- %.2f, %2.f, %.2f' % (
    np.max(t0_V1),
    np.max(t0_V2),
    np.max(t0_V3),

    np.max(mid_V1),
    np.max(mid_V2),
    np.max(mid_V3),

    np.max(t5_V1),
    np.max(t5_V2),
    np.max(t5_V3)))
plt.grid(True)
plt.show()

j = dde.gradients.Jacobian(restored_preds, inp_tensor)
dir(j)

dde.grad.jacobian(restored_preds,inp_tensor)

# library does not work for tf.v2

j = dde.gradients.Jacobian(output_tensor, input_tensor)
j()
print(j.J)




model._outputs_losses(
    False,
    model.train_state.X_test,
    model.train_state.y_test,
    model.train_state.test_aux_vars,
)

model.test_state

print(model.outputs_losses_train)
# with tf.GradientTape() as tape:
#     outputs_ = model.net(input_tensor, training=False)


x1 = torch.tensor(test_timesorted[:, 0], requires_grad = True)
one=torch.ones((2000,1))
psi = torch.tensor(torch.FloatTensor(one)*test_timesorted[:, 4])
psi.backward()

print(model.outputs)
print(dir(model))
print(model.print_model())

a = tf.constant(0.)
b = 2 * a
g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])
print(g)

x1 = test_timesorted[:, 0]
psi = test_timesorted[:, 4]

x1_tensor = tf.convert_to_tensor(x1)
print(x1_tensor)

inp = test_timesorted[:, 0:4]
input_tensor = tf.convert_to_tensor(inp)

output = model.predict(inp)
output_tensor = tf.convert_to_tensor(output)

tf.gradients(output_tensor, input_tensor)




        #############################################################################

        controlled_all_time_data = None
        if t_e > 0:
            '''
            adding control
            '''
            controlled_all_time_data = np.empty(
                (
                    initial_sample.shape[0],
                    initial_sample.shape[1],
                    len(t_samples))
                )

            controlled_dynamics_with_args = lambda state, t: controlled_dynamics(state, t, -alpha2, alpha2)
            for sample_i in range(initial_sample.shape[0]):
                sample_states = integrate.odeint(
                    controlled_dynamics_with_args,
                    initial_sample[sample_i, :],
                    t_samples)
                controlled_all_time_data[sample_i, :, :] = sample_states.T

#########################################################

print(model.outputs_losses_train)
# with tf.GradientTape() as tape:
#     outputs_ = model.net(input_tensor, training=False)

#########################################################

def WASS(y_true, y_pred):

    nSample=100

    x_grid = np.transpose(np.linspace(0., 6., nSample))

    y_grid = np.transpose(np.linspace(0., 6., nSample))



#    xpdf=y_true.np()

#    ypdf= y_pred.np()

#    sess = tf.compat.v1.InteractiveSession()

    xpdf= tf.make_ndarray(y_pred)

    ypdf= tf.make_ndarray(y_true)

    [X,Y] = np.meshgrid(x_grid,x_grid)

    C = (X - Y)**2

    cvector = C.flatten('F')





    A = np.concatenate((np.kron(np.ones((1,nSample)), sparse.eye(nSample).toarray()), np.kron(sparse.eye(nSample).toarray(),np.ones((1,nSample)))), axis=0)

    bvector = np.concatenate((xpdf,ypdf), axis=0)

    res=linprog(cvector, A_eq=A, b_eq=bvector, options={"disp": True})

    return res.fun

#########################################################

from scipy.interpolate import griddata as gd

target_t = 0.0

N = 50

test_timesorted = test[test[:, 3].argsort()]
timesorted = test_timesorted[:, 3]
test_ti = test_timesorted[np.where(np.abs(timesorted - target_t) < 1e-8), :][0] # 2k

ti_rho_opt = test_ti[:, 5]
ti_x1_x2_x3 = test_ti[:, 0:3]

####################################################################

x1 = np.linspace(state_min, state_max, N)
x2 = np.linspace(state_min, state_max, N)
x3 = np.linspace(state_min, state_max, N)

rho_opt = np.zeros((N,N,N))

closest_1 = [(np.abs(x1 - ti_x1_x2_x3[i, 0])).argmin() for i in range(ti_x1_x2_x3.shape[0])]
closest_2 = [(np.abs(x2 - ti_x1_x2_x3[i, 1])).argmin() for i in range(ti_x1_x2_x3.shape[0])]
closest_3 = [(np.abs(x3 - ti_x1_x2_x3[i, 2])).argmin() for i in range(ti_x1_x2_x3.shape[0])]

# some transposing going on in some reshape
# swapping closest_1/2 works well
rho_opt[closest_2, closest_1, closest_3] = ti_rho_opt

####################################################################

X1, X2, X3 = np.meshgrid(x1,x2,x3,copy=False) # each is NxNxN

RHO_OPT = gd(
  (ti_x1_x2_x3[:, 0], ti_x1_x2_x3[:, 1], ti_x1_x2_x3[:, 2]),
  ti_rho_opt,
  (X1, X2, X3),
  method='nearest')

####################################################################

x1_marginal = np.array([
    np.trapz(
        np.array([
            np.trapz(RHO_OPT[j, i, :], x=x3) # x3 slices for one x2 => R
            for i in range(len(x2))]) # x3 slices across all x2 => Rn
        , x=x2) # x2 slice for one x1 => R
for j in range(len(x1))])

x2_marginal = np.array([
    np.trapz(
        np.array([
            np.trapz(RHO_OPT[i, j, :], x=x3) # x3 slices for one x1 => R
            for i in range(len(x1))]) # x3 slices across all x1 => Rn
        , x=x1) # x1 slice for one x2 => R
for j in range(len(x2))])

x3_marginal = np.array([
    np.trapz(
        np.array([
            np.trapz(RHO_OPT[i, :, j], x=x2) # x2 slices for one x1 => R
            for i in range(len(x1))]) # x2 slices across all x1 => Rn
        , x=x1) # x1 slice for one x3 => R
for j in range(len(x3))])

####################################################################

# normalize all the pdfs so area under curve ~= 1.0
x1_pdf_area = np.trapz(x1_marginal, x=x1)
x2_pdf_area = np.trapz(x2_marginal, x=x2)
x3_pdf_area = np.trapz(x3_marginal, x=x3)
print("prior to normalization: %.2f, %.2f, %.2f" % (
    x1_pdf_area,
    x2_pdf_area,
    x3_pdf_area))

x1_marginal /= x1_pdf_area
x2_marginal /= x2_pdf_area
x3_marginal /= x3_pdf_area

x1_pdf_area = np.trapz(x1_marginal, x=x1)
x2_pdf_area = np.trapz(x2_marginal, x=x2)
x3_pdf_area = np.trapz(x3_marginal, x=x3)
print("after to normalization: %.2f, %.2f, %.2f" % (
    x1_pdf_area,
    x2_pdf_area,
    x3_pdf_area))

#########################################################

https://www.google.com/search?q=tensorflow+v1+symbolic+tensor+to+numpy+array&oq=tensorflow+v1+symbolic+tensor+to+numpy+array&aqs=chrome..69i57.11997j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy
https://github.com/tensorflow/models/issues/9706
https://www.google.com/search?q=tensorflow+v1+get+loss+function+session&sxsrf=ALiCzsbcKR1WCzKp0ESo8BpYAk1gzkgoHg%3A1662246352372&ei=0N0TY_m2Fti1qtsPjbqgoA4&ved=0ahUKEwj53vKa3vn5AhXYmmoFHQ0dCOQQ4dUDCA4&uact=5&oq=tensorflow+v1+get+loss+function+session&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEKIEMgUIABCiBDIFCAAQogQyBQgAEKIEOgoIABBHENYEELADOgQIIxAnOgUIABCRAjoLCAAQgAQQsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOggILhCxAxCDAToECAAQQzoHCAAQsQMQQzoICAAQgAQQsQM6BQgAEIAEOgYIABAeEBY6CAgAEB4QFhAKOgUIABCGAzoFCCEQoAE6CAghEB4QFhAdOgUIIRCrAjoHCCEQoAEQCjoHCAAQHhCiBEoECEEYAEoECEYYAFCfB1i4qwVgyqwFaANwAXgAgAHXAYgBmzSSAQYwLjM1LjOYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/compute_weighted_loss
file:///media/cyan3/30E1-D3BC/data/tiffany_tyler/BSc_Thesis_E_Wasei.pdf
https://www.arxiv-vanity.com/papers/1907.04502/
https://epubs.siam.org/doi/pdf/10.1137/19M1274067
https://stackoverflow.com/questions/64004445/how-to-convert-tensor-into-numpy-array
https://stackoverflow.com/questions/38647353/tensorflow-convert-tensor-to-numpy-array-without-eval-or-sess-run
https://pythonguides.com/tensorflow-tensor-to-numpy/
https://stackoverflow.com/questions/63869134/converting-tensorflow-tensor-into-numpy-array
https://blog.finxter.com/how-to-convert-a-tensor-to-a-numpy-array-in-tensorflow/
https://stackoverflow.com/questions/41607155/you-must-feed-a-value-for-placeholder-tensor-placeholder-with-dtype-float
https://www.google.com/search?q=deepxde+session&sxsrf=ALiCzsYSJxq7om1UPDowRRdvytsJYdwgIQ%3A1662245414633&ei=JtoTY-SYJrXFqtsPq4q46Ao&ved=0ahUKEwjkw9_b2vn5AhW1omoFHSsFDq0Q4dUDCA4&uact=5&oq=deepxde+session&gs_lcp=Cgdnd3Mtd2l6EAM6CggAEEcQ1gQQsAM6BAgjECc6BQgAEJECOhEILhCABBCxAxCDARDHARDRAzoLCAAQgAQQsQMQgwE6CwguELEDEIMBENQCOgQIABBDOg4ILhCABBCxAxDHARDRAzoHCC4Q1AIQQzoFCAAQgAQ6CwguEIAEEMcBEK8BOgcIABCABBAKOgYIABAeEBZKBAhBGABKBAhGGABQ5wVYuBtguhxoBHABeACAAcgBiAHsDZIBBTguNy4xmAEAoAEByAEIwAEB&sclient=gws-wiz
https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/backend/get_session
https://deepxde.readthedocs.io/en/latest/search.html?q=session&check_keywords=yes&area=default
https://www.tensorflow.org/api_docs/python/tf/compat/v1/InteractiveSession
https://danijar.com/what-is-a-tensorflow-session/
https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_default_session

#########################################################

https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb
https://colab.research.google.com/drive/1Y3n-Et56ZXedGlWahJ4VTQHpFqMy2I2m#scrollTo=ArT4LfbxT7AC
https://stackoverflow.com/questions/63869134/converting-tensorflow-tensor-into-numpy-array
https://stackoverflow.com/questions/70767238/unable-to-convert-tensorflow-python-framework-ops-tensor-object-to-numpy-array-f
https://www.tensorflow.org/api_docs/python/tf/py_function
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/py_function
https://www.google.com/search?q=scipy+linprog&oq=scipy+linprog&aqs=chrome..69i57.1693j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/49127834/removing-conda-environment
https://www.google.com/search?q=conda+list+environments&oq=conda+list+environments&aqs=chrome..69i57.2198j0j7&sourceid=chrome&ie=UTF-8
https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html
https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html
http://localhost:8888/notebooks/wass.ipynb
https://www.google.com/search?sxsrf=ALiCzsYR8IPv-XpexqqsdI3AYr2oxz08SQ:1662341520009&q=tensorflow.python.client.pywrap+tf_session+import+*+initialization+failed&spell=1&sa=X&ved=2ahUKEwj-ma7ewPz5AhWGlGoFHffTDZ8QBSgAegQIARA2&biw=960&bih=876&dpr=1
https://www.google.com/search?q=sparse.eye&oq=sparse.eye&aqs=chrome..69i57.1993j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/61903139/tf-py-function-not-able-to-return-a-list
https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html

#########################################################


def WASS3(y_true, y_pred):
    # xpdf=y_true.np()
    # ypdf= y_pred.np()
    # sess = tf.compat.v1.InteractiveSession()
    # xpdf = tf.make_ndarray(y_pred)
    # ypdf= tf.make_ndarray(y_true)

    # ipdb.set_trace();

    # xpdf = tf.make_ndarray(y_pred.op.get_attr('value'))
    # ypdf = tf.make_ndarray(y_true.op.get_attr('value'))

    # xpdf = y_pred.numpy()
    # ypdf = y_true.numpy()

    print("####")
    print(y_pred)
    print(type(y_pred))
    print(y_true)
    print(type(y_true))

    # print("####")
    # print(np.sum(xpdf))
    # print(np.sum(ypdf))

    # ipdb.set_trace();

    # nSample=1000
    # x_grid = np.transpose(np.linspace(0., 6., nSample))
    # y_grid = np.transpose(np.linspace(0., 6., nSample))

    # [X,Y] = np.meshgrid(x_grid,x_grid)
    # C = (X - Y)**2

    # cvector = C.flatten('F')

    # A = np.concatenate((np.kron(np.ones((1,nSample)), sparse.eye(nSample).toarray()), np.kron(sparse.eye(nSample).toarray(),np.ones((1,nSample)))), axis=0)

    # bvector = np.concatenate((xpdf,ypdf), axis=0)

    # res=linprog(cvector, A_eq=A, b_eq=bvector, options={"disp": True})

    # return res.fun

    # return tf.constant(1.0, dtype=config.real(tf))

    v = tf.Variable(1.)
    v = tf.constant(1.)

    # print("v: ", v.shape)

    # loss = tf.py_function(
    #     func=WASS,
    #     inp=[y_true, y_pred],
    #     Tout=tf.float32
    # )
    res = 1.0 + 0 * tf.math.reduce_min(y_true - y_pred)
    return res

    # res = tf.math.real(y_true)
    # print(res.shape)

    # print("####")
    # print("v", v)
    # print("type(res)", type(res))
    # print("res", res)
    # print("res.numpy()", res.numpy())

    # res = tf.constant(1.0)
    # dataset = tf.data.Dataset.from_tensor_slices(tensor1)

    # ipdb.set_trace();

    # return 3.0

    # return v
    # return v.ref().deref()
    # return v.read_value()
    # return res
    # return dataset[0]
    # return bkd.reduce_mean(bkd.square(y_true - y_pred))



    loss = tf.numpy_function(
        func=WASS,
        inp=[y_true, y_pred],
        Tout=tf.float32
    )
    # ipdb.set_trace();
    # loss.set_shape(y_true.get_shape())

(tf) cyan3@vipersim01:~/gradschool/231/research$ ./wass.py 
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From /usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Enable just-in-time compilation with XLA.

WARNING:tensorflow:From /usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/nn/initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Compiling model...
Building feed-forward neural network...
/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:103: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.031497 s

2022-09-05 15:24:27.698345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-05 15:24:28.099728: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-09-05 15:24:28.099770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6673 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:91:00.0, compute capability: 7.5
Tensor("Sum:0", shape=(), dtype=float32)
Tensor("Sum_1:0", shape=(), dtype=float32)
Tensor("Sum_2:0", shape=(), dtype=float32)
Tensor("Sum_3:0", shape=(), dtype=float32)
Tensor("Sum_4:0", shape=(), dtype=float32)
Tensor("Sum_5:0", shape=(), dtype=float32)
'compile' took 2.642019 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
2022-09-05 15:24:30.348215: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Training model...

2022-09-05 15:24:30.626271: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f434c008d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-05 15:24:30.626345: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Quadro RTX 4000, Compute Capability 7.5
2022-09-05 15:24:30.637792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-09-05 15:24:31.440390: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-09-05 15:24:31.446530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6673 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:91:00.0, compute capability: 7.5
start linprog
start linprog
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.15
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.30
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
start linprog
start linprog
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.09
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.30
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
Step      Train loss                                            Test loss                                             Test metric
0         [2.92e-02, 4.04e-04, 1.87e-01, 1.00e+00, 1.00e+00]    [2.92e-02, 4.04e-04, 1.87e-01, 1.00e+00, 1.00e+00]    []  
start linprog
start linprog
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.25
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
Running HiGHS 1.2.2 [date: 2022-08-30, git hash: n/a]
Copyright (c) 2022 ERGO-Code under MIT licence terms
Presolving model
Problem status detected on presolve: Infeasible
Model   status      : Infeasible
Objective value     :  0.0000000000e+00
HiGHS run time      :          0.15
WARNING: Method getModelStatus(const bool scaled_model) is deprecated: alternative method is getModelStatus()
end
None
2022-09-05 15:25:37.725396: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:462 : INVALID_ARGUMENT: Input to reshape is a tensor with 1 values, but the requested shape has 1000

         [[{{node gradients_16/sub_21_grad/Reshape}}]]

#########################################################

https://stackoverflow.com/questions/70767238/unable-to-convert-tensorflow-python-framework-ops-tensor-object-to-numpy-array-f/70770060#70770060
https://deepxde.readthedocs.io/en/latest/modules/deepxde.icbc.html
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html?highlight=compile#deepxde.model.Model.compile
https://www.tensorflow.org/api_docs/python/tf/py_function
https://stackoverflow.com/questions/35550451/tensorflow-py-func-or-custom-function
https://www.anycodings.com/1questions/3781488/using-tfpyfunc-as-loss-function-to-implement-gradient-descent
https://www.google.com/search?q=Shapes+of+all+inputs+must+match:+values%5B0%5D.shape+%3D+%5B%5D+!%3D+values%5B3%5D.shape+%3D+%5B1%5D+site:stackoverflow.com&sxsrf=ALiCzsa1Li5KOOhgmFxAp4Z3Nk_HFBzxWA:1662399748251&sa=X&ved=2ahUKEwi9h-DTmf75AhUClmoFHVF_AtcQrQIoBHoECAQQBQ&biw=960&bih=939&dpr=1
https://stackoverflow.com/questions/61553510/tensorflow-2-lstm-invalidargumenterror-shapes-of-all-inputs-must-match
https://stackoverflow.com/questions/69193435/fixing-error-invalidargumenterror-shapes-of-all-inputs-must-match-values0
https://www.google.com/search?q=tensorflow+v1+reduce_mean&oq=tensorflow+v1+reduce_mean&aqs=chrome..69i57.3459j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=tf+shape+%5B%5D&sxsrf=ALiCzsYAXxZLNy-fjv5Un9z60O1zLBK7qA%3A1662399973702&ei=5TUWY4i-KteuqtsPkYKl0Ak&ved=0ahUKEwjIxKC_mv75AhVXl2oFHRFBCZoQ4dUDCA8&uact=5&oq=tf+shape+%5B%5D&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIGCAAQHhAWMgYIABAeEBYyBggAEB4QFjIGCAAQHhAWMgYIABAeEBYyBggAEB4QFjIGCAAQHhAWMgYIABAeEBY6CAgAEIAEELADOggIABCwAxCRAjoHCAAQHhCwAzoECCMQJzoICC4QgAQQsQM6CwgAEIAEELEDEIMBOggIABCABBCxAzoECAAQQzoKCAAQsQMQgwEQQzoKCAAQgAQQhwIQFDoFCAAQkQJKBAhBGAFKBAhGGABQighY_hZgvRpoAnAAeAGAAe8BiAGfD5IBBjAuMTAuMZgBAKABAcgBCsABAQ&sclient=gws-wiz
https://www.google.com/search?q=py_function+loss+function&sxsrf=ALiCzsY3Svl2RHHvb_BSTKVDfo_YKLb1rQ%3A1662400367213&ei=bzcWY-u7DNa6qtsPsJ63mA4&ved=0ahUKEwirr_L6m_75AhVWnWoFHTDPDeMQ4dUDCA8&uact=5&oq=py_function+loss+function&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCrAjIFCCEQqwI6CggAEEcQ1gQQsAM6BQgAEJECOgUIABCABDoECAAQHjoGCAAQHhAWOgkIABAeEMkDEBY6CAgAEB4QDxAWOggIABAeEBYQCjoFCAAQhgM6CAghEB4QFhAdSgQIQRgASgQIRhgAUP8DWLUQYP0QaAFwAHgAgAHSAYgBkw2SAQYwLjExLjGYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html
https://www.google.com/search?q=tf+%22py_func%22+loss+output&sxsrf=ALiCzsYC70ZkIXseIpdiLRrN7nDlsEW8Hg%3A1662400865893&ei=YTkWY9aFNvC4qtsPnZKl0Ak&ved=0ahUKEwiWutfonf75AhVwnGoFHR1JCZoQ4dUDCA8&uact=5&oq=tf+%22py_func%22+loss+output&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMggIIRAeEBYQHToHCCMQsAMQJzoKCAAQRxDWBBCwAzoGCAAQHhAWOgUIABCGAzoFCCEQqwJKBAhBGABKBAhGGABQqUpYm1Vgo1ZoAnAAeACAAfcBiAHIDZIBBjAuMTAuMZgBAKABAcgBCcABAQ&sclient=gws-wiz
https://stackoverflow.com/questions/46273364/tensorflow-valueerror-shapes-must-be-equal-rank-but-are-0-and-2
https://stackoverflow.com/questions/50187667/using-python-code-with-tf-py-func-in-custom-keras-layer
https://www.tensorflow.org/api_docs/python/tf/compat/v1/py_func
https://stackoverflow.com/questions/45061344/gradient-update-with-a-custom-loss-function
https://stackoverflow.com/questions/68275702/pass-inputs-to-loss-function-in-eager-mode
https://stackoverflow.com/questions/66287603/how-can-i-implement-this-custom-loss-function-in-keras
https://keras.io/api/layers/core_layers/#lambda
https://stackoverflow.com/questions/69352679/attributeerror-tensor-object-has-no-attribute-numpy-while-using-tf-disable
https://stackoverflow.com/questions/50109996/whats-the-difference-between-tf-constant-and-tf-convert-to-tensor
https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean
https://www.google.com/search?q=tf+create+tensor+from+number&oq=tf+create+tensor+from+number&aqs=chrome..69i57.3366j0j7&sourceid=chrome&ie=UTF-8
https://www.guru99.com/tensor-tensorflow.html
https://www.tensorflow.org/api_docs/python/tf/Tensor
https://stackoverflow.com/questions/37071788/tensorflow-how-to-modify-the-value-in-tensor
https://www.google.com/search?q=tensorflow.python.framework.ops.EagerTensor&oq=tensorflow.python.framework.ops.EagerTensor&aqs=chrome..69i57.440j0j7&sourceid=chrome&ie=UTF-8
https://discuss.tensorflow.org/t/attributeerror-tensorflow-python-framework-ops-eagertensor-object-has-no-attribute-to-tensor/5044
https://stackoverflow.com/questions/49568041/tensorflow-how-do-i-convert-a-eagertensor-into-a-numpy-array
https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum
https://stackoverflow.com/questions/64403061/how-to-convert-a-tensor-to-eager-tensor-in-tensorflow-2-1-0
https://www.tensorflow.org/api_docs/python/tf/math/reduce_min
https://www.tensorflow.org/api_docs/python/tf/py_function
https://www.tensorflow.org/api_docs/python/tf/numpy_function
https://stackoverflow.com/questions/9757642/wrapping-specialised-c-template-class-with-swig
https://stackoverflow.com/questions/45735858/why-tensorflow-just-outputs-killed
https://stackoverflow.com/questions/50740771/how-can-i-use-c-struct-pointers-in-python-using-swig

#########################################################

from scipy.stats import truncnorm
N = nSample = 1000
def pdf1d_0(x):
    sigma = 1
    mu=2
    a, b = (0. - mu) / sigma, (6. - mu) / sigma
    rho_0=truncnorm.pdf(x, a, b, loc = mu, scale = sigma)
    return rho_0
x_T = np.transpose(np.linspace(0., 6., N))
rho_0=pdf1d_0(x_T).reshape(len(x_T),1)

x_grid = np.transpose(np.linspace(0., 6., nSample))
y_grid = np.transpose(np.linspace(0., 6., nSample))
[X,Y] = np.meshgrid(x_grid,x_grid)
C = (X - Y)**2
cvector = C.flatten('F')

A = np.concatenate(
    (
        np.kron(
            np.ones((1,nSample)),
            sparse.eye(nSample).toarray()
        ),
        np.kron(
            sparse.eye(nSample).toarray(),
            np.ones((1,nSample))
        )
    ), axis=0)
# 2*nSample

def WASS(y_true, y_pred):
    xpdf = y_pred.numpy()
    # ypdf = y_true.numpy()

    xpdf = np.where(xpdf < 0, 0, xpdf)

    ipdb.set_trace()

    bvector = np.concatenate((xpdf, rho_0), axis=0)
    bvector = bvector.reshape(-1)

    print("start linprog")
    # min cvector * x s.t. A * x = b. 

    # res=linprog(
    #     cvector,
    #     A_eq=A,
    #     b_eq=bvector,
    #     options={"disp": True})
    # print(res.fun)

    # Define and solve the CVXPY problem.
    x = cp.Variable(A.shape[1])
    prob = cp.Problem(
        cp.Minimize(cvector.T @ x),
        [A @ x == bvector])
    prob.solve()
    res = prob.value

    print("end")

    # must use a 'reduce'
    res = 1.0 + 0 * tf.math.reduce_min(y_true)
    return res

def WASS2(y_true, y_pred):
    loss = tf.py_function(
        func=WASS,
        inp=[y_true, y_pred],
        Tout=tf.float32
    )
    return loss

#########################################################
#########################################################
#########################################################
#########################################################

Tried with custom z function z2 + z
z is all about getting the trapz to be 1
30k iterations
does not really converge, minimum is 3
t=0 pred2 is a gaussian but trapz is 45

#########################################################

scratch0

try again using loss function written only with tf

cand = tf.abs(1.0 - tf.reduce_sum(y_pred))
loss function converges, but sum of 12

#########################################################

scratch1

cand = tf.abs(0.5 - tf.reduce_sum(y_pred))
loss function converges, but sum of 9

does not converge super well

#########################################################

scratch2

cand = tf.abs(0 - tf.reduce_sum(y_pred))
loss function converges, but sum of 6

converges well

#########################################################

goal actual
0    6
0.5  9
1    12

actual=state_max + state_max*goal

so to get it to actual=0

0=6+6*goal
-6 = 6*goal
-1 = goal

#########################################################

scratch3

cand = tf.abs(-1 - tf.reduce_sum(y_pred))
loss function converges, but sum of 0 (as hypothesized)
except it cheats because there are negative terms
so we need to add penalty for that

converges well

#########################################################

scratch 5

if we want actual=1

1 = 6+6*goal
-5 / 6 = goal

adding a penalty for negative terms with a gain of 50

output is as desired!

#########################################################

trying this for another rho_0 at another mu_0

#########################################################

http://localhost:8888/notebooks/wass_1d-scratch7.ipynb
http://localhost:8888/notebooks/wass_1d-scratch6.ipynb
https://numpy.org/doc/stable/reference/generated/numpy.trapz.html
https://stackoverflow.com/questions/47675074/correctly-calculate-the-area-beneath-curve-using-trapz-and-simpson-in-python
https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError
https://github.com/keras-team/keras/blob/b80dd12da9c0bc3f569eca3455e77762cf2ee8ef/keras/utils/losses_utils.py#L276
https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy
https://github.com/keras-team/keras/blob/v2.10.0/keras/losses.py#L112-L160
https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer
https://www.google.com/search?q=cvx+in+tensorflow&oq=cvx+in+tensorflow&aqs=chrome..69i57.7249j0j7&sourceid=chrome&ie=UTF-8
https://github.com/cvxgrp/cvxpylayers/tree/master/examples/tf
https://github.com/cvxgrp/cvxpylayers/blob/master/examples/tf/convex_approximate_dynamic_programming.ipynb
https://github.com/cvxgrp/cvxpylayers/blob/master/examples/tf/optimizing_stiffness_constants.ipynb
https://github.com/lululxvi/deepxde/discussions/902
https://github.com/lululxvi/deepxde/discussions/categories/q-a?discussions_q=minimize+category%3AQ%26A
https://github.com/lululxvi/deepxde/discussions/837
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html?highlight=loss%20function
https://sharepoint.msfc.nasa.gov/sites/programs/VIPER/IAU%20PDU/Forms/Active.aspx?RootFolder=%2Fsites%2Fprograms%2FVIPER%2FIAU%20PDU%2F02%20Documentation%2FPAPI%20Docs&FolderCTID=0x01200052EADA4F0363964E9C9062B1FA69C4ED&View=%7B24AE143B%2DCEA5%2D45F9%2DA2E8%2D65BF6BA05B36%7D
https://trunk.arc.nasa.gov/bitbucket/projects/VIPERGDS/repos/viper_devsim/permissions
https://trunk.arc.nasa.gov/confluence/pages/viewpage.action?spaceKey=VIPERSW&title=RTS+Table
https://deepxde.readthedocs.io/en/latest/modules/deepxde.html
https://www.tensorflow.org/api_docs/python/tf/math/reduce_min
https://www.databricks.com/tensorflow/linear-equations
https://www.cvxpy.org/examples/basic/linear_program.html
https://wikidocs.net/24605
https://trunk.arc.nasa.gov/confluence/display/VIPERMS/VERVE+Quick+Reference
https://www.google.com/search?q=tf+linear+program+solver&oq=tf+linear+program+solver&aqs=chrome.0.69i59.6148j0j7&sourceid=chrome&ie=UTF-8
https://www.tensorflow.org/api_docs/python/tf/linalg/solve
https://www.google.com/search?q=tf+autograd+minimize&sxsrf=ALiCzsYDnf2Veg7LztqmppBD_z9etdM5XQ:1662698822494&ei=RsUaY-flHcupqtsPvJm42AE&start=10&sa=N&ved=2ahUKEwjnoLvl84b6AhXLlGoFHbwMDhsQ8tMDegQIARA7&biw=2400&bih=1182&dpr=0.8
https://github.com/brunorigal/autograd-minimize
http://localhost:8888/notebooks/wass_1d-scratch0.ipynb
http://localhost:8888/notebooks/wass_1d-scratch0.ipynb
https://docs.carrington.edu/catalog/carrington-college.pdf
https://outlook.office365.com/mail/
https://calendar.google.com/calendar/u/0/r/week?pli=1
http://localhost:8888/notebooks/wass_1d-scratch0.ipynb

https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
https://open.spotify.com/collection/tracks
https://www.google.com/search?q=pytorch+speeding+up&oq=pytorch+speeding+up&aqs=chrome..69i57j69i59.5916j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/56223799/speeding-up-cvxpy-processing-speed
https://www.cvxpy.org/tutorial/advanced/index.html
https://github.com/cvxgrp/cvxpylayers/blob/master/cvxpylayers/torch/cvxpylayer.py
https://github.com/cvxgrp/diffcp
https://github.com/embotech/ecos/wiki
https://pytorch.org/docs/stable/generated/torch.nan_to_num.html
https://github.com/cvxpy/cvxpy/issues/1046
https://discuss.pytorch.org/t/how-to-try-catch-errors-during-training/108619


#########################################################

integral notion of distance, not analytical like other ideas of distance, implicit differentiation
but implementation in tensorflow is a different idea

autodiff finds 'implicit derivatives'

"analytical vs integral notions of distance and differentiability"
"all research happens after midnight"
"this is definitely the right direction"
"every neural network is sneaky"

----------------------------------------------------

theory / analytically / the geometry:
	wasserstein distance in the lp means that
	wasserstein distance is a function of neural network parameters

	for us, our y_pred pinn output for the rho_0 boundary condition
	goes into th bvector of Ax == b constraint, the RHS
	this couples the linear program for wasserstein distance
	to the PINN neural network parameters
the implementation:
	tensorflow does auto-diff, so it finds derivatives in functions that cannot be found in any other way / gradients in losses
	it does auto-diff by watching computation forward and backwards in an epoch maybe, and a graph of tensors
	
	when we use tf.py_func, we evaluate a tensor OUTSIDE THE GRAPH
		for example, using scipy.linprog we use this to convert the tensor to a numpy array
	which means that whatever happens inside the eager / non-graph function is a black box to the gradient / auto-diff
	so because there is no gradient, this cost / loss will NOT CONVERGE, so there is no learning, no training actually

	and empirically, we see that
		when we use scipy.linprog, the cost function for rho_0 trunnorm never converges
		it goes, fail to solve, solve with loss=3, solve with loss=4, solve with loss=3, fail, 3, 4, 3, 4
		it doesn't converge, will never succeed

		when we define a loss function that uses tf functions (mse), it does converge
hypothesis:
	we need to compute both the wasserstein LP loss AND have a gradient for tf
	to compute the LP we need to solve the cvx LP problem with differentiation, a differentiable CVX
	https://github.com/cvxgrp/cvxpylayers
	https://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf

----------------------------------------------------

normalization
every forward prop:
normalize by trapze

in loss function
normalize by element sum (reduce_sum)

truncated gaussian != gaussian
user supplied pdf (rho_0 / rho_T)
gaussian integral is only 1 from -inf to inf
truncated gaussian is 1 in finite domain
	multivariable truncated gaussian
be consistent with same type of gaussian

pinn does not know everything should be pdf, must be enforced
"non-negotiable constraints (pdf, non-negative) should not be in cost"

normalize by correct trapze: enforce it is a pdf
normalize by element sum: enforce what is fed to lp is sum(1)

#########################################################

1. 
run 1d example, with wass

1.
add to linprog:
x >= 0
x < inf

2.
return np.sqrt(res.fun)

3.
rho_0
rho_T
how do we enforce network prediction is pdf?
in wass loss function, penalize each element being less than 0, sum of y_pred diff vs 1

#########################################################

---------------

got tfv2 example of wass distance + gradient
wrote tfv2 rho0_wass loss function
when training, tf.Tensor cvxpy Parameter not compatible (no .numpy() again)
  ask on forum, consider different backend

got torch example of computing wass distance + gradient
when trying to train torch, get CUDA error
  so upgrade torch version to use CUDA 11.3, so that it can use my GPU device
    https://pytorch.org/get-started/locally/
    pip uninstall torch -y
    pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
  set CUDA_HOME, and make sure that CUDA_HOME has a libdevice.10.bc symlinked in /bin/nvvm/libdevice/
  have to add some type and device explicit code to get example code to run and converge

works after the above fixes, can use cvxpylayer and get gradient with cpu or cuda:0!!!

---------------

started training with wass loss function! very slow, but does seem to be converging for 5.0,1.0-0.0,1.0
4 hours for 100 epochs
but the solver starts to fail for 1.0 to 3.0

found that for TimePDE, the loss functions are called with loss(diff, zero), so y_pred is the DIFF, and y_true is a 0-vector, this explains what we saw earlier

so computing a wasserstein distance on this is not the semantics / distance we really want

so i fixed it in the source code to pass y_pred and y_true so i can compute wasserstein in loss function

i also want to train for a pmf first, and then switch to training for wasserstein

and i want to sanity check that the pmf training is working, test output does not seem right
but then i save the model and this seems right? it's not saving the right model?

it is saving the last model (not intermediate ones?)

but the test data parsing code is wrong because it gets more than N
samples for the rho_0 test data because it was thresholding by t=0
but num_initial test data also has data at t=0, so this means more net output
than there is for t=0 really
so instead sample how the pde samples it, by bc indices

test_ti2 = np.loadtxt('./test.dat')
# test_ti2 = test_ti2[test_ti2[:, T_IDX].argsort()]
# test_ti2 = test_ti2[np.where(np.abs(test_ti2[:, T_IDX]) < 1e-8), :][0] # 2k
# import ipdb; ipdb.set_trace();
test_ti2 = test_ti2[0:N, :]
ind = np.lexsort((test_ti2[:,X_IDX],test_ti2[:,T_IDX]))
test_ti2 = test_ti2[ind]
# use beg:end 0:N N:2N to sample 0_BC, T_BC
# instead of thresholding by time
# because num_initial will also have samples at t=0, etc.




#########################################################


#########################################################

cvxpy: ECOS is faster than SCS most of the time

the boundary condition loss values are VERY important:

10000     [4.66e-06, 3.85e-03, 9.55e-05, 4.18e-02, 1.04e-01]    [4.66e-06, 3.85e-03, 9.55e-05, 4.18e-02, 1.04e-01]    []

e-01 is NOT good enough, was not a PDF / PMF

but the better model:

8824      [2.73e-06, 3.84e-03, 1.02e-04, 2.49e-02, 4.77e-02]    [2.73e-06, 3.84e-03, 1.02e-04, 2.49e-02, 4.77e-02]    []

has a VERY close match on the boundary conditions

so this metric you need to be careful, small differences in metric mean different outcomes

#########################################################

use cdist to define C matrix for wasserstein distance so C table entries are pairwise distances between 3d points, not 1d points
https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist

N=10, use meshgrid so N**3 is the BC size and inputs to wasserstein (C = (N**3)**2, still a 2d grid but now pairwise distance between 3d points, A = 2*(N**3), B = 2*(N**3))

vs01:
run ./wass_3d_torch_scratch1.py --N 10 --js 3,2,1 --q 0.0

vs08:
run ./wass_3d_torch_scratch1.py --N 8 --js 1,1,2 --q 0.0

vs04:
run ./wass_3d_torch_scratch1.py --N 8 --js 1,1,2 --q 0.5

vs05:
run ./wass_3d_torch_scratch1.py --N 8 --js 3,2,1 --q 0.5

vs03:?


vs08:
cvx failed, returning mse
31        [3.50e-03, 3.87e-03, 4.17e-05, 5.63e-05]    [3.50e-03, 3.87e-03, 4.17e-05, 5.63e-05]    []
Epoch 31: train loss improved from 4.97e-03 to 3.87e-03, saving model to /usr/local/home/cyan3/gradschool/231/r
esearch/wass_3d_model-31.pt ...

File /usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/scipy/sparse/_coo.py:400, in coo_matrix.tocsr(self, copy)
    397 row = self.row.astype(idx_dtype, copy=False)
    398 col = self.col.astype(idx_dtype, copy=False)
--> 400 indptr = np.empty(M + 1, dtype=idx_dtype)
    401 indices = np.empty_like(col, dtype=idx_dtype)
    402 data = np.empty_like(self.data, dtype=upcast(self.dtype))

MemoryError: Unable to allocate 514. GiB for an array with shape (68988175361,) and data type int64


File /usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/scipy/sparse/_coo.py:400, in coo_matri
x.tocsr(self, copy)
    397 row = self.row.astype(idx_dtype, copy=False)
    398 col = self.col.astype(idx_dtype, copy=False)
--> 400 indptr = np.empty(M + 1, dtype=idx_dtype)
    401 indices = np.empty_like(col, dtype=idx_dtype)
    402 data = np.empty_like(self.data, dtype=upcast(self.dtype))

MemoryError: Unable to allocate 7.29 TiB for an array with shape (1002001002001,) and data type int64

In [2]: 

#########################################################

so because of above, i need to compute the wasserstein-2 distance differently because this linprog way requires A matrix of size (N**3)**2 x 2*(N**3) => N**9

and constructing the Cvxpylayers object and standing up the underlying problem solvers requires even more memory

it is computationally infeasible

#########################################################

several options are available for a faster wasserstein+gradient (must be diffentiable in pytorch to learn)
https://gist.github.com/Flunzmas/6e359b118b0730ab403753dcc2a447df
https://gist.github.com/jcowles/1de8b12c38603ce932b0154bc6d59d60
https://gist.github.com/sethaxen/6408d7f104ba44abfbb74981f8a8deb8
https://gist.github.com/Flunzmas/6e359b118b0730ab403753dcc2a447df
https://gist.github.com/pratheeksh/32fa493676222252cc9774b947280397
https://github.com/dfdazac/wassdistance/blob/master/sinkhorn.ipynb

these require statistical metrics on the 3d pdf grid: mu / sigma

so the question is how to find the mu / sigma artifacts given an arbitrary pinn output / 3d pdf?

#########################################################

https://stats.stackexchange.com/questions/297141/finding-mean-and-covariance-of-an-arbitrary-joint-pdf

however, as we empirically see, for a multivariate non-truncated pf, we need to have a sufficiently large state-space to recover the mu accurately:

N:  10
js:  1.0 1.0 2.0
q:  0.0
0.9999999999999999
r=3.500 recovered mu: [2.332, 2.332, 2.332], err=0.668
1.0
r=8.579 recovered mu: [2.999, 2.999, 2.999], err=0.001
1.0000000000000002
r=13.658 recovered mu: [2.980, 2.980, 2.980], err=0.020
1.0
r=18.737 recovered mu: [2.239, 2.239, 2.239], err=0.761
1.0
r=23.816 recovered mu: [2.648, 2.648, 2.648], err=0.352
1.0
r=28.895 recovered mu: [3.211, 3.211, 3.211], err=0.211
1.0
r=33.974 recovered mu: [3.775, 3.775, 3.775], err=0.775
1.0000000000000002
r=39.053 recovered mu: [4.339, 4.339, 4.339], err=1.339
1.0
r=44.132 recovered mu: [4.904, 4.904, 4.904], err=1.904
0.9999999999999998
r=49.211 recovered mu: [5.468, 5.468, 5.468], err=2.468
0.9999999999999998
r=54.289 recovered mu: [6.032, 6.032, 6.032], err=3.032
1.0000000000000002
r=59.368 recovered mu: [6.596, 6.596, 6.596], err=3.596
1.0
r=64.447 recovered mu: [7.161, 7.161, 7.161], err=4.161
1.0
r=69.526 recovered mu: [7.725, 7.725, 7.725], err=4.725
1.0
r=74.605 recovered mu: [8.289, 8.289, 8.289], err=5.289
1.0
r=79.684 recovered mu: [8.854, 8.854, 8.854], err=5.854
1.0
r=84.763 recovered mu: [9.418, 9.418, 9.418], err=6.418
1.0
r=89.842 recovered mu: [9.982, 9.982, 9.982], err=6.982
1.0
r=94.921 recovered mu: [10.547, 10.547, 10.547], err=7.547
1.0
r=100.000 recovered mu: [11.111, 11.111, 11.111], err=8.111

https://math.stackexchange.com/questions/445164/is-the-mean-of-the-truncated-normal-distribution-monotone-in-mu

#########################################################

r=3.500, N=13 recovered mu: [1.737, 1.737, 1.737], err=0.263
r=8.579, N=18 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=13.658, N=23 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=18.737, N=28 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=23.816, N=33 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=28.895, N=38 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=33.974, N=43 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=39.053, N=49 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=44.132, N=54 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=49.211, N=59 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=54.289, N=64 recovered mu: [2.000, 2.000, 2.000], err=0.000
r=59.368, N=69 recovered mu: [1.999, 1.999, 1.999], err=0.001
r=64.447, N=74 recovered mu: [2.001, 2.001, 2.001], err=0.001
r=69.526, N=79 recovered mu: [1.999, 1.999, 1.999], err=0.001
r=74.605, N=84 recovered mu: [2.001, 2.001, 2.001], err=0.001
r=79.684, N=89 recovered mu: [1.999, 1.999, 1.999], err=0.001
r=84.763, N=94 recovered mu: [2.001, 2.001, 2.001], err=0.001
r=89.842, N=99 recovered mu: [1.999, 1.999, 1.999], err=0.001
r=94.921, N=104 recovered mu: [2.001, 2.001, 2.001], err=0.001
r=100.000, N=110 recovered mu: [2.001, 2.001, 2.001], err=0.001


r=3.500, N=13 recovered mu: [1.737, 1.737, 1.737], err=0.263
r=3.500, N=18 recovered mu: [1.739, 1.739, 1.739], err=0.261
r=3.500, N=23 recovered mu: [1.739, 1.739, 1.739], err=0.261
r=3.500, N=28 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=33 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=38 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=43 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=49 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=54 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=59 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=64 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=69 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=74 recovered mu: [1.740, 1.740, 1.740], err=0.260
r=3.500, N=79 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=84 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=89 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=94 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=99 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=104 recovered mu: [1.741, 1.741, 1.741], err=0.259
r=3.500, N=110 recovered mu: [1.741, 1.741, 1.741], err=0.259

as you seen, you can change N, but the correct estimation of mu is related to the numerical RANGE, state_min / state_max, because it is a norm, not a truncnorm, so the equation for mu is integral from -inf to inf

if you had a trunnorm pdf, then it would be integral from a to b, and the state bounds would not matter for mu estimation

#########################################################

use julia multivariate trunc norm to get a 3d trunc norm from julia into python, cached

import Pkg; Pkg.add("Distributions")
import Pkg; Pkg.add("Primes")
import Pkg; Pkg.add("Parameters")
include("./TMvNormals.jl/src/TMvNormals.jl")
using .TMvNormals
using LinearAlgebra

d = TMvNormal(
 zeros(3),
 ones((3,3)) + 5*I(3),
 [-1, -1, -Inf],
 [1, 2, 5]
)


import julia
from julia.api import Julia
jl = Julia(compiled_modules=False)
jl.eval('import Pkg; Pkg.add("Primes")')
jl.eval('import Pkg; Pkg.add("Parameters"); Pkg.add("Distributions");')
jl.eval('include("/home/cyan3/Dev/jim/TMvNormals.jl/src/TMvNormals.jl")')
jl.eval('using .TMvNormals')
jl.eval('using LinearAlgebra')
jl.eval('d = TMvNormal(zeros(3),ones((3,3)) + 5*I(3),[-1, -1, -Inf],[1, 2, 5])')
jl.eval('pdf(d, zeros(3))')


#########################################################

Step      Train loss                                  Test loss                                   Test metric
0         [1.02e+00, 5.53e-01, 1.56e+01, 5.78e-01]    [1.02e+00, 5.53e-01, 1.56e+01, 5.78e-01]    []  
c
 tensor([[     8.908,     -0.000,      0.000],
        [    -0.000,     11.303,     -0.000],
        [     0.000,     -0.000,      2.796]], grad_fn=<MulBackward0>)
a max
 tensor([[    2.985,     0.000,     0.004],
        [    0.000,     3.362,     0.000],
        [    0.004,     0.000,     1.672]], grad_fn=<AddBackward0>)
ysig max
 tensor([[ 8.960, -3.345,  1.258],
        [-3.345, 11.369, -0.571],
        [ 1.258, -0.571,  2.813]], grad_fn=<AddBackward0>)
b max
 tensor(10.086, grad_fn=<AddBackward0>)
ym
 tensor([-0.034,  1.576, -3.068], grad_fn=<CatBackward0>)
/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in SqrtBackward0. Traceback of forward call that caused the error:
  File "/usr/local/home/cyan3/Dev/jim/gradschool/231/research/./wass_3d_torch_scratch1.py", line 441, in <module>
    losshistory, train_state = model.train(
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 583, in train
    self._train_sgd(iterations, display_every)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 600, in _train_sgd
    self._train_step(
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 504, in _train_step
    self.train_step(inputs, targets)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 339, in train_step
    self.opt.step(closure)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 333, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 301, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 286, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/data/pde.py", line 183, in losses
    losses.append(loss_fn[len(error_f) + i](
  File "/usr/local/home/cyan3/Dev/jim/gradschool/231/research/./wass_3d_torch_scratch1.py", line 294, in rho0_WASS_cuda0
    a = torch.sqrt(c)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/autograd/python_anomaly_mode.cpp:102.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/usr/local/home/cyan3/Dev/jim/gradschool/231/research/./wass_3d_torch_scratch1.py", line 441, in <module>
    losshistory, train_state = model.train(
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 583, in train
    self._train_sgd(iterations, display_every)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 600, in _train_sgd
    self._train_step(
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 504, in _train_step
    self.train_step(inputs, targets)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 339, in train_step
    self.opt.step(closure)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/model.py", line 336, in closure
    total_loss.backward()
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'SqrtBackward0' returned nan values in its 0th output.

now you are using the statistical wass definition, but running into gradient issues, so you need REGULARIZATION?

#########################################################

tensorflow v1 / v2 cannot compute wass-opt loss with scipy without
    popping tensors out of the graph, eval, then putting back into graph, but you lose the gradient if you do this, at least i could not accomplish, with tf.py_func
    so then we switch to pytorch

then i realize a mathematical problem in the implementation: we are only training for the diagonal cells in the 3d grid for boundary conditions, so we change to meshgrid
    fixed issues with using 1D wass distance c table / vector
    fixed issues with normalization the pdf by 1D x instead of 3d normalization

but then the meshgrid means the wass-opt loss is too expensive to compute, for N=10, M=10**3, C=10**6, A=2*10**9
2 billion, and then there is underlying gradient memory
this blows up the nasa machines, so 
    memory issues when training with N=10, for cvxpylayers for optimization-based wass distance, so switch to statistical based wass distance, which requires computing differentiable metrics mu and covariance for pinn output

run into issues of backprop gradient = nan when you take sqrt of degenerate covariance , sqrt(0), so need to regularize / hybrid train
    try regularization, doesn't really help
    realize torch.sqrt of matrix does element-wise sqrt, so instead of approximate matrix sqrt from library, starts training

#########################################################

Hi Abhishek

In a prior meeting I remember we discussed we should normalize the PINN output to a PDF, and then convert it to a PMF to compute the wasserstein.
I understood this as we should not penalize the PINN output if it was not a PDF, but turn it into a PDF ourselves.
I also understood then that if we are normalizing the output ourselves to a PMF to get the optimization-based wass, I could swap the optimization-based wass with the statistics-based wass.

It is correct that our ultimate goal is to get the PINN to:
1. natively output a PDF
2. s.t. the PDF normalized to a PMF is wass close to the rho0 / rhoT PMFs.

This result shows the PINN, when normalized (made non-negative, sum=1), achieves goal 2., but not natively, so not 1.


Here is the pipeline for this result:
1. normalize the PINN output to a PDF
(0ing out negative values, dividing by trapz integral) 
2. then computing the wasserstein distance
(normalize to PMF, then solving lin prog)

What I did here was to swap step 2 from optimization-based to statistics-based because the optimization-based wasserstein loss is not computable on the machines.
It is definitely true that very often the PINN output, when artificially normalized to a PMF, is degenerate in the sense the covariance matrix is 0, and I am using an approximate matrix square root function.
This result shows the statistics-based wasserstein-1 distance steers the output (normalized as PMF) towards the desired shape



So in the loss function I normalize it to a PDF explicitly, and then normalize it to PMF to find wass loss.
What I have done here is replace the way the loss is found, I thought I could because we are already artificially normalizing the output to a PMF anyway when solving the optimization-based wass loss.

But are you saying that the PINN output should be:
1. penalized / taught into outputting a PDF
2. only normalized artificially into a PMF

#########################################################

ipdb> x.shape
torch.Size([16000, 70])
ipdb> self.linears[:-1]
ModuleList(
  (0): Linear(in_features=4, out_features=70, bias=True)
  (1): Linear(in_features=70, out_features=70, bias=True)
  (2): Linear(in_features=70, out_features=70, bias=True)
)

here x is between 0 and 1

> /usr/local/home/cyan3/miniforge/envs/tf/lib/python3.9/site-packages/deepxde/nn/pytorch/fnn.py(35)forward()
     34         x = self.linears[-1](x)
---> 35         if self._output_transform is not None:
     36             x = self._output_transform(inputs, x)

ipdb> x
tensor([[-0.150,  1.082],
        [-0.149,  1.083],
        [-0.148,  1.083],
        ...,
        [-0.203,  1.054],
        [-0.207,  1.045],
        [-0.194,  1.074]], grad_fn=<AddmmBackward0>)

ipdb> self.linears[-1]
Linear(in_features=70, out_features=2, bias=True)

now x has negative terms

* the weights of the last layer of the nn can be anything, so a sigmoid activation in the hidden layer + negative weights => negative output

last layer weights have shape:

ipdb> x[5].weight.shape
torch.Size([2, 70])

* should i implement 'pdf output' as an '_output_transform'?
* should i implement a CLAMP on the last layer weights so that, with sigmoid activation, and non-negative last-layer weights on the boundary conditions?
  would this overfit the problem?

tmp1 = x, 16000 x 70
tmp2 = self.linears[-1].weight.data, 2 x 70
output = torch.mm(tmp2, torch.t(tmp1)), 2 x 16000

tmp3 = torch.t(torch.mm(tmp2, torch.t(tmp1))), 16000 x 2

self.linears[-1].weight.clamp_min(0.0)

clamped_weights = self.linears[-1].weight.clamp(0.0, 1.0)
self.linears[-1].weight.data = clamped_weights

clamped_weight = self.linears[-1].weight[1, :].clamp(0.0, 1.0)
self.linears[-1].weight.data[1, :] = clamped_weight

#########################################################

sinkhorn-divergence
wass dist

sinkhorn recursion

5-10 convergence iterations
gamma=10e-1,10e-2
lowest=0.05

regularization:
given exact problem, solve it approximately
change it to an approximate problem, solve it exactly

#########################################################

talk to ece, withdraw LOA for winter have abhishek cover tuition if possible

#########################################################

vs01:
sigmoid, not batched, d=2, + p2

Best model at step 94993:
  train loss: 4.99e-02
  test loss: 4.99e-02
  test metric: []

'train' took 54145.762730 s

Saving loss history to /usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/loss.dat ...
Saving training data to /usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/train.dat ...
Saving test data to /usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/test.dat ...
/usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/wass_3d_model-100000.pt

vs08:
sigmoid, batched, d=3, + p2

vs04:
sigmoid, batched, d=3, no p2

batch=500 is the max you can do with d=3, N=15

concerns:
sigmoid activation causes 'zero-solution' to the planck equation, it starts out e-5, so it is not correct

does batching preserve outcome correctness?

does sigmoid preserve correctness?

###############################################

vs07:
tanh, batched, d=3, p1+p2

#########################################################

run in ipython, so you can CONTINUE training after you finish if it's not good enough

#########################################################

torch.matmul(t, s)
torch.einsum('ij, j->i', t, s)

torch.matmul(t.t(), s)
torch.einsum('ji, j->i', t, s)

#########################################################

vs08:
sigmoid, batched, d=3, + p2
HEALTHY

vs04:
sigmoid, batched, d=3, no p2
STUCK, STOPPED

vs07:
tanh, batched, d=3, p1+p2
STUCK

vs01:
sigmoid, batched 750, d=3, dist + p2
STUCK, STOPPED

#########################################################

stop normalizing into a pmf, maybe that's why it gets 'stuck'
because the sum could be very high, so pmf is very low?

normalize p1, p2 by numel in y_pred, to not overwhelm sinkhorn dist

#########################################################

tensor(0., grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
33        [5.38e-02, 8.11e-01, 4.85e+00, 1.03e+01] 

tanh / negative pinn output getting 'stuck'
also see in sigmoid output, getting 'stuck' with all 0 output?

45        [5.42e-02, 3.59e-01, 3.58e+00, 5.01e+02]    [5.42e-02, 3.59e-01, 3.58e+00, 5.01
e+02]    []
tensor(19.070, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.531, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.531, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
46        [5.05e-02, 3.29e-01, 3.57e+00, 5.01e+02]    [5.05e-02, 3.29e-01, 3.57e+00, 5.01
e+02]    []
tensor(18.531, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.319, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.319, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
47        [4.77e-02, 3.04e-01, 3.56e+00, 5.01e+02]    [4.77e-02, 3.04e-01, 3.56e+00, 5.01
e+02]    []
tensor(18.319, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.892, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(18.892, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)


for tanh, what happens is
the output starts high
is penalized for not summing to 1
so it drops
and then hits negative output
and then the loss zero's the output to 0
and the gradient is based on the neg count + diff to 1

but i think the diff to 1 operation masks the problem
so it gets stuck on a negative output

and the summing of the negative count, nor the diff to 1 is a good gradient to tell pinn to 'multiply -1' the outputs
instead, 

y_pred_proxy = torch.min(y_pred)

y_pred = torch.where(y_pred < 0, 0, y_pred)

s = torch.sum(y_pred)
p2 = torch.abs(s - 1)

if s < 1e-3:
    # mostly negative, then do not compute
    # wass distance
    # to avoid getting 'stuck' in 0 gradient
    
    # this is BETTER than checking if max is negative
    # because pinn could output a very small
    # positive value, and overcome that case
    if y_pred_proxy < 0.0:
        return -y_pred_proxy
    else:
        # for the sigmoid case where output is always non-negative
        # this steers the pinn to outputing a higher sum
        return 1 - y_pred_proxy

40        [5.23e-02, 5.94e-01, 4.05e+00, 9.17e-02]    [5.23e-02, 5.94e-01, 4.05e+00, 9.17e-02]    []
tensor(22.070, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(23.961, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(23.961, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
41        [5.13e-02, 5.90e-01, 3.96e+00, 2.05e-02]    [5.13e-02, 5.90e-01, 3.96e+00, 2.05e-02]    []
tensor(23.961, grad_fn=<SumBackward0>)
tensor(0., grad_fn=<SumBackward0>)
tensor(26.096, grad_fn=<SumBackward0>)
tensor(0.134, grad_fn=<SumBackward0>)
tensor(26.096, grad_fn=<SumBackward0>)
tensor(0.134, grad_fn=<SumBackward0>)
42        [5.03e-02, 5.93e-01, 3.85e+00, 3.07e+01]    [5.03e-02, 5.93e-01, 3.85e+00, 3.07e+01]    []
tensor(26.096, grad_fn=<SumBackward0>)
tensor(0.134, grad_fn=<SumBackward0>)
tensor(24.972, grad_fn=<SumBackward0>)
tensor(5.837, grad_fn=<SumBackward0>)
tensor(24.972, grad_fn=<SumBackward0>)
tensor(5.837, grad_fn=<SumBackward0>)
43        [4.91e-02, 6.26e-01, 3.81e+00, 2.37e+01]    [4.91e-02, 6.26e-01, 3.81e+00, 2.37e+01]    []
tensor(24.972, grad_fn=<SumBackward0>)
tensor(5.837, grad_fn=<SumBackward0>)
tensor(23.701, grad_fn=<SumBackward0>)
tensor(31.979, grad_fn=<SumBackward0>)
tensor(23.701, grad_fn=<SumBackward0>)
tensor(31.979, grad_fn=<SumBackward0>)

#########################################################

vs07:
tanh, batched 500, d=3

vs01:
sigmoid, batched 750, d=3

#########################################################

27343     [1.98e-09, 3.47e-04, 1.04e+00, 1.02e+00]    [1.98e-09, 3.47e-04, 1.04e+00, 1.02
e+00]    []
27344     [1.98e-09, 3.50e-04, 1.04e+00, 1.03e+00]    [1.98e-09, 3.50e-04, 1.04e+00, 1.03
e+00]    []
27345     [1.94e-09, 3.39e-04, 1.05e+00, 1.03e+00]    [1.94e-09, 3.39e-04, 1.05e+00, 1.03
e+00]    []
27346     [1.93e-09, 3.42e-04, 1.04e+00, 1.03e+00]    [1.93e-09, 3.42e-04, 1.04e+00, 1.03
e+00]    []
27347     [1.98e-09, 3.54e-04, 1.05e+00, 1.03e+00]    [1.98e-09, 3.54e-04, 1.05e+00, 1.03
e+00]    []
27348     [2.02e-09, 3.55e-04, 1.05e+00, 1.03e+00]    [2.02e-09, 3.55e-04, 1.05e+00, 1.03
e+00]    []
27349     [1.92e-09, 3.45e-04, 1.04e+00, 1.03e+00]    [1.92e-09, 3.45e-04, 1.04e+00, 1.03
e+00]    []

but you see, the pinn if punished for the MINIMUM element
will exactly tune just that one element? so it gets only slightly less stuck, but it is still stuck

the correction /gradient must be global

#########################################################

sigmoid:
weights are dialed down but can be dialed TOO LOW

so the sum ~= 0, and gradients are very small, so it gets STUCK


#########################################################

41019     [7.83e-07, 9.54e-05, 1.14e+00, 9.53e-01]    [7.83e-07, 9.54e-05, 1.14e+00, 9.53e-01]    []  
41020     [7.83e-07, 9.54e-05, 1.14e+00, 9.53e-01]    [7.83e-07, 9.54e-05, 1.14e+00, 9.53e-01]    []  
41021     [7.83e-07, 9.53e-05, 1.14e+00, 9.53e-01]    [7.83e-07, 9.53e-05, 1.14e+00, 9.53e-01]    []  
41022     [7.83e-07, 9.52e-05, 1.14e+00, 9.53e-01]    [7.83e-07, 9.52e-05, 1.14e+00, 9.53e-01]    []  

#####################################################

scp cyan3@vipersim01.ndc.nasa.gov:/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/wass_3d_model-21717* .

./plot_bc.py --testdat ./wass_3d_model-60970-60970.dat --modelpt ./wass_3d_model-60970.pt --diff_on_cpu 0 --fullstate 0

./plot_bc.py --testdat ./wass_3d_model-21717-21717.dat --modelpt ./wass_3d_model-21717.pt --diff_on_cpu 0

./distribution0.py --control_data ./wass_3d_model-60970_3375_all_control_data.npy

/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/wass_3d_model-79422

vs08: wrapping up a good thing, lucky

vs01: STUCK!, cancelled
vs01: training on T_t = 5

vs04: T_t = 5 training

scorpius: grid interpolated linearly the dphi_dx/y/z and integrating with 30 time samples

if this does NOT work, it means:

sigmoid activation and clamped last layer weights for the shroedinger equation is NOT correct
or
what???

RHO_0 sum 5.781647
RHO_T sum 12.583499

#########################################################

distribution forced system NOT working
    linear interpolation for dphi_dx/y/z
    grid lookup by l2 norm argmin
    either 3 axis lookup or 4 axis

    tried nearest interpolation
    autodiff on cpu, cuda
    doubling the interp resolution N=15, N=30

    integration from T_0 to T_t (20s) with 30 time samples

    concern 1 (training):
    RHO_0 sums to 6
    RHO_T sums to 12
    natively, they are not PMFs

    concern 2 (math):
    sigmoid activation, clmaped last-layer weights
    so not correctly solving the planck equation?

    concern 3 (training):
    maybe q = 0.0 is an issue?

    concern 4 (numerics):
    maybe i'm not integrating finely enough?

    not clear what's wrong

but running plot_bc with fullstate or batched shows similar distributions, so batching preserves correctness

#########################################################

vs08 finishes:

Best model at step 96420:
  train loss: 1.16e-02
  test loss: 1.16e-02
  test metric: []

'train' took 210666.569459 s

scp cyan3@vipersim08.ndc.nasa.gov:/home/cyan3/gradschool/research/wass_3d/strategy5/wass_3d_model-99505* .
scp cyan3@vipersim08.ndc.nasa.gov:/home/cyan3/gradschool/research/wass_3d/strategy5/loss.dat .
scp cyan3@vipersim08.ndc.nasa.gov:/home/cyan3/gradschool/research/wass_3d/strategy5/test.dat .
scp cyan3@vipersim08.ndc.nasa.gov:/home/cyan3/gradschool/research/wass_3d/strategy5/train.dat .

../../../plot_loss_func.py --lossdat ./loss.dat --modelpt ./wass_3d_model-99505.pt

./plot_bc.py --testdat ./wass_3d_model-99505-99505.dat --modelpt ./wass_3d_model-99505.pt --diff_on_cpu 0 --grid_n 30

./distribution0.py --control_data ./wass_3d_model-99505_3375_0_linear_30_all_control_data.npy

#########################################################

./distribution0.py --control_data ./wass_2d_model-99875_225_0_linear_30_all_control_data.npy

#########################################################

scp cyan3@vipersim08.ndc.nasa.gov:/usr/local/home/cyan3/gradschool/research/wass_3d/strategy5/sigmoid_full_d2_p2/wass_3d_model-35636* .

Epoch 35636: train loss improved from 7.54e-02 to 7.49e-02, x, y_pred

./plot_bc.py --testdat ./wass_2d_model_t5-35636-35636.dat --modelpt ./wass_2d_model_t5-35636.pt --diff_on_cpu 0 --grid_n 45 --interp_mode linear

./distribution0.py --control_data ./wass_2d_model_t5-35636_225_0_linear_45_5_all_control_data.npy

#########################################################

Epoch 67844: train loss improved from 4.71e-02 to 4.60e-02, x, y_pred

/usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/sigmoid_batched500_d3_p2/wass_3d_model-67844.pt

scp cyan3@vipersim01.ndc.nasa.gov:/usr/local/home/cyan3/Desktop/gradschool/research/wass_3d/strategy5/sigmoid_batched500_d3_p2/wass_3d_model-67844* .

./plot_bc.py --testdat ./t5_q0/wass_3d_model-67844-67844.dat --modelpt ./t5_q0/wass_3d_model-67844.pt --diff_on_cpu 0 --grid_n 30

not good!

#########################################################
#########################################################

vs08: d=2, T_t = 5, q=0.5
run07
Epoch 96656: train loss improved from 2.80e-02 to 2.78e-02, x, y_pred
../../../../plot_loss_func.py --lossdat ./loss.dat --modelpt ./wass_2d_model-96656.pt
/usr/local/home/cyan3/Dev/jim/gradschool/research/wass_3d/strategy5/sigmoid_full_d2_p2$ ./plot_bc.py --testdat ./run07/wass_2d_model-96656-96656.dat --modelpt ./run07/wass_2d_model-96656.pt --diff_on_cpu 0 --grid_n 45 --interp_mode linear
as expected, the control is smaller, but it is already too small

vs04: d=3, T_t = 5, q=0.5
run06

vs01: d=3, T_t = 5, q=0
run05

#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################
#########################################################

i had a dream where i was searching online and i found a paper that does exactly what my research is doing, and did it

i then sought out the author, a woman, and she was so helpful explaining to me how it works and showing me this and that about it and i'm not sure how to react

it was exactly what i had been solving for. how do i take this? is my investment then just completely worthless? but if it's about the problem, shouldn't this be a good thing?

should i tell my faculty advisor? what if he dismisses me then? should i bring this up to iman, my senpai?

in math, 'what' IS 'what'?
as in, the 'wass distance' IS 'the solution to this optimization problem'
or the 'psuedo-inverse' IS the 'convex optimization solution'
that a number is the sum of a sequence
that some result of an operation is a variable

its a discontinuous partition:
  the people who are most like you exist, just not near you

the truth blinking in and out
  it is true only to you, and you glimpse it, and you can't speak to it because it falls out of scope right away

what you are doing when you 'machine-learn'

you compile a graph (net + loss functions) that supports auto-diff (symbolic gradients)
this is loss = f(theta)(x) where f(theta) is the graph mapping input to loss
where analytical differentiation is not possible and numerical diff is meaningless

then you forward propagate the input to an output and further out into losses
all of this while the machine is 'watching' the symoblic forward prop
then the machine auto-diff's out the d(loss)/d(theta)
and then the optimizer steps along the gradient to find some minimum global / local

a Tensor is an interesting entity
it is a matrix of some shape that is also a node in a graph
it is a matrix whose elements are symbolic functions

#########################################################

abhishek questions:

math for kids vs math for adults? is everything math?

how useful is math to reality? or is the math dept the best place to find 'useful' math?  and how do you feel when math is or is not 'useful'?
pagerank, k-space, etc. gradient descent, iterative methods, computational geometery

how do you shape your curiosity? do you have any direction you want to go in, and what shaped that direction? was it related to your background at all?

how do you come across new maths? like wasserstein distance (a very useful tool for wasserstein). can you come across useful math without devoting full-time to it?

#########################################################

import cvxpy as cp
import tensorflow as tf
from cvxpylayers.tensorflow import CvxpyLayer

print(cvector.shape)

c_shape_half = int(cvector.shape[0]/2)

# Define and solve the CVXPY problem.
x1 = cp.Variable(
    c_shape_half,
    nonneg=True
)
x2 = cp.Variable(
    c_shape_half,
    nonneg=True
)


cvector1 = cvector[:c_shape_half, 0]
cvector2 = cvector[c_shape_half:, 0]


A1 = A[:N, :]
A2 = A[N:, :]


pred = cp.Parameter((A1.shape[0],))



problem = cp.Problem(
    cp.Minimize(cvector1.T @ x1 + cvector2.T @ x2),
    [
        A1 @ x1 == pred,
        A2 @ x2 == rho_0,
    ],
)
assert problem.is_dpp()
print(problem.parameters())

'''

cvxpylayer = CvxpyLayer(
    problem,
    parameters=[pred],
    variables=[x])

rho_I=pdf1d(x_T, 4.0, 1.0).reshape(len(x_T),1)
rho_I = np.where(rho_I < 0, 0, rho_I)
rho_I = rho_I / np.sum(np.abs(rho_I))

y_pred = tf.Variable(tf.zeros((100,), tf.double))
y_pred = tf.constant(rho_I, shape=(100,))
rho_0_tf = tf.constant(rho_0, shape=(100,))

with tf.GradientTape() as tape:
  # solve the problem, setting the values of A, b to A_tf, b_tf
  x_sol, = cvxpylayer(y_pred)
# compute the gradient of the summed solution with respect to A, b
grad_ypred = tape.gradient(x_sol, [y_pred])

print("x_sol")
print(x_sol)

print(cvector.T @ x_sol.numpy())

print("grad_ypred")
print(grad_ypred)

print("problem.value", problem.value)
'''

https://github.com/lululxvi/deepxde
https://github.com/tensorly/tensorly
https://github.com/davidshimjs/qrcodejs
https://lifewithjazz.com/wp-content/uploads/2022/08/image-13.png
https://lifewithjazz.com/wp-content/uploads/2022/08/image-10.png
https://en.wikipedia.org/wiki/Automatic_differentiation
https://www.google.com/search?q=laplace+matrix+exponential&oq=laplace+matrix+exponential&aqs=chrome..69i57.3571j0j7&sourceid=chrome&ie=UTF-8
https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2047675.m570.l1313&_nkw=camera+fpv+transmitter+receiver&_sacat=0
https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2047675.m570.l1313&_nkw=tvl+camera+usb&_sacat=0
https://www.ebay.com/itm/323963747822?hash=item4b6dbe7dee:g:XkkAAOSw5VFWLlLh&amdata=enc%3AAQAHAAAA4NngUPFZftRc0mnyoHa9X13Ufso4a%2FRm2jY%2BPuA5DYOftmCpornMKJu8VddZ8bZz9FdwU84C5OvsjCEtHy6Yaa0YvRoUuOnAE43ZrPBm1UKuN5Te%2Bny8IAJ69cgMcl%2FaQDmrB%2FnnEmG%2FFGr8deFaBXuTFp0QSxEGOAqNE8d%2BhP8M7ijjwdsnDGkLQhP%2FfklSh0dgHVVR%2Bv4HL9YTqoQ3%2FD6ffqemOjbBqEbF%2B3nfM2%2F%2FmSA0AhblalV8OqxYzYqszIA244vz7rcpLNnimxzf2Cv6BJk1m02k2CHQL0nsu4pQ%7Ctkp%3ABk9SR472w5rpYA
https://www.google.com/search?q=skydroid+linux+github&oq=skydroid+linux+github&aqs=chrome..69i57.5936j0j7&sourceid=chrome&ie=UTF-8
https://forum.bitcraze.io/viewtopic.php?t=3966
https://leetcode.com/
https://www.youtube.com/watch?v=zQ_YG2HQe54
https://www.youtube.com/watch?v=f4jUEEzjEJw
https://www.youtube.com/watch?v=aPQY__2H3tE
https://www.youtube.com/watch?v=H9bfqozjoqs
https://www.youtube.com/watch?v=Y0lT9Fck7qI
https://www.youtube.com/watch?v=kB9YyG2V-nA
https://github.com/Jonas-Nicodemus/PINNs-based-MPC
https://github.com/do-mpc/do-mpc
https://github.com/AtsushiSakai/PyAdvancedControl
https://github.com/locuslab/mpc.pytorch
https://github.com/gasagna/mpc
https://github.com/Shunichi09/PythonLinearNonlinearControl
https://github.com/vkotaru/nonlinear_controls
https://github.com/anassinator/ilqr
https://github.com/mmmfarrell/nmpc_quadcopter
https://github.com/TylerReimer13/6DOF_Quadcopter_MPC
https://stackoverflow.com/questions/6939864/what-is-the-difference-between-section-and-div#:~:text=The%20tag%20defines%20sections,to%20format%20them%20with%20CSS.
https://www.techrepublic.com/article/how-to-easily-add-an-ssh-fingerprint-to-your-knownhosts-file-in-linux/
https://serverfault.com/questions/132970/can-i-automatically-add-a-new-host-to-known-hosts
https://phoenixnap.com/kb/ssh-permission-denied-publickey
https://github.com/tanersener/mobile-ffmpeg
https://www.avrfreaks.net/forum/pcb-hole-size-254mm-pin-headers-soldering-purpose
https://docs.google.com/spreadsheets/d/1MG4PqBMiFMIlG9CeMuyddmCjwkTpC23EEIAEkpXbL8A/edit#gid=2038720634
https://web.whatsapp.com/
https://www.google.com/search?q=8**3&oq=8**3&aqs=chrome..69i57.775j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=np.eye+sparse&oq=np.eye+sparse&aqs=chrome..69i57.1975j0j7&sourceid=chrome&ie=UTF-8



https://numpy.org/doc/stable/reference/generated/numpy.kron.html
https://www.google.com/search?q=numpy+sparse+matrix&oq=numpy+sparse+matrix&aqs=chrome..69i57.4414j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=sparse+wasserstein+distance&oq=sparse+wasserstein+distance&aqs=chrome..69i57.3494j0j7&sourceid=chrome&ie=UTF-8
http://proceedings.mlr.press/v84/blondel18a/blondel18a.pdf
https://kejing.me/publications/2021_AAAI_SWIFT_slides.pdf
https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Computing_Wasserstein-p_Distance_Between_Images_With_Linear_Cost_CVPR_2022_paper.pdf
https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html
https://www.google.com/search?q=github+wasserstein+tensor&sxsrf=ALiCzsawploZpsBRJxEjnqSBtTz5K5QuvQ%3A1663543707221&ei=m6knY-qSDcTEkPIPo7-p2AQ&ved=0ahUKEwiqzPKdv5_6AhVEIkQIHaNfCksQ4dUDCA4&uact=5&oq=github+wasserstein+tensor&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgAToKCAAQRxDWBBCwAzoECCMQJzoECAAQQzoKCC4QxwEQ0QMQQzoFCAAQkQI6CwgAEIAEELEDEIMBOhAILhCxAxCDARDHARDRAxBDOg0ILhCxAxDHARDRAxBDOg4ILhCABBCxAxCDARDUAjoNCAAQgAQQhwIQsQMQFDoKCAAQgAQQhwIQFDoHCAAQsQMQQzoOCC4QgAQQsQMQxwEQ0QM6CAgAEIAEELEDOgoIABCxAxCDARBDOggIABCxAxCRAjoFCAAQgAQ6BggAEB4QFjoICAAQHhAPEBY6BQgAEIYDOgUIIRCrAkoECEEYAEoECEYYAFDpCFieJmDWKGgEcAB4AIABgAGIAboTkgEEMTguOJgBAKABAcgBCMABAQ&sclient=gws-wiz
https://github.com/zsteve/wtf
https://github.com/t-vi/pytorch-tvmisc/blob/master/wasserstein-distance/Pytorch_Wasserstein.ipynb
https://github.com/zsteve/wtf/blob/main/src/wtf.py
https://github.com/google/wasserstein-dist
https://github.com/topolearn/topo-clustering
https://math.stackexchange.com/questions/3660192/algorithms-to-compute-the-wasserstein-distance-between-two-discrete-probability
https://arxiv.org/pdf/1803.00567.pdf
https://www.google.com/search?q=sliced+wasserstein+distance&oq=sliced+wasserstein+distance&aqs=chrome..69i57.3423j0j7&sourceid=chrome&ie=UTF-8
https://proceedings.neurips.cc/paper/2019/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf
https://openaccess.thecvf.com/content_cvpr_2018/papers/Kolouri_Sliced_Wasserstein_Distance_CVPR_2018_paper.pdf
https://tel.archives-ouvertes.fr/tel-03533097/document
https://www.google.com/search?q=sliced+wasserstein+distance+python+github&sxsrf=ALiCzsbvEQW6AhMPL44Y9hr6DFFXB-aGfw%3A1663553323497&ei=K88nY4P_HaOJ0PEPpuSgsAM&ved=0ahUKEwjDwKWH45_6AhWjBDQIHSYyCDYQ4dUDCA4&uact=5&oq=sliced+wasserstein+distance+python+github&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKsCMgUIIRCrAjoKCAAQRxDWBBCwAzoHCAAQsAMQQzoECCMQJzoFCAAQgAQ6BAgAEEM6BggAEB4QFjoFCAAQhgM6BQghEKABOggIIRAeEBYQHUoECEEYAEoECEYYAFCVC1jiGmDxG2gCcAF4AIABqAGIAd0LkgEDNi44mAEAoAEByAEKwAEB&sclient=gws-wiz
https://github.com/koshian2/swd-pytorch
https://github.com/FlorentinCDX/Fast-Approx-SW
https://arxiv.org/pdf/2106.15427.pdf
https://blog.shikoan.com/swd-pytorch/#SWD%E3%81%A8%E3%81%AF
https://pythonot.github.io/auto_examples/sliced-wasserstein/plot_variance.html
https://tel.archives-ouvertes.fr/tel-03533097/document
https://github.com/krumo/swd_pytorch
https://github.com/apple/ml-cvpr2019-swd
https://arxiv.org/abs/1903.04064
https://github.com/skolouri/swgmm/blob/master/SWM_GMM_RMSProp_Demo_GitHub.ipynb
https://github.com/martinarjovsky/WassersteinGAN
https://github.com/caogang/wgan-gp/tree/master/results/cifar10
https://dfdazac.github.io/blog/
https://github.com/vincentherrmann/wasserstein-notebook/blob/master/Wasserstein_Kantorovich.ipynb
https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2
https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html
https://www.tutorialspoint.com/how-to-compute-gradients-in-pytorch#:~:text=To%20compute%20the%20gradients%2C%20a,a%20tensor%20with%20requires_grad%20%3D%20True.
https://pythonot.github.io/auto_examples/sliced-wasserstein/plot_variance.html
https://link.springer.com/article/10.1007/s10851-014-0506-3


https://arxiv.org/pdf/2009.14075.pdf
https://www.stat.uchicago.edu/~lekheng/work/probdist.pdf
https://ftp.deas.harvard.edu/techreports/tr-02-13.pdf
https://people.csail.mit.edu/jsolomon/assets/convolutional_w2.compressed.pdf
https://ftp.deas.harvard.edu/techreports/tr-02-13.pdf
https://github.com/rsinghlab/SCOT
https://gist.github.com/robintibor/63355773e08e2c98f6465b1613e3d45b
https://gist.github.com/jmsquare/3f8a988055e47ea10ffa57530d1ec076
https://gist.github.com/jmsquare/3f8a988055e47ea10ffa57530d1ec076
https://gist.github.com/karhunenloeve/293a0ab760c458911c89089cfc24f563
https://gist.github.com/cristina-esposito/7167b79c92399cc9bdf198d9a009ab7c
https://gist.github.com/binvec/22efc7c113a75ca5bd060fe9897e488e
https://gist.github.com/Officium/0c636d33f6f258aa76b101ef8f8526e7
https://github.com/FlorentinCDX/Fast-Approx-SW/issues/1
https://github.com/kimiandj/slicedwass_abc
https://github.com/dorianHe/wasserstein_distance
https://github.com/scipy/scipy/issues/13014
https://github.com/tirthajyoti/Stats-Maths-with-Python/blob/master/Prob_Distributions_Discrete.ipynb
https://github.com/Lornatang/WassersteinGAN-PyTorch
https://github.com/topics/wasserstein-distance-estimation
https://github.com/topics/wasserstein-metric
https://github.com/scikit-tda/pervect
https://gist.github.com/sethaxen/6408d7f104ba44abfbb74981f8a8deb8
https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/
https://juliastats.org/Distributions.jl/stable/multivariate/
https://florentincdx.github.io/#about
https://arxiv.org/pdf/2106.15427.pdf
https://www.tutorialspoint.com/how-to-compute-gradients-in-pytorch#:~:text=To%20compute%20the%20gradients%2C%20a,a%20tensor%20with%20requires_grad%20%3D%20True.
https://pytorch.org/docs/stable/generated/torch.rand.html
https://stackoverflow.com/questions/70794323/python-scipy-genarate-random-samples-from-custom-multivariate-probability-d
https://stats.stackexchange.com/questions/70855/generating-random-variables-from-a-mixture-of-normal-distributions
https://stackoverflow.com/questions/49106806/how-to-do-a-simple-gaussian-mixture-sampling-and-pdf-plotting-with-numpy-scipy
https://www.google.com/search?q=github+multivariate+wasserstein+distance+torch&biw=2048&bih=1048&sxsrf=ALiCzsYORNpLyJrwl__B_lis4-wQWIb5ew%3A1663631897472&ei=GQIpY5SrHMqHqtsPsLmN-Ao&ved=0ahUKEwjUm6Tih6L6AhXKg2oFHbBcA68Q4dUDCA4&uact=5&oq=github+multivariate+wasserstein+distance+torch&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgATIFCCEQoAE6CggAEEcQ1gQQsAM6BQghEKsCSgQIQRgASgQIRhgAUOUEWKIKYIkLaAFwAXgAgAHuAYgB2gaSAQUwLjEuM5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://pythonot.github.io/auto_examples/plot_compute_emd.html#sphx-glr-auto-examples-plot-compute-emd-py
https://stackoverflow.com/questions/11373192/generating-discrete-random-variables-with-specified-weights-using-scipy-or-numpy
https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html
https://dfdazac.github.io/sinkhorn.html
http://www.kernel-operations.io/geomloss/index.html
https://openreview.net/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf
https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/
https://www.researchgate.net/publication/227100230_The_Wasserstein_Distance_and_Approximation_Theorems/link/53d10bd60cf2f7e53cfbc2bc/download
file:///home/cyan3/Desktop/Wassersteindistance.pdf
file:///home/cyan3/Desktop/Multivariate_stable_approximation_in_Wasserstein_d.pdf
https://www.youtube.com/watch?v=52iS58l1GJ0
https://www.youtube.com/watch?v=chxwRC9tSSc
https://www.youtube.com/watch?v=L2ywWG4BD-8
https://www.google.com/search?q=multivariate+pdf+find+mean&sxsrf=ALiCzsagw1Qe6QfmN5tnUF7vikTgUvkoeg%3A1663642875562&ei=-ywpY4v0IcqwqtsP_Iq4mA4&ved=0ahUKEwjL3YXVsKL6AhVKmGoFHXwFDuMQ4dUDCA4&uact=5&oq=multivariate+pdf+find+mean&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgAToKCAAQRxDWBBCwAzoECAAQQzoFCAAQkQI6DgguEIAEELEDEMcBEK8BOggILhCABBCxAzoKCAAQsQMQgwEQQzoECCMQJzoNCAAQsQMQgwEQQxCLAzoOCAAQgAQQsQMQgwEQiwM6CwgAEIAEELEDEIMBOgsIABCxAxCDARCRAjoKCAAQgAQQhwIQFDoFCAAQgAQ6BggAEB4QFjoICAAQHhAPEBY6CAgAEB4QFhAKOgUIABCGAzoFCCEQqwI6CAghEB4QFhAdSgQIQRgASgQIRhgAULAcWO01YPk2aANwAXgCgAHhA4gB7BmSAQoxNy41LjEuMC4ymAEAoAEByAEIuAECwAEB&sclient=gws-wiz
https://www.math.hkbu.edu.hk/~hpeng/Math3806/Lecture_note3.pdf
https://stats.stackexchange.com/questions/297141/finding-mean-and-covariance-of-an-arbitrary-joint-pdf
https://openstax.org/books/statistics/pages/4-2-mean-or-expected-value-and-standard-deviation#:~:text=NOTE,%E2%88%91%20x%20P%20(%20x%20)%20.
https://math.stackexchange.com/questions/526434/solving-for-the-covariance-of-a-joint-pdf
https://stats.stackexchange.com/questions/490069/what-is-the-intuitive-difference-between-wasserstein-1-distance-and-wasserstein
https://stats.stackexchange.com/questions/404775/calculate-earth-movers-distance-for-two-grayscale-images
https://stats.stackexchange.com/questions/133369/the-total-area-underneath-a-probability-density-function-is-1-relative-to-wh
https://stats.stackexchange.com/questions/71036/test-if-multidimensional-distributions-are-the-same
https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg
https://datascience.stackexchange.com/questions/108921/does-sliced-wasserstein-distance-work-in-higher-than-2-dimensions

https://towardsdatascience.com/how-to-embed-your-julia-code-into-python-to-speed-up-performance-e3ff0a94b6e
https://www.google.com/search?q=using+julia+in+python&sxsrf=ALiCzsYc7D4_PU1l5EmkwD7chQxSnWujIw%3A1663717796136&ei=pFEqY9_zB7fekPIPoYiMwAE&ved=0ahUKEwjf1frhx6T6AhU3L0QIHSEEAxgQ4dUDCA8&uact=5&oq=using+julia+in+python&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgYIABAeEBYyBggAEB4QFjIFCAAQhgM6CggAEEcQ1gQQsAM6BwgAELADEEM6DQgAEOQCENYEELADGAE6DwguENQCEMgDELADEEMYAjoECCMQJzoLCAAQgAQQsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOg4ILhCABBCxAxDHARDRAzoFCAAQkQI6BAgAEEM6CAgAEIAEELEDOgoIABCABBCHAhAUSgQIQRgASgQIRhgBUJgIWLwXYLkYaANwAXgAgAHiAYgBrBWSAQYwLjE5LjGYAQCgAQHIARHAAQHaAQYIARABGAnaAQYIAhABGAg&sclient=gws-wiz
https://pyjulia.readthedocs.io/en/latest/usage.html
https://www.google.com/search?q=wrap+julia+in+python&oq=wrap+julia+in+python&aqs=chrome..69i57.2956j0j7&sourceid=chrome&ie=UTF-8
https://pyjulia.readthedocs.io/en/latest/usage.html
https://www.juliabloggers.com/how-to-call-julia-code-from-python/
https://stackoverflow.com/questions/37200025/how-to-import-custom-module-in-julia
https://stackoverflow.com/questions/57270276/identity-matrix-in-julia
https://discourse.julialang.org/t/is-there-an-easy-way-to-get-at-the-history-of-repl-input/18025/8
https://flow.byu.edu/FLOWUnsteady/tutorials/installation-instructions/
https://blog.esciencecenter.nl/how-to-call-julia-code-from-python-8589a56a98f2
https://stackoverflow.com/questions/69872872/how-to-create-a-zeros-array-in-julia
https://docs.julialang.org/en/v1/base/arrays/
https://github.com/JuliaPy/pyjulia/issues/308
https://docs.julialang.org/en/v1/manual/functions/
https://en.wikipedia.org/wiki/Truncated_normal_distribution

https://gist.github.com/Flunzmas/6e359b118b0730ab403753dcc2a447df
https://gist.github.com/jcowles/1de8b12c38603ce932b0154bc6d59d60
https://gist.github.com/sethaxen/6408d7f104ba44abfbb74981f8a8deb8
https://gist.github.com/Flunzmas/6e359b118b0730ab403753dcc2a447df
https://gist.github.com/pratheeksh/32fa493676222252cc9774b947280397
https://github.com/dfdazac/wassdistance/blob/master/sinkhorn.ipynb

https://discuss.pytorch.org/t/gradient-value-is-nan/91663/3
https://pytorch.org/maskedtensor/main/notebooks/nan_grad.html
https://www.google.com/search?q=torch+check+matrix+positive+semi-definite&ei=1awsY7eQG7nckPIP19GB2AE&ved=0ahUKEwj3zu-Bh6n6AhU5LkQIHddoABsQ4dUDCA4&uact=5&oq=torch+check+matrix+positive+semi-definite&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEKABEAoyBwghEKABEAoyBQghEKsCOgoIABBHENYEELADOgQIABBDOgcIABBDEIsDOgoIABCxAxCDARBDOg0IABCxAxCDARBDEIsDOg4IABCxAxCDARCRAhCLAzoLCAAQsQMQgwEQkQI6CAgAEJECEIsDOg4IABCABBCxAxCDARCLAzoICAAQgAQQiwM6BQgAEIAEOgcIABAKEIsDOggILhCABBCLAzoECAAQCjoGCAAQHhAWOgUIABCGAzoFCCEQoAE6CAghEB4QFhAdOgoIIRAeEA8QFhAdSgQIQRgASgQIRhgAUJ0OWOtAYKpCaANwAXgAgAGpA4gBhUaSAQswLjE5LjIwLjEuMZgBAKABAcgBCLgBAsABAQ&sclient=gws-wiz
https://github.com/steveli/pytorch-sqrtm
https://github.com/steveli/pytorch-sqrtm/blob/master/sqrtm.py
https://pytorch.org/docs/stable/generated/torch.linalg.cholesky.html#torch.linalg.cholesky
https://www.google.com/search?q=covariance+matrix+check&oq=covariance+matrix&aqs=chrome.0.69i59l2j69i57j69i60l2j69i61.2275j0j1&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/57627406/how-to-use-autograd-gradcheck-in-pytorch
https://www.google.com/search?q=derive+sqrt+of+a+matrix&oq=derive+sqrt+of+a+matrix&aqs=chrome..69i57.3474j0j1&sourceid=chrome&ie=UTF-8
https://hulk.ndc.nasa.gov/yamcs/telemetry/parameters?c=viper__realtime&filter=ViperGround%2FTime%2FserverTime
https://www.google.com/search?q=c%2B%2B+all+types+of+const&ei=PbMsY6TGFbHfkPIPo76keA&ved=0ahUKEwjk2OuPjan6AhWxL0QIHSMfCQ8Q4dUDCA4&uact=5&oq=c%2B%2B+all+types+of+const&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QFjIFCAAQhgMyBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDOgoIABBHENYEELADOgUIIRCgAToICCEQHhAWEB06BQghEKsCOgoIIRAeEA8QFhAdSgQIQRgASgQIRhgAUKwFWNgKYL8LaAFwAXgAgAHPAYgB5wmSAQUwLjYuMZgBAKABAcgBB8ABAQ&sclient=gws-wiz
https://www.geeksforgeeks.org/auto_ptr-unique_ptr-shared_ptr-weak_ptr-2/
https://www.geeksforgeeks.org/different-ways-to-use-const-with-reference-to-a-pointer-in-c/
https://dzone.com/articles/c-type-casting-with-example-for-c-developers
https://www.geeksforgeeks.org/const-keyword-in-cpp/
https://outlook.office365.com/mail/inbox/id/AAQkADAyZDgxZDI4LTg1ZDctNDEyYi1hMzA3LTZhMDg3Y2IxZjE1ZgAQADGD28%2BWidhHvHJmX6xWazI%3D
https://www.google.com/search?q=torch+regularizing+nan+gradient&oq=torch+regularizing+nan+gradient&aqs=chrome..69i57.4635j0j1&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/model.html
https://deepxde.readthedocs.io/en/latest/modules/deepxde.nn.html#module-deepxde.nn.regularizers
https://www.google.com/search?q=deepxde+regularizer&oq=deepxde+regularizer&aqs=chrome..69i57.2363j0j1&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=pytorch+regularizer&oq=pytorch+regularizer&aqs=chrome..69i57.2486j0j4&sourceid=chrome&ie=UTF-8
https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058
https://chrome.google.com/webstore/detail/tab-snap/ajjloplcjllkammemhenacfjcccockde
https://stackoverflow.com/questions/55671735/loss-is-nan-all-the-time-when-training-the-neural-network-in-pytorch
https://discuss.pytorch.org/t/loss-is-not-nan-but-the-gradients-are/21224/5
https://github.com/koshian2/swd-pytorch/blob/master/swd.py

https://www.google.com/search?q=python+file+directory+sync+github&sxsrf=ALiCzsbIjHAwUXdNjgnjFJeoLu9DDvdZxg%3A1663897491450&ei=kw8tY_jvGvmq0PEPrMmrqAs&ved=0ahUKEwi4ha6X5an6AhV5FTQIHazkCrUQ4dUDCA4&uact=5&oq=python+file+directory+sync+github&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMggIIRAeEBYQHToKCAAQRxDWBBCwAzoFCCEQqwI6BwghEKABEApKBAhBGABKBAhGGABQwgNYuglgnwpoAXABeACAAa8BiAHcBZIBAzQuM5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://unix.stackexchange.com/questions/24952/script-to-monitor-folder-for-new-files
https://github.com/emcrisostomo/fswatch
https://github.com/c24o/watch-and-do
https://github.com/db42/PySyncIt
https://github.com/jackd248/file-sync-tool
https://github.com/Jwink3101/PyFiSync
https://www.google.com/search?q=pytorch+matrix+sqrt&oq=pytorch+matrix+sqrt&aqs=chrome..69i57.3230j0j7&sourceid=chrome&ie=UTF-8
https://github.com/steveli/pytorch-sqrtm
https://www.google.com/search?q=pytorch+not+positive+semi-definite&oq=pytorch+not+positive+semi-definite&aqs=chrome..69i57.7662j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=pytorch+check+gradient+not+nan&oq=pytorch+check+gradient+not+nan&aqs=chrome..69i57.5994j0j7&sourceid=chrome&ie=UTF-8
https://discuss.pytorch.org/t/loss-is-not-nan-but-the-gradients-are/21224/12
https://teams.microsoft.com/_#/conversations/19:16012332c12a457bbf78a007acf5d3b6@thread.v2?ctx=chat
https://stackoverflow.com/questions/71639484/nan-error-during-backpropagation-of-torch-sqrt-even-though-an-epsilon-is-added
https://www.google.com/search?q=pytorch+sqrt+gradient+is+nan+site:stackoverflow.com&sxsrf=ALiCzsZRloz9v4m1759Rax_pAm1SCN-Fsw:1663898111422&sa=X&ved=2ahUKEwjAhf6-56n6AhXVLUQIHT8YCIoQrQIoBHoECA0QBQ&biw=1215&bih=664&dpr=1.25
https://pytorch.org/docs/stable/type_info.html
https://www.google.com/search?q=pytorch+approximate+sqrt&oq=pytorch+approximate+sqrt&aqs=chrome..69i57.3372j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=approximate+matrix+sqrt+github+pytorch&sxsrf=ALiCzsYlqk9j2bRT24XDceaIoa0p4UDVJA%3A1663898575167&ei=zxMtY6HuCbPK9AOt56roCA&ved=0ahUKEwjhkY-c6an6AhUzJX0KHa2zCo0Q4dUDCA4&uact=5&oq=approximate+matrix+sqrt+github+pytorch&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEKABEAoyBwghEKABEAo6CggAEEcQ1gQQsAM6BQghEKABOgUIIRCrAkoECEEYAEoECEYYAFCQDFiRE2DjE2gDcAF4AIABgwGIAbgFkgEDNC4zmAEAoAEByAEDwAEB&sclient=gws-wiz
https://arxiv.org/abs/2105.02498
https://github.com/KingJamesSong/DifferentiableSVD
https://github.com/KingJamesSong/FastDifferentiableMatSqrt
https://github.com/KingJamesSong/FastDifferentiableMatSqrt/blob/main/torch_utils.py
https://www.google.com/search?q=covariance+matrix+properties&sxsrf=ALiCzsa-AFlhI1zrR2fRYB8x-6dwtPRTqw%3A1664339619078&ei=o84zY76qBL2m5NoP3YS-aA&ved=0ahUKEwj-h5-e1Lb6AhU9E1kFHV2CDw0Q4dUDCA4&uact=5&oq=covariance+matrix+properties&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIGCAAQHhAWMgYIABAeEBYyBggAEB4QFjIGCAAQHhAWMggIABAeEA8QFjIICAAQHhAPEBYyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoECCMQJzoECAAQQzoFCAAQkQI6FAguEIAEELEDEIMBEMcBENEDEIsDOg4IABCABBCxAxCDARCLAzoRCC4QsQMQgwEQiwMQqAMQmwM6CAgAEJECEIsDOgcIABBDEIsDOhAILhBDEIsDEJoDEJsDEKgDOgcIABCxAxBDOgoIABCxAxCDARBDOgYIABAKEEM6CwguEIAEEMcBEK8BOgsIABCABBCxAxCDAToRCC4QgAQQsQMQgwEQxwEQ0QNKBAhBGABKBAhGGABQ8gtY4SJg3SNoA3ABeACAAeEBiAGxIpIBBjAuMjUuMpgBAKABAcgBCLgBAsABAQ&sclient=gws-wiz
https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.sqrtm.html
https://pytorch.org/docs/stable/generated/torch.norm.html
https://discuss.pytorch.org/t/most-efficient-way-to-get-just-the-off-diagonal-elements-of-a-tensor/131065
https://pytorch.org/docs/stable/generated/torch.diagonal.html
https://github.com/pymanopt/pymanopt
https://doc.cgal.org/latest/Polygon/classCGAL_1_1Polygon__with__holes__2.html
https://datascience.stackexchange.com/questions/41046/how-to-force-pytorch-model-to-predict-only-positive-values
https://discuss.pytorch.org/t/proper-way-to-constraint-the-final-linear-layer-to-have-non-negative-weights/103498/2
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
https://towardsdatascience.com/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-1-7d8ade494cee
https://datascience.stackexchange.com/questions/13061/when-to-use-he-or-glorot-normal-initialization-over-uniform-init-and-what-are
https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1


https://www.google.com/search?q=pytorch+last+layer+non-negative&oq=pytorch+last+layer+non-negative&aqs=chrome..69i57.11728j0j7&sourceid=chrome&ie=UTF-8
https://discuss.pytorch.org/t/how-can-my-net-produce-negative-outputs-when-i-use-relu/19483/6
https://www.google.com/search?q=torch+constrain+weights+non-negative&oq=torch+constrain+weights+non-negative&aqs=chrome..69i57j69i60l3.4995j0j7&sourceid=chrome&ie=UTF-8
https://discuss.pytorch.org/t/positive-weights/19701/8
https://discuss.pytorch.org/t/proper-way-to-constraint-the-final-linear-layer-to-have-non-negative-weights/103498
https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620/7
https://www.google.com/search?q=deepxde+non+negative&oq=deepxde+non+negative&aqs=chrome..69i57.3074j0j7&sourceid=chrome&ie=UTF-8
https://deepxde.readthedocs.io/en/latest/search.html?q=weights&check_keywords=yes&area=default#
https://www.google.com/search?q=pytorch+nn+clamp+weights+to+be+between+0+and+1&oq=pytorch+nn+clamp+weights+to+be+between+0+and+1&aqs=chrome..69i57.7749j0j7&sourceid=chrome&ie=UTF-8
https://stackoverflow.com/questions/70330169/forcing-nn-weights-to-always-be-in-a-certain-range
https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/2
https://andrewpwheeler.com/tag/pytorch/
https://login.microsoftonline.com/common/oauth2/authorize?client_id=00000002-0000-0ff1-ce00-000000000000&redirect_uri=https%3a%2f%2foutlook.office.com%2fowa%2f&resource=00000002-0000-0ff1-ce00-000000000000&response_mode=form_post&response_type=code+id_token&scope=openid&msafed=1&msaredir=1&client-request-id=592fc6cc-3a3b-9d26-f3f4-f667685b32f8&protectedtoken=true&claims=%7b%22id_token%22%3a%7b%22xms_cc%22%3a%7b%22values%22%3a%5b%22CP1%22%5d%7d%7d%7d&nonce=638000426159958013.12dc8803-0366-48ec-a746-bbe73a25de3f&state=DYvdboIwAIVhe5fdMftDC70wSzAQtohKKeC4a2k3JzIIEtSn26utF985-XJyXMdxni1PFhfYcAKKQwCAjygkjJEQQPwKkW7DEGAPYEo9PzStJwOfekqZAEtEtMFfrv3-uavhJldv11nOZg1fJqN_JtPOYljLlIM2zej2wRZ95FeF2LTtWd_0l3MjMrQryFkhsKg6GdWGjar_WIztZsOivEqmvOSRqca4wSfJ4w6IhP9m1j8x3_O4InXd3WUMaS52WZWMgyjtDrtHcTxJ2ZX3utdRWSaHsiaj7kMgxDfk6HYrwGVfpO-wRZof8n8
https://docs.google.com/spreadsheets/d/1MG4PqBMiFMIlG9CeMuyddmCjwkTpC23EEIAEkpXbL8A/edit#gid=2038720634
https://mail.google.com/mail/u/0/#inbox
https://www.google.com/search?q=torch+clamp+output+to+sum+of+1&oq=torch+clamp+output+to+sum+of+1&aqs=chrome..69i57.13668j0j7&sourceid=chrome&ie=UTF-8

http://localhost:8888/notebooks/sinkhorn.ipynb#
http://localhost:8889/notebooks/sinkhorn_stable.ipynb
https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame
https://www.youtube.com/watch?v=BfOjrQAhG4M
https://discuss.pytorch.org/t/no-gradient-on-cuda/144807
https://discuss.pytorch.org/t/gradients-have-different-shape-for-cuda-and-cpu/89878
https://stackoverflow.com/questions/72385812/speed-up-multiplication-of-matrix-by-transposed
https://medium.com/@ml_kid/pro-tip-matrix-transpose-in-pytorch-lesson-4-62bcd9efea52
https://pytorch.org/docs/stable/generated/torch.logsumexp.html
https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose
https://stackoverflow.com/questions/57512113/how-to-take-a-transpose-for-each-matrix-in-a-batch-in-pytorch
https://pytorch.org/docs/stable/generated/torch.as_strided.html
https://pytorch.org/docs/stable/generated/torch.einsum.html
https://stackoverflow.com/questions/45896939/using-python-numpy-einsum-to-obtain-dot-product-between-2-matrices
https://pytorch.org/docs/stable/generated/torch.tensordot.html
https://discuss.pytorch.org/t/how-to-cast-a-tensor-to-another-type/2713/5
https://www.youtube.com/watch?v=TRGHIN2PGIA
https://pytorch.org/docs/stable/generated/torch.matmul.html
https://discuss.pytorch.org/t/how-to-do-elementwise-multiplication-of-two-vectors/13182
http://localhost:8888/notebooks/memory_test.ipynb
https://pytorch.org/docs/stable/generated/torch.set_printoptions.html
https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html
https://ajcr.net/Basic-guide-to-einsum/
https://www.google.com/search?q=logsumexp&oq=logsumexp&aqs=chrome.0.69i59l4j69i61j69i60l2.1948j0j7&sourceid=chrome&ie=UTF-8
https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/
https://www.google.com/search?q=torch.exp+nan&oq=torch.exp+nan&aqs=chrome..69i57.2280j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=torch+avoid+nan+values&sxsrf=ALiCzsYs0ozGxQmqd6tloIQVVxbHhAKM_g%3A1665180631688&ei=16NAY-rPKZnhkPIP3sGowAg&ved=0ahUKEwiql6egkc_6AhWZMEQIHd4gCogQ4dUDCA4&uact=5&oq=torch+avoid+nan+values&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgATIFCCEQqwI6CggAEEcQ1gQQsAM6BAgjECc6BAgAEEM6BQgAEJECOgoIABCxAxCDARBDOgsIABCxAxCDARCRAjoLCAAQgAQQsQMQgwE6BQgAEIAEOgcIABCABBAKOgoIABCABBCHAhAUOggIABAeEA8QFjoGCAAQHhAWOgUIABCGA0oECEEYAEoECEYYAFCdB1ioH2CgIGgDcAF4AIABsAGIAb0VkgEEMC4yMZgBAKABAcgBCMABAQ&sclient=gws-wiz
https://pytorch.org/docs/stable/tensors.html
https://www.google.com/search?q=pytorch+zero+tensor+grad&sxsrf=ALiCzsaL4eksv1zJjVIsRRwy69LYpfqzcg%3A1665191237589&ei=Rc1AY-rXI9nXkPIP_uy9uAo&ved=0ahUKEwiqjszhuM_6AhXZK0QIHX52D6cQ4dUDCA4&uact=5&oq=pytorch+zero+tensor+grad&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QFjIFCAAQhgMyBQgAEIYDMgUIABCGAzIFCAAQhgM6CggAEEcQ1gQQsAM6BwgAELADEEM6BAgjECc6BAgAEEM6CggAELEDEIMBEEM6BQgAEJECOgsIABCABBCxAxCDAToKCAAQgAQQhwIQFDoFCAAQgAQ6BwgAEIAEEAo6BQghEKABOgUIIRCrAjoICCEQHhAWEB06BQgAEKIEOgcIIRCgARAKSgQIQRgASgQIRhgAUMAFWNo1YN02aAtwAHgAgAHUAYgBohmSAQYyMi45LjGYAQCgAQHIAQrAAQE&sclient=gws-wiz
https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html
https://stackoverflow.com/questions/70956960/shall-i-use-grad-zero-in-pytorch-with-or-without-gradient-tracking
https://stackoverflow.com/questions/54648053/how-to-set-gradients-to-zero-without-optimizer
http://localhost:8889/notebooks/sinkhorn_stable.ipynb
https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
https://stackoverflow.com/questions/53266350/how-to-tell-pytorch-to-not-use-the-gpu
https://pytorch.org/docs/stable/generated/torch.nn.Module.html
https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html

https://stackoverflow.com/questions/36367986/how-to-make-inline-plots-in-jupyter-notebook-larger
https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html
https://www.tutorialspoint.com/plotting-an-imshow-image-in-3d-in-matplotlib
https://gist.github.com/WetHat/1d6cd0f7309535311a539b42cccca89c
https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.FancyArrowPatch.html

https://www.physicsread.com/latex-plus-minus/
https://mathworld.wolfram.com/L2-Norm.html
https://www.google.com/search?q=l1+norm&oq=l1+norm&aqs=chrome..69i57j69i59.728j0j7&sourceid=chrome&ie=UTF-8
https://math.stackexchange.com/questions/2157042/1-norm-less-than-or-equal-to-n-times-infinity-norm
https://www.google.com/search?q=torch.div&oq=torch.div&aqs=chrome..69i57j69i60.2472j0j7&sourceid=chrome&ie=UTF-8
https://math.stackexchange.com/questions/1463140/proof-for-why-a-matrix-multiplied-by-its-transpose-is-positive-semidefinite
https://stackoverflow.com/questions/45604688/apply-function-on-each-row-row-wise-of-a-numpy-array
https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html
https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html
https://math.stackexchange.com/questions/957465/relationship-between-inner-product-and-norm
https://math.stackexchange.com/questions/1393301/frobenius-norm-of-product-of-matrix
https://mathworld.wolfram.com/VectorNorm.html
https://www.google.com/search?q=l2+norm+notation&oq=l2+norm+notation&aqs=chrome..69i57.4130j0j7&sourceid=chrome&ie=UTF-8
https://math.stackexchange.com/questions/1954167/do-positive-semidefinite-matrices-have-to-be-symmetric
https://mathworld.wolfram.com/MatrixDiagonalization.html#:~:text=Matrix%20diagonalization%20is%20the%20process,properties%20of%20the%20underlying%20matrix.
https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix
https://findstuffsonline.com/jessica-barlett-latest-hd-pictures-and-wallpapers/
https://mathinsight.org/dot_product_matrix_notation
https://math.stackexchange.com/questions/2517797/does-a-symmetric-matrix-necessarily-have-a-symmetric-square-root
https://numpy.org/doc/stable/reference/generated/numpy.linspace.html
https://problemsolvingwithpython.com/06-Plotting-with-Matplotlib/06.16-3D-Surface-Plots/
https://www.google.com/search?q=sublime+text+jump+to+end+of+file&oq=sublime+text+jump+to+end+of+file&aqs=chrome..69i57.3608j0j7&sourceid=chrome&ie=UTF-8
https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix
https://en.wikipedia.org/wiki/Diagonalizable_matrix
https://math.stackexchange.com/questions/1956787/uniqueness-of-orthogonal-decomposition-of-a-symmetric-matrix
https://www.google.com/search?q=positive+semidefinite+symmetric+matrices+cos+of+angles&sxsrf=ALiCzsaDU9I0zaK8ejMVeFYCr-ZyswN9Tg:1664738874396&ei=OuY5Y97jF_jbkPIPzY-n6Ac&start=10&sa=N&ved=2ahUKEwje6IHKo8L6AhX4LUQIHc3HCX0Q8tMDegQIARA7&biw=1024&bih=1042&dpr=1.25
https://en.wikipedia.org/wiki/Orthogonal_matrix#Matrix_properties
https://math.stackexchange.com/questions/653133/eigenvalues-in-orthogonal-matrices
https://math.stackexchange.com/questions/2822149/clarification-is-it-true-trace-of-ata-is-induced-matrix-norm-squared
https://math.stackexchange.com/questions/33083/what-is-the-difference-between-the-frobenius-norm-and-the-2-norm-of-a-matrix
https://en.wikipedia.org/wiki/Lp_space
https://math.stackexchange.com/questions/4159983/derivation-of-l2-norm-of-matrix-formula
https://www.google.com/search?q=frobenius+norm+of+matrix&sxsrf=ALiCzsYko_RP4NO4M_wiQBs5IctW1trGjw%3A1664742914837&ei=AvY5Y-TNMpvEqtsP37qtgAY&ved=0ahUKEwjk1tLQssL6AhUbomoFHV9dC2AQ4dUDCA4&uact=5&oq=frobenius+norm+of+matrix&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIGCAAQHhAWMgYIABAeEBYyBggAEB4QFjIGCAAQHhAWMgYIABAeEBYyBggAEB4QFjIGCAAQHhAWMgYIABAeEBY6CggAEEcQ1gQQsAM6CggAEIAEEIcCEBRKBAhBGABKBAhGGABQwQRY9gpgyAtoAXABeACAAY4CiAGCDpIBAzItN5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://pytorch.org/docs/stable/generated/torch.matmul.html
https://en.wikipedia.org/wiki/Matrix_norm#Square_matrices
https://en.wikipedia.org/wiki/Matrix_norm#Matrix_norms_induced_by_vector_norms
https://en.wikipedia.org/wiki/Rayleigh_quotient
https://en.wikipedia.org/wiki/Lagrange_multiplier
https://en.wikipedia.org/wiki/Mathematical_optimization
https://mathworld.wolfram.com/L2-Norm.html
https://www.google.com/search?q=l2+norm+of+matrix+is+equal+to+product+transpose&oq=l2+norm+of+matrix+is+equal+to+product+transpose&aqs=chrome..69i57.13260j0j7&sourceid=chrome&ie=UTF-8
https://linuxtect.com/move-end-of-line-in-vim-vi/#:~:text=We%20may%20need%20to%20end%20the%20last%20line%20from%20anywhere,the%20end%20of%20the%20line.
https://linuxtect.com/how-to-make-bash-script-executable-with-chmod/
https://math.stackexchange.com/questions/1274924/eigenvalues-of-ata
https://www.youtube.com/watch?v=kG8yaAjkNQE
https://www.google.com/search?q=orthogonal+matrix+product+with+transpose&oq=orthogonal+matrix+product+with+transpose&aqs=chrome..69i57.6791j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=eigenvalue+of+square+matrix+transposed&oq=eigenvalue+of+square+matrix+transposed&aqs=chrome..69i57.4403j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=matrix+characteristic+polynomial&oq=matrix+characteristic+polynomial&aqs=chrome..69i57.4097j0j7&sourceid=chrome&ie=UTF-8
https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c#:~:text=If%20the%20L0%20norm%20of,username%20and%20password%20are%20incorrect.
https://math.stackexchange.com/questions/3967922/singular-values-and-matrix-rank
https://math.berkeley.edu/~hutching/teach/54-2017/svd-notes.pdf
https://en.wikipedia.org/wiki/Normal_matrix
https://math.stackexchange.com/questions/1754712/orthogonal-matrix-norm
https://math.stackexchange.com/questions/3249516/orthogonal-matrices-and-matrix-norms
https://math.stackexchange.com/questions/2105555/proof-that-the-2-norm-of-orthogonal-transformation-of-a-matrix-is-invariant
https://www.google.com/search?q=orthogonal+matrix+preserves+l2+matrix+norm+site:math.stackexchange.com&sxsrf=ALiCzsZHh2yM-ONv_7hCNLmxUE-VzWCCZQ:1664818497262&ei=QR07Y67XD9zgkPIPnPSQ2Ag&start=10&sa=N&ved=2ahUKEwju-5OZzMT6AhVcMEQIHRw6BIsQ8tMDegQIARA7&biw=1225&bih=687&dpr=1.25
https://math.stackexchange.com/questions/586663/why-does-the-spectral-norm-equal-the-largest-singular-value
https://www.google.com/search?q=spectral+norm%2C+l2+norm+frobenius+norm&oq=spectral+norm%2C+l2+norm+frobenius+norm&aqs=chrome..69i57.4782j0j7&sourceid=chrome&ie=UTF-8
https://en.wikipedia.org/wiki/Lp_space
https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality
https://www.google.com/search?q=l1+norm&oq=l1+norm&aqs=chrome.0.69i59l4j69i60.1726j0j7&sourceid=chrome&ie=UTF-8
https://en.wikipedia.org/wiki/Unitary_matrix
https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c
https://math.stackexchange.com/questions/1378075/does-an-n-times-n-matrix-a-only-have-an-inverse-if-ranka-n-if-so-why


https://noodlemagazine.com/watch/-152128902_456240589
https://noodlemagazine.com/watch/226200998_169032219
https://noodlemagazine.com/watch/-110014691_456239537
https://noodlemagazine.com/watch/-148989495_456244804
https://noodlemagazine.com/watch/-200562178_456239277
https://www.nudevista.com/?q=tiffany+tyler+massage&s=t&start=25
https://txxx.com/videos/15284169/tiffany-tyler-amazing-massage/?promo=10376
https://www.tnaflix.com/search.php?what=tiffany%20tyler&tab=
https://www.tnaflix.com/hardcore-porn/Sweetheart-brunette-Tiffany-Tyler-gets-laid/video1625296?isFeatured=0
https://www.tnaflix.com/celebrity-porn/Tiffany-Tyler-nude-Erotic-Karma-2012/video2940235?isFeatured=0
https://www.tnaflix.com/hd-videos/Young-babe-Tiffany-Tyler-fucked-hard-by-monster-white-dick/video4054174?isFeatured=0
https://www.tnaflix.com/big-boobs/Capri-Cavanni-and-Tiffany-Tyler-in-Rent-Reduction/video4828121?isFeatured=0
https://www.eporner.com/hd-porn/Y26vP1kfgT8/Hard-Cock-Needs-Massage/
https://www.tnaflix.com/babe-videos/Tiffany-Tyler-Fuckedhard-18.mp4/video6512180#
https://www.sextvx.com/video/705379/tiffany-tyler-massage-parlor
https://www.tnaflix.com/big-boobs/LezBFriends-Party-With-Puma-Swede-Nina-Elle-Tiffany-Tyler%21/video4191157?isFeatured=0
https://www.tnaflix.com/asian-porn/AMWF-Tiffany-Tyler-interracial-with-Asian-guy-video-1/video957531?isFeatured=0
https://www.youporn.com/search/?query=tiffany+tyler&page=2
https://www.youporn.com/watch/16945328/sexy-tiffany-cant-get-enough-of-this-giant-white-dick/
https://www.youporn.com/watch/16827570/abigail-mac-eats-out-tiffany-tyler/
https://www.youporn.com/watch/15520006/two-sexy-pornstar-kiss-and-lick-each-other-to-an-orgasm/
https://www.youporn.com/watch/11563325/hayden-hawkins-and-tiffany-tyler-girl-bang/
https://www.youporn.com/watch/335200/tiffany-tyler-thanks-for-the-tattoo/
https://www.youporn.com/watch/9384215/tiffany-tyler-licking-out-lola-foxx/
https://www.youporn.com/watch/12648763/reallesbianexposed-unexpected-wild-lesbian-threesome/

https://colab.research.google.com/drive/1bzCH3Yaq8gK0ZByxlcaRaj3pOc2u6zht#scrollTo=ezUlfYJ59jWl
https://stackoverflow.com/questions/65900110/does-pytorch-broadcast-consume-less-memory-than-expand
https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html
https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html
https://www.google.com/search?q=pytorch+unsqueeze+without+memory&sxsrf=ALiCzsY81Q1FJANuS0TdMlc0PJmq1F-xMQ%3A1664902130186&ei=8mM8Y5boCrHckPIPyZC_0A4&ved=0ahUKEwiWqbjgg8f6AhUxLkQIHUnID-oQ4dUDCA4&uact=5&oq=pytorch+unsqueeze+without+memory&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgAToKCAAQRxDWBBCwAzoECCMQJzoGCAAQHhAWOgUIABCGAzoFCCEQqwI6CAghEB4QFhAdSgQIQRgASgQIRhgAUJAFWLAbYKMcaARwAXgBgAHVAogBhx-SAQUyLTUuOJgBAKABAcgBCMABAQ&sclient=gws-wiz
https://pytorch.org/docs/stable/generated/torch.unsqueeze.html
https://stackoverflow.com/questions/71383309/can-i-define-a-function-to-replace-torch-einsum-sum-with-any-custom-function
https://www.google.com/search?q=torch.expand&oq=torch.expand&aqs=chrome..69i57.1708j0j7&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=torch+fill+with+zeros&sxsrf=ALiCzsY_YjUTl_Kb5LNZDm-L6J-AhMM3ZA%3A1664909869478&ei=LYI8Y6zkHJqE0PEPpcOakAY&ved=0ahUKEwjsrunKoMf6AhUaAjQIHaWhBmIQ4dUDCA4&uact=5&oq=torch+fill+with+zeros&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QFjIGCAAQHhAWMgUIABCGAzIFCAAQhgMyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoKCAAQsQMQgwEQQzoECAAQQzoFCAAQgAQ6BAgjECc6EAgAEIAEEIcCELEDEIMBEBQ6CwgAEIAEELEDEIMBOgUIABCRAjoKCAAQgAQQhwIQFDoHCAAQgAQQCjoICAAQHhAWEAo6CAgAEB4QDxAWSgQIQRgASgQIRhgAUNYMWPQeYJUgaANwAXgBgAHxBIgBwyWSAQoyLTE0LjAuMS4xmAEAoAEByAEIwAEB&sclient=gws-wiz
https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch
https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html
https://www.google.com/search?q=pytorch+count+memory+usage&sxsrf=ALiCzsZMILIzcREjvDq28834yLQxTk14qg%3A1664925874473&ei=ssA8Y8qyHLq-0PEP6OOK2A4&ved=0ahUKEwjK08ya3Mf6AhU6HzQIHeixAusQ4dUDCA4&uact=5&oq=pytorch+count+memory+usage&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEB4QFjIFCAAQhgMyBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDOgoIABBHENYEELADOgUIABCABDoFCCEQoAE6BQghEKsCSgQIQRgASgQIRhgAUMYKWIYWYP4WaAJwAXgAgAGBA4gBuxWSAQYyLTEwLjGYAQCgAQHIAQjAAQE&sclient=gws-wiz
https://discuss.pytorch.org/t/how-to-check-the-gpu-memory-being-used/131220
https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3
https://colab.research.google.com/drive/1bzCH3Yaq8gK0ZByxlcaRaj3pOc2u6zht#scrollTo=ezUlfYJ59jWl
https://github.com/abhishekhalder/AM229F20
https://github.com/abhishekhalder/AMS-232-S19/blob/master/HW1/HW1Solution.ipynb
https://github.com/abhishekhalder/AM229F20/blob/main/HW1%20Solution.ipynb
https://opg.optica.org/ao/fulltext.cfm?uri=ao-61-10-2805&id=470826
https://doc.cgal.org/latest/Optimal_transportation_reconstruction_2/index.html#title20
https://math.stackexchange.com/questions/3660192/algorithms-to-compute-the-wasserstein-distance-between-two-discrete-probability
https://en.wikipedia.org/wiki/Cobra_maneuver
https://blog.arduino.cc/2022/09/14/its-here-please-welcome-arduino-ide-2-0/
https://github.com/leeoniya/uFuzzy
https://www.jeffgeerling.com/blog/2022/you-cant-buy-raspberry-pi-right-now
https://shop.pimoroni.com/products/raspberry-pi-pico-w?variant=40059369619539
https://stackoverflow.com/questions/7825055/what-does-the-operator-do-in-c
https://www.nature.com/articles/s41586-022-05172-4
https://www.google.com/search?q=pytorch+use+numpy+constant+matrix&oq=pytorch+use+numpy+constant+matrix&aqs=chrome..69i57.7216j0j7&sourceid=chrome&ie=UTF-8
https://github.com/jeanfeydy/geomloss/tree/main/geomloss
https://arxiv.org/pdf/1306.0895.pdf
https://gist.github.com/louity/0629e2d12ff4ac96573b3a541246e162#file-sinkhorn_logsumexp-py-L22
https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html
https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html
https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management
https://pytorch.org/docs/stable/generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary
https://www.tutorialspoint.com/how-to-perform-element-wise-addition-on-tensors-in-pytorch
https://pytorch.org/docs/stable/generated/torch.sub.html
https://pytorch.org/docs/stable/generated/torch.logsumexp.html
https://math.stackexchange.com/questions/3978967/exponential-of-a-matrix-sum
https://proofwiki.org/wiki/Exponential_of_Product
https://pytorch.org/docs/stable/generated/torch.exp.html
https://stats.stackexchange.com/questions/381936/vectorised-computation-of-logsumexp
https://www.google.com/search?q=sinkhorn+barycenter+github&sxsrf=ALiCzsZghWX4Bkys8zJbuarw_aS9higtsw%3A1665121517625&ei=7bw_Y63YJezdptQPofuI0A8&ved=0ahUKEwjtgsOEtc36AhXsrokEHaE9AvoQ4dUDCA4&uact=5&oq=sinkhorn+barycenter+github&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABOggIABCiBBCwAzoECCMQJzoECAAQQzoFCAAQkQI6CAgAEIAEELEDOggILhCABBCxAzoLCAAQgAQQsQMQgwE6BQgAEIAEOgYIABAeEBY6CAgAEB4QFhAKOgUIABCGAzoFCCEQqwJKBAhBGAFKBAhGGABQ2w5YwSZgqidoA3AAeACAAZEBiAGjEpIBBDE5LjaYAQCgAQHIAQTAAQE&sclient=gws-wiz
https://gist.github.com/louity/0629e2d12ff4ac96573b3a541246e162
https://github.com/GiulsLu/Sinkhorn-Barycenters
https://github.com/GiulsLu/Sinkhorn-Barycenters/blob/master/documentation.md
https://github.com/GiulsLu/Sinkhorn-Barycenters
https://github.com/hichamjanati/debiased-ot-barycenters
https://arxiv.org/pdf/2006.02575.pdf
https://arxiv.org/pdf/2006.02575.pdf
https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.core.sinkhorn.sinkhorn.html
https://hal.archives-ouvertes.fr/hal-02303456/document
https://twitter.com/gabrielpeyre/status/955360271656194049?lang=en
https://en.wikipedia.org/wiki/Softmax_function
https://en.wikipedia.org/wiki/LogSumExp
https://www.google.com/search?q=logsumexp+trick&oq=logsumexp+trick&aqs=chrome..69i57.2639j0j7&sourceid=chrome&ie=UTF-8
https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/
https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879
https://www.google.com/search?q=sinkhorn+divergence&oq=sinkhorn+divergence&aqs=chrome..69i57j69i60j69i61l2.4935j0j7&sourceid=chrome&ie=UTF-8
https://arxiv.org/pdf/1910.12958.pdf


https://trunk.arc.nasa.gov/bamboo/browse/VIPERSW-VXK-JOB1-226/artifact
https://trunk.arc.nasa.gov/bamboo/browse/VIPERSW-VXK-JOB1-218
https://canvas.ucsc.edu/courses/56479/quizzes/86954
https://www.surfline.com/surf-news/surflines-rating-surf-heights-quality/1417
https://the-algorithms.com/
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.8201&rep=rep1&type=pdf
https://github.com/TheAlgorithms
https://www.google.com/search?q=short+proofs&oq=short+proofs&aqs=chrome..69i57.5742j0j7&sourceid=chrome&ie=UTF-8
https://advancedorthocenters.com/iliotibial-band-syndrome/
https://www.google.com/search?q=vim+jump+to+start+of+line&sxsrf=ALiCzsYQoB01m2lRO6C_grX7vWPr2ABAQQ%3A1664814341269&ei=BQ07Y7f-D6ngkPIPqJudsAg&ved=0ahUKEwj3lLbbvMT6AhUpMEQIHahNB4YQ4dUDCA4&uact=5&oq=vim+jump+to+start+of+line&gs_lcp=Cgdnd3Mtd2l6EAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyCggAEEcQ1gQQsAMyBwgAELADEEMyBwgAELADEENKBAhBGABKBAhGGABQvhdYhRtggBxoBHAAeACAAQCIAQCSAQCYAQCgAQHIAQrAAQE&sclient=gws-wiz
https://pytorch.org/docs/stable/generated/torch.set_printoptions.html
https://www.google.com/search?q=3d+wasserstein+distance+github&sxsrf=ALiCzsYq2C85vq9_d3i2RsHLyktZ0gwfRg%3A1664772194632&ei=Ymg6Y82SJoiF0PEP5ea2-Ag&ved=0ahUKEwjNy6van8P6AhWIAjQIHWWzDY8Q4dUDCA4&uact=5&oq=3d+wasserstein+distance+github&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKsCOgoIABBHENYEELADOgUIIRCgAToICCEQHhAWEB1KBAhBGABKBAhGGABQ0QVYmw1gkg5oAXABeACAAZkBiAHdBZIBAzQuM5gBAKABAcgBCMABAQ&sclient=gws-wiz
https://pythonot.github.io/auto_examples/gromov/plot_gromov.html
https://www.google.com/search?q=pytorch+sinkhorn&oq=pytorch+sinkhorn&aqs=chrome..69i57.2858j0j7&sourceid=chrome&ie=UTF-8#imgrc=kzS33Rv5K7oVqM
https://gist.github.com/wohlert/8589045ab544082560cc5f8915cc90bd
https://notebook.community/t-vi/pytorch-tvmisc/wasserstein-distance/Pytorch_Wasserstein
https://marcocuturi.net/ot.html
https://dfdazac.github.io/sinkhorn.html
https://github.com/cyan-at/wassdistance/blob/master/layers.py
https://github.com/dfdazac/wassdistance/blob/master/layers.py
https://arxiv.org/abs/1803.00567
https://www.kernel-operations.io/geomloss/_auto_examples/sinkhorn_multiscale/plot_optimal_transport_cluster.html#sphx-glr-auto-examples-sinkhorn-multiscale-plot-optimal-transport-cluster-py
http://www.kernel-operations.io/keops/python/installation.html
http://www.kernel-operations.io/keops/python/installation.html#part-checkpython
http://www.kernel-operations.io/keops/python/installation.html
https://github.com/getkeops/keops/issues/257
https://trunk.arc.nasa.gov/confluence/display/VIPERMS/How+to+run+RFSW+under+QEMU
https://trunk.arc.nasa.gov/bamboo/browse/VIPERSW-QEMU/latestSuccessful
https://trunk.arc.nasa.gov/bamboo/browse/VIPERSW-VXK/latestSuccessful
https://trunk.arc.nasa.gov/bamboo/browse/VIPERSW-QEMU-90/artifact
https://trunk.arc.nasa.gov/bitbucket/projects/VIPERSW/repos/viper/browse
https://trunk.arc.nasa.gov/confluence/display/VIPERSW/Windriver+-+VxWorks+support+and+install
https://www.tutorialspoint.com/how-to-squeeze-and-unsqueeze-a-tensor-in-pytorch
https://open.spotify.com/collection/tracks
https://www.youtube.com/
https://stackoverflow.com/questions/53819383/how-to-assign-a-new-value-to-a-pytorch-variable-without-breaking-backpropagation
https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/
https://pytorch.org/docs/stable/generated/torch.einsum.html
https://stackoverflow.com/questions/53051913/modifying-a-pytorch-tensor-and-then-getting-the-gradient-lets-the-gradient-not-w
https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd
https://stackoverflow.com/questions/55788093/how-to-free-gpu-memory-by-deleting-tensors
https://discuss.pytorch.org/t/how-to-delete-pytorch-objects-correctly-from-memory/947/5
https://discuss.pytorch.org/t/about-torch-cuda-empty-cache/34232/7
https://stackoverflow.com/questions/63145729/how-to-make-sure-pytorch-has-deallocated-gpu-memory
http://localhost:8889/notebooks/sinkhorn.ipynb


https://en.wikipedia.org/wiki/Norm_(mathematics)
https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality#Statement_of_the_inequality
https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality
https://math.stackexchange.com/questions/218046/relations-between-p-norms
https://math.stackexchange.com/questions/3215756/how-does-matrix-vector-multiplication-scale-a-norm-of-vector
https://towardsdatascience.com/calculating-vector-p-norms-linear-algebra-for-data-science-iv-400511cffcf0
https://math.stackexchange.com/questions/3927958/is-there-a-way-to-prove-that-the-2-norm-of-a-matrix-is-greater-than-the-largest
https://www.google.com/search?q=when+is+2+norm+less+than+1+norm&sxsrf=ALiCzsb-SkcDETfHUypGMTmwkkSs3ZtOqw%3A1664933384993&ei=CN48Y9iXPPuv0PEPicuO2As&ved=0ahUKEwjYu_KX-Mf6AhX7FzQIHYmlA7sQ4dUDCA4&uact=5&oq=when+is+2+norm+less+than+1+norm&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCrAjIFCCEQqwI6CggAEEcQ1gQQsAM6BwgAELADEEM6CwgAEIAEELEDEIMBOgQIABBDOgsILhCABBCxAxCDAToFCC4QgAQ6DgguEIAEELEDEMcBENEDOgsILhCABBDHARDRAzoOCC4QgAQQxwEQ0QMQ1AI6CAguEIAEENQCOgQIIxAnOgUIABCRAjoLCC4QgAQQsQMQ1AI6CAgAELEDEIMBOgUIABCABDoICAAQgAQQsQM6BwgAEIAEEAo6CggAEIAEEMkDEAo6BQgAEIYDOgYIABAeEBY6CAgAEB4QDxAWOggIIRAeEBYQHUoECEEYAEoECEYYAFC758QEWKmHxQRgpojFBGgFcAF4AIAB3AGIAesXkgEGMjIuOC4xmAEAoAEByAEKuAECwAEB&sclient=gws-wiz
https://www.google.com/search?q=norm+of+vector&biw=841&bih=598&sxsrf=ALiCzsa0yt3gG9mGUxJupB52SSdLGEiV7Q%3A1664942992631&ei=kAM9Y--TJunn0PEPqdqd-Ak&ved=0ahUKEwjvm5b9m8j6AhXpMzQIHSltB58Q4dUDCA4&uact=5&oq=norm+of+vector&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEOgQIIxAnOgQILhBDOgQIABBDOgUIABCRAjoLCAAQgAQQsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOg4ILhCABBCxAxCDARDUAjoUCC4QgAQQsQMQgwEQxwEQ0QMQiwM6BwgAEEMQiwM6DgguEIAEELEDENQCEIsDOhEILhCxAxCDARCLAxCoAxCdAzoHCC4QsQMQQzoICAAQkQIQiwM6CwguEIAEELEDEIMBOggIABCxAxCDAToICAAQgAQQsQM6CgguEIAEEIcCEBQ6CwgAELEDEIMBEJECOgoIABCxAxCDARBDOgUILhCABDoKCAAQgAQQhwIQFEoECEEYAUoECEYYAFDhWlisZ2CaaWgDcAB4AIABiAGIAb4KkgEDOS41mAEAoAEBuAECwAEB&sclient=gws-wiz
https://math.stackexchange.com/questions/1334854/relation-between-two-p-norms
https://www.google.com/search?q=finite+many+vectors+p-norm+ratio+site:math.stackexchange.com&sxsrf=ALiCzsawZidd2cEZqfNja7Ed31tPOwPCqw:1664947654895&source=lnms&tbm=bks&sa=X&ved=2ahUKEwjRmaisrcj6AhWJAzQIHVOuAAY4KBD8BSgDegQIARAF&biw=1769&bih=1042&dpr=1.25
https://math.stackexchange.com/questions/57686/understanding-of-the-theorem-that-all-norms-are-equivalent-in-finite-dimensional
https://math.stackexchange.com/questions/4094/how-do-you-show-monotonicity-of-the-ellp-norms
https://proofwiki.org/wiki/P-Norm_is_Norm/Complex_Numbers
https://math.stackexchange.com/questions/3623915/linear-algebra-a-level-p-norm-homogeneity

https://github.com/
https://github.com/storpipfugl/pykdtree
https://m2n037.github.io/awesome-mecheng/
https://github.com/humiaozuzu/awesome-flask
https://github.com/mjhea0/awesome-flask
https://www.google.com/search?q=flask+google+login+github&sxsrf=ALiCzsbeYnAdI48nmfmasHnqAg-IoLWBaA:1665252644227&source=lnt&tbs=qdr:y&sa=X&ved=2ahUKEwjI6cfCndH6AhX_LEQIHWlABvkQpwV6BAgBEBk&biw=1024&bih=1042&dpr=1.25
https://flask-dance.readthedocs.io/_/downloads/en/latest/pdf/
https://github.com/dpgaspar/Flask-AppBuilder
https://trunk.arc.nasa.gov/bitbucket/projects/VIPERGDS/repos/viper_devsim/browse
https://stackoverflow.com/questions/32428193/saving-matplotlib-graphs-to-image-as-full-screen
https://discuss.pytorch.org/t/converting-model-into-16-points-precisoin-float16-instead-of-32/102622/3
https://pytorch.org/docs/stable/amp.html
https://deepxde.readthedocs.io/en/latest/demos/pinn_forward/poisson.1d.multiscaleFourier.html?highlight=pderesidualresampler
https://github.com/lululxvi/deepxde/issues/320
https://deepxde.readthedocs.io/en/latest/search.html?q=batch_size&check_keywords=yes&area=default
https://deepxde.readthedocs.io/en/latest/_modules/deepxde/icbc/boundary_conditions.html?highlight=batch_size#
https://www.google.com/search?q=%60GLIBCXX_3.4.30%27+not+found&ie=UTF-8
https://askubuntu.com/questions/575505/glibcxx-3-4-20-not-found-how-to-fix-this-error
http://localhost:8888/notebooks/HW2.ipynb
https://www.google.com/search?q=hand+tattoo+santa+cruz&sxsrf=ALiCzsa0Mn-xOnfmQKVEEe3tskm7g5Rblg%3A1665263565683&ei=zedBY522KerdkPIPscG2yA8&ved=0ahUKEwjd0KiaxtH6AhXqLkQIHbGgDfkQ4dUDCA4&uact=5&oq=hand+tattoo+santa+cruz&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCrAjIICCEQHhAWEB0yCAghEB4QFhAdOgoIABBHENYEELADOgcIABCwAxBDOgQIIxAnOgcIABCxAxBDOgQIABBDOgoIABCxAxCDARBDOggIABCABBCxAzoFCAAQkQI6CAgAELEDEJECOggIABDJAxCRAjoFCAAQgAQ6CAgAEB4QFhAKOgYIABAeEBY6CggAEB4QDxAWEAo6BQgAEIYDSgQIQRgASgQIRhgAUOwHWMMZYNMaaAFwAXgAgAHlAYgB5gqSAQU2LjUuMZgBAKABAcgBCsABAQ&sclient=gws-wiz
https://github.com/dfdazac/wassdistance
https://stackoverflow.com/questions/55749202/getting-gradient-of-vectorized-function-in-pytorch
https://discuss.pytorch.org/t/how-to-compute-the-gradient-of-a-component-of-a-vector-valued-function/157581
https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch-autograd-grad
https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch
https://discuss.pytorch.org/t/grad-attribute-of-a-non-leaf-tensor-being-accessed/82313/6
https://www.ebay.com/sch/i.html?_from=R40&_nkw=mens%27+climbing+harness&_sacat=0&LH_BIN=1&_sop=15
https://www.ebay.com/itm/134201921592?hash=item1f3f0ed038%3Ag%3A5pkAAOSwNZ1i%7EIsg&amdata=enc%3AAQAHAAAAsCa%2FXOdKruRmT24wcPfK5IAYCsk9qu0tduSNBdDjY3LOHzwjBvUn%2BENhBi6nW6i0ETn%2BbqW1w%2BG6d69Q14PIrAjCWmlS%2FtSP7%2BLKqRcJOwUQy%2BgfIUy4tG%2FzCCRlF7bIgtlqrUucbhn9XFxbV2ZdiJHmgXIiuUr4gumncHI8kQfTvBI%2FCOIf4FUEj53jRX3%2B3xoquHalex8MF9KhtDb4FsHXJOeDPetJKyiUIQNzUHKi%7Ctkp%3ABk9SR7jIhbf3YA&LH_BIN=1&autorefresh=true
https://www.ebay.com/itm/175411330403?hash=item28d7547d63%3Ag%3AN7cAAOSw79linreO&amdata=enc%3AAQAHAAAAoCpYCgPuDXV82ZC9nBUc%2FSnMCQeEea4NQd7PQ1P79IIhmjK%2B%2Fh%2B9uLHw2Mfgn%2FWo0R2ieQN6BoRsUDH%2BbAxiD09VEecSfSRvchFV24VPWB%2F617ROp78c%2FHGFO9YaM%2Bt%2BkU3U6Ugt%2Fb2rOjZFB13PhYfHUSfIjlUsu2ElR3drb6pHf%2F1HgvVyRNjwsHuxok6ig%2BxD1uR6fJ3xs69ncWdIbnw%3D%7Ctkp%3ABk9SR7jIhbf3YA&LH_BIN=1
https://www.ebay.com/itm/195110350479?hash=item2d6d7bae8f%3Ag%3Ak4YAAOSwZEhinA%7E6&amdata=enc%3AAQAHAAAAoEBISOQ3lhg9GLF8CnwysVNzyfyHfPLe9Z%2FDoemS%2B1EDdG7R8z%2BbLgOyp4rfSvC2aL1hLlZB%2FDimu04fyoOMdOXGmzhQeuTX%2BJnFC%2B3amTx2naY0p0e87bVEyuuBchGq9ey9ed2PqBZVj%2FqAdIrHcXSZHLMfTdXzv%2B%2F%2BI0vsdcnlmRgHMVDTStPkLMsnzmtBUGmS94u%2Fi1N3Y4RdH637JLs%3D%7Ctkp%3ABk9SR7jIhbf3YA&LH_BIN=1
https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html
https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch
https://stackoverflow.com/questions/69148622/difference-between-autograd-grad-and-autograd-backward
https://www.crunchyroll.com/mobile-suit-gundam-the-witch-from-mercury/episode-2-the-cursed-mobile-suit-871205
https://pytorch.org/docs/stable/generated/torch.numel.html
https://www.google.com/search?q=pykeops&oq=pykeops&aqs=chrome..69i57j69i59j69i61.1164j0j7&sourceid=chrome&ie=UTF-8
https://www.kernel-operations.io/keops/python/installation.html
https://gist.github.com/jeanfeydy/d9395912e4d8bf1c0340bc93f78ea4bd
https://www.google.com/search?q=fast_sinkhorn+github&oq=fast_sinkhorn+github&aqs=chrome..69i57.5786j0j7&sourceid=chrome&ie=UTF-8
https://github.com/paigautam/CVPR21_FastSinkhornFilters
https://medium.com/coders-mojo/day-7-of-30-days-of-data-structures-and-algorithms-and-system-design-simplified-1-d-dynamic-2560f585499
https://github.com/fwilliams/scalable-pytorch-sinkhorn
https://github.com/pyqtgraph/pyqtgraph
https://github.com/pyqtgraph/pyqtgraph/blob/master/pyqtgraph/examples/text.py
https://stackoverflow.com/questions/52113401/np-loadtxt-ignores-the-header-how-can-i-save-the-header-data
https://stackoverflow.com/questions/28439701/how-to-save-and-load-numpy-array-data-properly
https://stackoverflow.com/questions/40219946/python-save-dictionaries-through-numpy-save
https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2334524.m570.l1313&_nkw=men%27s+9.5+rock+climbing+shoes&_sacat=0&LH_TitleDesc=0&_odkw=men%27s+9.5+climbing+shoes&_osacat=0&LH_BIN=1&_sop=15
https://www.ebay.com/sch/i.html?_from=R40&_nkw=men%27s+climbing+harness+medium&_sacat=0&LH_PrefLoc=2&_sop=15&rt=nc&LH_BIN=1
https://www.ebay.com/itm/314180295320?hash=item49269ada98%3Ag%3AS24AAOSwhVpjQd67&amdata=enc%3AAQAHAAAAsGXQUzcHYvo2tnOT8BgRTNe5qqkWhzsRlmvPwlikFWUeHhRKiz79cyeEVw879laBKkW8Z4a0xsRVZGSo%2BDyKVjRzM81A9sV9DsQiTQX3PR3YxvP9OMeGBBS%2BD9hZuZzgNNkLCJro028dm%2BLJKUj7IYxpJgYovFbBxZxKAFEZrWL319%2FV7OfQrFIVlZwxFLxBGLgNCPhyzZbjnZQdquwGlXFOaU6arW01A0WqHoNEXne1%7Ctkp%3ABk9SR6T45er3YA&LH_BIN=1
https://www.ebay.com/itm/274877871147?var=575138996871&hash=item3fffff742b%3Ag%3A0wkAAOSwbdNg-uvp&amdata=enc%3AAQAHAAAAsDkOBCeabrrUXHfRNptWnGT3%2FTPlo6dVjnNjdPgrtdrzdvyqcEuoq%2B0KKVkoWob0Mh6jIBXzIOH4WnuDPx0hoZ%2FATNLQ7lQFswem9wKS7Hxg3W5wqNOOthHggaCihIaJQyC5e7Vbhfc3PmLBFbbPA1CfWNJhywnUGk7BNqK%2BN%2FILA3P1UNWMkvdWZVg9S9kJy6TE5Eg1Lt2ECDFGb%2BTCuIyRmVKiOuV1RjbL3f0COlbm%7Ctkp%3ABk9SR6T45er3YA&LH_BIN=1
https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html
