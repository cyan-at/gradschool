{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [10 x 7 = 70 points] Convex Functions\n",
    "\n",
    "For each of the following, **state clearly if the function is convex, concave, neither convex nor concave**. **Then prove your statement**.\n",
    "\n",
    "(a) $f(x) = \\dfrac{\\|Ax + b\\|_2^2}{\\langle c, x\\rangle + d}$ over ${\\rm{dom}}(f)=\\{x\\in\\mathbb{R}^{n}\\mid \\langle c, x\\rangle + d > 0\\}$ where $A\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m}$, $c\\in\\mathbb{R}^{n}$, $d\\in\\mathbb{R}$. \n",
    "\n",
    "(b) $f(X)=\\sum_{i=1}^{n}\\lambda_{i}(X)\\log\\lambda_i(X)$ over ${\\rm{dom}}(f)=\\mathbb{S}^{n}_{++}$ where $\\lambda_{i}(X)$ denotes the $i$th eigenvalue of $X$.\n",
    "\n",
    "(c) $f(x) = \\underset{i=1,...,k}{\\max}\\|A^{(i)}x - b^{(i)}\\|_3$ over ${\\rm{dom}}(f)=\\mathbb{R}^{n}$ where $A^{(i)}\\in\\mathbb{R}^{m\\times n}$, $b^{(i)}\\in\\mathbb{R}^{m}$ for all $i=1,...,k$.\n",
    "\n",
    "(d) $f(x) = {\\rm{ReLU}}\\left(\\langle a , x\\rangle + b\\right)$ over ${\\rm{dom}}(f)=\\mathbb{R}^{n}$ where $a\\in\\mathbb{R}^{n}$, $b\\in\\mathbb{R}$, ${\\rm{ReLU}}(y):=\\max\\{0,y\\}$ for $y\\in\\mathbb{R}$.\n",
    "\n",
    "(e) $f(x) = -\\int_{0}^{2\\pi}\\log\\left(\\sum_{k=1}^{n}x_{k}\\cos((k-1)\\theta)\\right){\\rm{d}}\\theta$ over ${\\rm{dom}}(f)=\\{x\\in\\mathbb{R}^{n}\\mid \\sum_{k=1}^{n}x_{k}\\cos((k-1)\\theta) > 0 \\; \\forall \\; \\theta \\in [0, 2\\pi)\\}$.\n",
    "\n",
    "(f) $f(X) = \\lambda_{\\min}(X)$ over ${\\rm{dom}}(f)=\\mathbb{S}^{n}$ where $\\lambda_{\\min}(X)$ denotes the minimum eigenvalue of $X$.\n",
    "\n",
    "(g) $f(x) = \\log\\left({\\rm{logistic}}(x)\\right)$ over ${\\rm{dom}}(f)=\\mathbb{R}$ where ${\\rm{logistic}}(x):=\\frac{\\exp(x)}{1+\\exp(x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a **convex**\n",
    "1. $A x + b$ and $\\langle c, x\\rangle + d$ are **affine functions**, and both **convex**\n",
    "2. From Lec 8 pg 11, $p=2$ norm of a function is also **convex**, so $\\lVert A x + b \\rVert_2$ is a **convex** function\n",
    "3. Finally, let $y = \\lVert A x + b \\rVert_2$ and $z = \\langle c, x\\rangle + d$, then $f(x) = f(y, z) = \\frac{z^2}{y} = \\frac{\\lVert A x + b \\rVert_2^2}{\\langle c, x\\rangle + d}$ is **convex** (y is given to be strictly positive, $y \\in \\mathbb{R}_{++}$) and $z \\in \\mathbb{R}$, so it satisfies the $\\rm{dom}(f)$ requirement from lec 8 pg 12 / book pg 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b **concave**\n",
    "1. From the book 3.26 we know that $g(X) = \\log \\det(X) = \\Sigma_{i=1}^{n} \\log \\lambda_i(X)$, and that it is **concave** \n",
    "1. Since all the eigenvalues of a positive **definite** matrix are by definition $\\gt 0$, then $f = \\alpha g(X), \\alpha = \\lambda_i(X) \\gt 0$ is a nonnegative weighted sum of a concave function, therefore from the book pg 79, it is **concave**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c **convex**\n",
    "1. For $i$, $A^{(i)} x - b^{(i)}$ is an **affine function**, this a **convex function** (Leg 8 pg 8)\n",
    "2. From Lec 8 pg 11, $p=3$ norm of a function is also **convex**, so for a given $i$, $\\lVert A^{(i)} x - b^{(i)} \\rVert_3$ is a **convex** function\n",
    "3. Finally, Lec 9 pg 10 gives us that the **maximum** of $k$ **convex functions** is **convex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d **convex**\n",
    "1. From Lec 8 pg 8, $g(x) = \\langle a , x\\rangle + b$ is a **convex function**, and $0 = \\langle a , x\\rangle + b$ for the special case of $a = 0$, also a **convex function**\n",
    "2. $f(x) = \\rm{ReLU}(g(x)) = \\rm{max}(0, g(x))$ is the **maximum** of convex functions, then by Lec 9 pg 10, $f(x)$ **is convex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e **convex**\n",
    "1. $\\rm{dom}(f)$ is given to be $\\{ \\mathbb{R^{n}} \\mid g(x) = \\mathbb{R} \\gt 0 \\}$, for $g(x) = \\sum_{k=1}^{n}x_{k}\\cos((k-1)\\theta)$\n",
    "2. From Leg 8 pg 10, $-\\log(g(x))$ is **convex** \n",
    "3. And from the book pg 79,  the integral of a convex function, $f(x) = \\int_{0}^{2\\pi} -\\log(g(x))$ is **also convex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f **concave**\n",
    "1. From lec 9 pg 13 we know that $f(X) = \\lambda_{min}(X)$ is the **pointwise infimum of linear functions** $\\underline{x}^T X \\underline{x}$ for $\\lVert \\underline{x} \\rVert_2 = 1$\n",
    "2. We also know from lec that linear functions are both concave and convex\n",
    "3. Then from 1. and 2., $f(x)$ is a **pointwise inf over concave functions**\n",
    "3. Finally, from lec 9 pg 11 we know that the **pointwise inf over concave functions** is **concave**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g **concave**\n",
    "1. We can express this function as a composition:\n",
    "\\begin{align}\n",
    "f(x) & = h(x) \\circ g(x) \\\\\n",
    "h(x) & = \\log(x) (concave) \\\\\n",
    "g(x) & = \\rm{logistic}(x) \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "2.\n",
    "We can take the respective derivatives like so:\n",
    "\\begin{align}\n",
    "h^{\\prime\\prime} & = -\\frac{1}{x^{2}} \\lt 0\\\\\n",
    "g^{\\prime} & = \\frac{\\partial{}}{\\partial{x}} \\frac{e^x}{1+e^x} = \\frac{e^x}{(e^x + 1)^2} \\gt 0 \\\\\n",
    "& \\Rightarrow g^{\\prime}(x)^{2} \\gt 0 \\\\\n",
    "& \\Rightarrow h^{\\prime\\prime}(g(x)) g^{\\prime}(x)^{2} \\lt 0\n",
    "\\end{align}\n",
    "and\n",
    "\\begin{align}\n",
    "h^{\\prime} & = \\frac{1}{x} \\\\\n",
    "x < 0 & \\Rightarrow h^{\\prime} < 0 \\\\\n",
    "g^{\\prime\\prime} & = \\frac{\\left(1 - e^{x}\\right) e^{x}}{(e^{x} + 1)^{3}} \\\\\n",
    "x \\geq 0 & \\Rightarrow g^{\\prime\\prime} \\leq 0 \\\\\n",
    "& \\Rightarrow h^{\\prime}(g(x)) g^{\\prime\\prime} \\leq 0\n",
    "\\end{align}\n",
    "then combined\n",
    "\\begin{align}\n",
    "f^{\\prime\\prime}(x) = h^{\\prime\\prime}(g(x))g^{\\prime}(x)^{2} + h^{\\prime}(g(x))g^{\\prime\\prime}(x) \\leq 0\n",
    "\\end{align}\n",
    "<br>\n",
    "Note that $g(x) \\gt 0$, so $h^{\\prime}(g(x)), h^{\\prime\\prime}(g(x))$ is always finite\n",
    "<br>\n",
    "3. Then we see that from the book pg 84 and from Lec 6 pg 8, this function **is concave**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. [15 + 15 = 30 points] Legendre-Fenchel Conjugates\n",
    "\n",
    "For each the following function $f$, **derive** the Legendre-Fenchel conjugate $f^{*}$ and its domain ${\\rm{dom}}(f^{*})$.\n",
    "\n",
    "(a) $f(x)=\\sum_{i=1}^{n}x_{i}\\log x_{i}$ over ${\\rm{dom}}(f)=\\mathbb{R}^{n}_{++}$.\n",
    "\n",
    "(b) $f(X) = {\\rm{tr}}\\left(X^{-1}\\right)$ over ${\\rm{dom}}(f)=\\mathbb{S}^{n}_{++}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a\n",
    "1. By definition (leg 9 pg 16), $f^{*}(\\underline{y}) = \\sup\\{ \\underline{y}^{T} \\underline{x} - f(\\underline{x}) \\}$, then we take the derivative and set to 0 to find $\\underline{x}_{opt}$:\n",
    "\\begin{align}\n",
    "\\nabla \\underline{y}^{T} \\underline{x}_{opt} - \\nabla f(x)  & = 0 \\\\\n",
    "\\Rightarrow y_i - (\\log(x_i) + 1) & = 0 \\\\\n",
    "\\Rightarrow x_i = e^{y_i - 1} \\\\\n",
    "\\end{align}\n",
    "Note that the LHS is strictly positive, and to satisfy that $y_i$ can be anything, then $\\rm{dom}(f^{*}) = \\mathbb{R}^{n}$\n",
    "2. Then substituting $x_i$ back into $f^{*}(\\underline{y})$:\n",
    "\\begin{align}\n",
    "f^{*}(\\underline{y}) & = \\underline{y}^{T} \\underline{x} _{opt} - f(\\underline{x}_{opt}) \\\\\n",
    "& = \\Sigma_{i=1}^{n} y_i e^{y_i - 1} - \\Sigma_{i=1}^{n} e^{y_i - 1} \\log(e^{y_i - 1}) \\\\\n",
    "& = \\Sigma_{i=1}^{n} y_i e^{y_i - 1} - \\Sigma_{i=1}^{n} e^{y_i - 1} (y_i - 1) \\\\\n",
    "& = \\Sigma_{i=1}^{n} y_i e^{y_i - 1} - (e^{y_i - 1} (y_i - 1)) \\\\\n",
    "& = \\Sigma_{i=1}^{n} e^{y_i - 1} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b\n",
    "1. From Lec4 pg 17, we know that $\\frac{\\partial{f}}{\\partial{X}} = -(X^{-2})^{T} = -X^{-2}$\n",
    "2. We can also derive $\\frac{\\partial{tr(Y^T X)}}{\\partial{X}}$:\n",
    "\\begin{align}\n",
    "\\mathcal{D}_{z} f(X) & = \\lim_{h\\rightarrow0}{\\frac{tr(Y^T(X+hZ)) - tr(Y^T X)}{h}} \\\\\n",
    "& = \\lim_{h\\rightarrow0}{\\frac{tr(Y^T X) + htr(Y^T Z) - tr(Y^T X)}{h}} \\\\\n",
    "& = tr(Y^T Z) \\\\\n",
    "\\Rightarrow \\frac{\\partial{tr(Y^TX)}}{\\partial{X}} & = Y\n",
    "\\end{align}\n",
    "3. Then to find $f^{*}(Y) = \\sup\\{ tr(Y^{T} X) - f(X) \\}$, we set the derivative to 0 and find $X_{opt}$:\n",
    "\\begin{align}\n",
    "\\nabla tr(Y^{T} X_{opt}) - \\nabla f(X_{opt})  & = 0 \\\\\n",
    "\\Rightarrow Y - (-X_{opt}^{-2}) & = 0 \\\\\n",
    "\\Rightarrow Y & = -X_{opt}^{-2} \\\\\n",
    "\\Rightarrow -Y & = X_{opt}^{-2} \\\\\n",
    "\\Rightarrow (-Y)^{-\\frac{1}{2}} & = X_{opt}\n",
    "\\end{align}\n",
    "Note that since $\\rm{dom}(f) = \\mathbb{S}^{n}_{++}$, the above derivation shows that $\\rm{dom}(f^{*}) = \\mathbb{S}^{n}_{--}$\n",
    "3. Then we can substitute this into the equation for $f^{*}(Y)$\n",
    "\\begin{align}\n",
    "f^{*}(Y) & = tr(Y^T ((-Y)^{-\\frac{1}{2}})) - tr(((-Y)^{-\\frac{1}{2}})^{-1}) \\\\\n",
    "& = -tr(-Y^T ((-Y)^{-\\frac{1}{2}})  ) - tr((-Y)^{\\frac{1}{2}}) \\\\\n",
    "& = -tr((-Y)^{\\frac{1}{2}}) - tr((-Y)^{\\frac{1}{2}}) \\\\\n",
    "& = -2 tr((-Y)^{\\frac{1}{2}})\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
