{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [(1+9) x 5 = 50 points] Optimization Problems: Convex vs Nonconvex\n",
    "\n",
    "For each of the following, **state** if the problem is nonconvex or convex. If your answer is nonconvex, then **explain why**. If your answer is convex, then **explain what type of convex optimization problem it is** among the following: LP, QP, QCQP, SOCP, SDP.\n",
    "\n",
    "(a) $\\underset{x\\in\\Delta^{n-1}}{\\min} \\langle c, x\\rangle$ where $c\\in\\mathbb{R}^{n}$.\n",
    "\n",
    "(b) $\\underset{x\\in\\mathbb{R}^{n}}{\\min} \\langle c, x\\rangle$ subject to $Ax \\leq b$ where $A\\in\\mathbb{R}^{n\\times n}$ nonsingular, and $b,c\\in\\mathbb{R}^{n}$.\n",
    "\n",
    "(c) $\\underset{M\\in\\mathbb{R}^{n\\times n}}{\\min}\\;\\langle C, M\\rangle$ subject to $M\\geq 0$ (elementwise), $M\\boldsymbol{1}=a$, $M^{\\top}\\boldsymbol{1}=b$ where $a,b\\in\\mathbb{R}^{n}$, $C\\in\\mathbb{R}^{n\\times n}$, and $\\boldsymbol{1}\\in\\mathbb{R}^{n}$ denotes a vector of ones.\n",
    "\n",
    "(d) $\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\;x^{\\top}P x$ subject to $x^{\\top}Q x \\leq 1$ where $P\\in\\mathbb{S}^{n}$. $Q\\in\\mathbb{S}^{n}_{++}$.\n",
    "\n",
    "(e) $\\underset{(x,y,z)\\in\\mathbb{R}^{n}\\times\\mathbb{R}_{+}\\times\\mathbb{R}_{+}}{\\min} \\bigg\\{\\langle c, x\\rangle + \\alpha y + \\beta z\\bigg\\}$ subject to $x^{\\top}x\\leq yz$ where $c\\in\\mathbb{R}^{n}$, $\\alpha,\\beta\\in\\mathbb{R}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a\n",
    "1. We know from HW2 1.c.ii that the feasible domain $\\mathcal{X} = \\Delta^{n-1}$ **is a convex set**. And it can be expressed as $Ax = b$ where $A$ is a vector of 1 and $b = 1$\n",
    "2. We also know from lec 8 pg 8 that the objective function $f(x) = \\langle c, x\\rangle$ is a **convex function**\n",
    "3. Then from lec 11 pg 2, this is a **convex** optimization problem, an **LP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b\n",
    "1. From lec 11 pg 4, this is **convex** and **LP** (linear objective function, $\\mathcal{X}$ is a polyhedron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c\n",
    "1. From lec 11 pg 10, $M \\geq 0$ describes **cone**\n",
    "2. From lec 11 pg 10, $M \\boldsymbol{1} = a$ describes the $M$ **row-sum** $=a$, and $M^{T} \\boldsymbol{1} = b$ is the **column-sum**. So this constrains $M$ to be a doubly stochastic matrix and from HW3 2c we know that this can be rewritten as $2n$ hyperplanes and $2n^2$ halfspaces, and is thus a **polyhedron**\n",
    "3. So then from 1, 2, the feasible domain $X$ is an intersection of a cone and a polyhedron, and is thus **convex**\n",
    "4. Finally, the objective function is an inner product between numerical matrix $C$ and $M$, thus linear. \n",
    "5. Then, from lec 11 pg 2, this is a **convex** optimization problem, and it is a **SDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d\n",
    "1. Consider $P \\in \\mathbb{S}_{-}^{n} \\in \\mathbb{S}^{n}$, then the function is from lec 8 pg 8, **concave**, so the problem is **nonconvex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e\n",
    "1. We can re-express the objective function as a linear transform, and is a **convex function**\n",
    "\\begin{align}\n",
    "f(x) & = \\langle c, x \\rangle + \\alpha y + \\beta z \\\\\n",
    "& = \\begin{bmatrix} c & \\alpha & \\beta & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ y-z \\\\ y+z \\end{bmatrix} \\\\\n",
    "& = \\underline{d}^{T} \\underline{u}\n",
    "\\end{align}\n",
    "2. Consider the feasible domain, (rotated-second-order cone)\n",
    "\\begin{align}\n",
    "4x^{T}x & = 4yz \\\\\n",
    "4x^{T}x + y^2 + z^2 & \\leq 4yz + y^2 + z^2 \\\\\n",
    "4x^{T}x + y^2 + z^2 - 2yz & \\leq 4yz + y^2 + z^2 \\\\\n",
    "4x^{T}x + (y - z)^2 & \\leq (y + z)^2 \\\\\n",
    "\\sqrt{4x^{T}x + (y-z)^2} & \\leq y + z \\\\\n",
    "\\lVert \\begin{bmatrix} 2x \\\\ y - z \\end{bmatrix} \\rVert_2 & \\leq \\begin{bmatrix} 0 & 1 & 1 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ y-z \\\\ y+z \\end{bmatrix} \\\\\n",
    "\\lVert A \\begin{bmatrix} x \\\\ y \\\\ z \\\\ y-z \\\\ y+z \\end{bmatrix} \\rVert_2 & \\leq \\begin{bmatrix} 0 & 1 & 1 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ y-z \\\\ y+z \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "3. Then it is a **convex problem, SOCP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. [15 + 15 + 20 = 50 points] Analytical Solutions\n",
    "\n",
    "(i) Consider the Prob. 1(a) above. Use basic logical reasoning and linear algebra (and no knowledge of this course) to **analytically compute the minimum value**.\n",
    "\n",
    "(ii) Consider Prob. 1(b) above. Use basic logical reasoning and linear algebra (and no knowledge of this course) to **analytically compute the minimum value**.\n",
    "\n",
    "(iii) Consider the Prob. 1(d) above. Use basic logical reasoning and linear algebra (and no knowledge of this course) to **analytically compute the minimum value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.\n",
    "1. For the vector $c \\in R^{n}$, pick the minimum element $i = \\rm{argmin}(c)$\n",
    "2. Then $\\rm{min}_{\\Delta^{n-1}} \\langle c, x \\rangle = \\langle c, e_i \\rangle$. In english, we 'put it all on black', and collect the entire mass of $x$ the minimum element of $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii\n",
    "1. Since we are told that $A$ is **non-singular** and **square**, then it has an inverse $A^{-1}$, s.t.\n",
    "\\begin{align}\n",
    "A x & \\leq b \\\\\n",
    "x & \\leq d = A^{-1} b \\\\\n",
    "\\end{align}\n",
    "2. Then for every element $c_i$, we pick $x_i$to minimize $c_i x_i$ such that $x_i \\leq d_i$, then this will minimize $\\langle c, x \\rangle$\n",
    "3. Case 1: $c_i > 0$ for any $i$, then minimimum = $-\\infty$, since we can select $x_i = -\\infty$ for the $c_i > 0$\n",
    "4. Case 2: $c_i \\leq 0$ for all $i$, then for any $d_i = A^{-1} b_i > 0$ we set that set $x_i = d_i$ to multiply the negative $c_i$ by the largest positive $x_i$, and for all $d_i \\leq 0$, we set $x_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii\n",
    "1. Given that $\\mathcal{Q}$ is positive definite, it has **positive** eigenvalues. Then the relation is that $x^{T}x \\leq \\frac{1}{\\lambda_{min}(Q)}$\n",
    "2. Case 1, $P$ has a negative eigenvalue, then to minimize the function, then the minimimum value will be $$\\lambda_{min}(P) \\frac{1}{\\lambda_{min}(Q)} = \\frac{\\lambda_{min}(P)}{\\lambda_{min}(Q)}$$\n",
    "3. case 2: $P$ has only positive eigenvalues, then to minimize the objective, pick $$x = 0 \\Rightarrow \\rm{min} f(x) = 0$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
