{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [60 points] Making Sandcastle\n",
    "\n",
    "Grad student Alice is at Cancun to spend her end-of-the-year vacation. Relaxing at the beautiful white sand beach, she contemplates making a sandcastle while minimizing her effort to construct it. \n",
    "\n",
    "Alice approximates the sandy beach as part of a 2D plane. She decides to move the sand mass from $m$ known source locations $x_{i}^{\\text{source}}\\in\\mathbb{R}^{2}$, $i=1,...,m$, to $n$ known destination locations $x_{j}^{\\text{sandcastle}}\\in\\mathbb{R}^{2}$ , $j=1,...,n$. \n",
    "\n",
    "Sand particles in the beach have nonuniform mass. In particular, the $i$th source location has a known mass $\\alpha_i>0$, $i=1,...,m$. Alice's sandcastle design requires the $j$th destination location to have a known mass $\\beta_j>0$, $j=1,...,n$. Conservation of mass requires $\\sum_{i}\\alpha_i = \\sum_{j}\\beta_{j}$. Without loss of generality, Alice normalizes the mass, i.e., sets $\\sum_{i}\\alpha_i = \\sum_{j}\\beta_{j}=1$. In other words, known $\\alpha_i$ denotes the fraction of total mass at the $i$th source location. Similar interpretation holds for $\\beta_{j}$.\n",
    "\n",
    "Alice models the cost $C_{ij}$ of moving **unit amout of sand** from the source location $x_{i}^{\\text{source}}$ to the destination location $x_{j}^{\\text{sandcastle}}$ as squared Euclidean distance, i.e., $C_{ij} = \\|x_{i}^{\\text{source}} - x_{j}^{\\text{sandcastle}}\\|_{2}^{2}$. This defines a matrix $C\\equiv [C_{ij}]\\in\\mathbb{R}^{m\\times n}$. \n",
    "\n",
    "## (a) [5 + 10 + (5 + 5) + 10 = 35 points] Formulation\n",
    "\n",
    "(i) **Explain why** Alice's model for $C_{ij}$ is reasonable. \n",
    "\n",
    "(ii) If Alice decides to move $M_{ij}$ amount of mass from $x_{i}^{\\text{source}}$ to $x_{j}^{\\text{sandcastle}}$, then she incurs a cost $C_{ij}M_{ij}$ for that particular route. Taking the matrix $M\\equiv [M_{ij}]\\in\\mathbb{R}^{m\\times n}$ as the decision variable, **clearly write down the optimization problem** Alice needs to solve for making her sandcastle. The input parameters for the problem should be the matrix $C\\in\\mathbb{R}^{m\\times n}$, the vector $\\alpha\\in\\mathbb{R}^{m}_{++}$, and the vector $\\beta\\in\\mathbb{R}^{n}_{++}$. Assume that the problem data already guarantees $\\langle\\boldsymbol{1}_{m},\\alpha\\rangle = \\langle\\boldsymbol{1}_{n},\\beta\\rangle = 1$ where $\\boldsymbol{1}_{m},\\boldsymbol{1}_{n}$ denote the vector of all ones of size $m\\times 1$ and $n\\times 1$, respectively.\n",
    "\n",
    "(iii) **Mathematically explain why** this is a convex optimization problem. **Mathematically argue what type** of convex optimization problem is this.\n",
    "\n",
    "Side remark: Unlike the exercises in HW5 Problem 2, this problem has no analytical solution in terms of the problem data.\n",
    "\n",
    "(iv) **Carefully argue the size of the optimization problem**, i.e., how many variables are to be solved for and how many constraints are there.\n",
    "\n",
    "## (b) [25 points] Numerical solution\n",
    "\n",
    "Fix $m=150$, $n=225$. Write a code in MATLAB/Python/Julia to load the input from CANVAS Files section: HW problems and solutions: alpha.txt, beta.txt, x_source.txt, x_sandcastle.txt, and use cvx/cvxpy/Convex.jl in the same code to solve the optimization problem in part (a). Report the **numerically computed optimal value (minimized cost) and submit your code**. **Also report the computational time needed to solve the problem by cvx/cvxpy/Convex.jl** (only the computational time for solving, not for setting up the problem data).\n",
    "\n",
    "Side remark: It is recommended (but not required) that in your code, you also check if your optimal solution obtained from using cvx/cvxpy/Convex.jl matches with linprog in MATLAB, or with scipy.optimize.linprog in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.\n",
    "1. Defined as $C_{ij} = \\|x_{i}^{\\text{source}} - x_{j}^{\\text{sandcastle}}\\|_{2}^{2}$, every element $C_{ij}$ of the rectangular matrix $C$ is **non-negative** and **convex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.\n",
    "\\begin{align}\n",
    "p^{*} = &{\\min} \\quad \\langle C, M \\rangle \\\\\n",
    "&\\text{subject to} \\quad 0 \\leq M_{ij} \\\\\n",
    "&\\text{subject to} \\quad \\langle E_i, M \\rangle = \\alpha_i, \\quad\\forall\\,i=1,...,m, \\\\\n",
    "&\\text{subject to} \\quad \\langle D_j, M \\rangle = \\beta_j, \\quad\\forall\\,j=1,...,n, \\\\\n",
    "&\\text{where $E_i$ is [0] matrix, 1 along row $i$} \\\\\n",
    "&\\text{where $D_j$ is [0] matrix, 1 along column $j$} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.\n",
    "1. The objective function is a **linear function in $x$**, (another form of it is to convert $C$ to a vector $\\underline{c}$ by concatenating columns of $C$ into a vector $\\underline{c}$). Then same can be done for $M$ to form the vector $\\underline{m}$, then the frobenius inner product becomes equivalent to $\\underline{c}^{T} \\underline{m}$\n",
    "2. The constraint sets is a **polyhedron**, of $m + n$ equalities and $m \\times n$ inequalities.\n",
    "  * The equalities come from the vectorizing $E_i$ into a vector $\\underline{e_i}$ and $\\underline{e_i}^{T} \\underline{m} = \\alpha_i$ and likewise for $D_j$.\n",
    "  * The halfspace / inequalities come from $a_{ij}$ being the null vector except -1 at the $m \\times i + j$ location, such that $a_{ij}^{T} \\underline{m} = -m_{ij} \\leq 0 \\Leftrightarrow m_{ij} \\geq 0$\n",
    "3. Then from lec 11 pg 4, we know that this is a convex optimization problem, specifically a **linear program** because of the reasons above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv.\n",
    "1. Constraints are covered above, there are $m + n$ equalities, $m \\times n$ inequalities, and there are $m \\times n$ decision variables in the matrix $M$ or vector $\\underline{m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 225\n",
      "0.9999999995\n",
      "1.0000000003\n",
      "e_stacked.shape (150, 33750)\n",
      "d_stacked.shape (225, 33750)\n",
      "ed_stacked.shape (375, 33750)\n",
      "solution 0.729278783683019\n",
      "solve time 0.5193018913269043\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "alpha = None\n",
    "with open('./alpha.txt') as f: \n",
    "    alpha = np.array([float(x) for x in f.read().splitlines()])\n",
    "beta = None\n",
    "with open('./beta.txt') as f: \n",
    "    beta = np.array([float(x) for x in f.read().splitlines()])\n",
    "x_source = None\n",
    "with open('./x_source.txt') as f: \n",
    "    x_source = f.read().splitlines()\n",
    "    x_source = [[float(y) for y in x.split(\"\\t\")] for x in x_source]\n",
    "x_source = np.array(x_source)\n",
    "    \n",
    "x_sandcastle = None\n",
    "with open('./x_sandcastle.txt') as f: \n",
    "    x_sandcastle = f.read().splitlines()\n",
    "    x_sandcastle = [[float(y) for y in x.split(\"\\t\")] for x in x_sandcastle]\n",
    "x_sandcastle = np.array(x_sandcastle)\n",
    "    \n",
    "m = len(alpha)\n",
    "n = len(beta)\n",
    "\n",
    "print(len(alpha), len(beta))\n",
    "print(np.sum(alpha))\n",
    "print(np.sum(beta))\n",
    "\n",
    "C = cdist(x_source, x_sandcastle, 'sqeuclidean')\n",
    "cvector = C.reshape(m*n, order='F')# F == column wise!!!, C default is row wise!!!\n",
    "\n",
    "# row equalities\n",
    "e_stacked = np.kron(\n",
    "    np.ones((1,n)), # pick that column's element for every column\n",
    "    sparse.eye(m).toarray() # an eye picks that column's element for every row\n",
    ")\n",
    "\n",
    "# column equalities\n",
    "d_stacked = np.kron(\n",
    "    sparse.eye(n).toarray(), # 1 for every col, so eye(n)\n",
    "    np.ones((1,m)) # 1 for every row in column, so m 1s in a row\n",
    ")\n",
    "\n",
    "ed_stacked = np.vstack((e_stacked, d_stacked))\n",
    "\n",
    "print(\"e_stacked.shape\", e_stacked.shape)\n",
    "print(\"d_stacked.shape\", d_stacked.shape)\n",
    "print(\"ed_stacked.shape\", ed_stacked.shape)\n",
    "\n",
    "alpha_beta = np.hstack((alpha, beta))\n",
    "\n",
    "col_major_m = cp.Variable(\n",
    "    m * n,\n",
    "    nonneg=True\n",
    ")\n",
    "prob = cp.Problem(\n",
    "    cp.Minimize(cvector @ col_major_m),\n",
    "    [\n",
    "        e_stacked @ col_major_m == alpha,\n",
    "        d_stacked @ col_major_m == beta,\n",
    "#         ed_stacked @ col_major_m == alpha_beta,\n",
    "#         col_major_m >= -2*np.ones(m*n),\n",
    "    ])\n",
    "\n",
    "start = time.time()\n",
    "prob.solve(verbose=False, solver='ECOS_BB')\n",
    "end = time.time()\n",
    "print(\"solution\", prob.value)\n",
    "print(\"solve time\", end - start)\n",
    "\n",
    "# start = time.time()\n",
    "# prob.solve(verbose=False, solver='SCS')\n",
    "# end = time.time()\n",
    "# print(\"solve time\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. [15 + 15 + 10 = 40 points] Lagrange Dual Problem\n",
    "\n",
    "Consider the primal convex optimization problem\n",
    "\\begin{align}\n",
    "p^{*} = &\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\quad\\frac{1}{2}x^{\\top}P_{0}x + \\langle q_0, x\\rangle + r_0\\\\\n",
    "&\\text{subject to} \\quad \\frac{1}{2}x^{\\top}P_{i}x + \\langle q_i, x\\rangle + r_i \\leq 0, \\quad\\forall\\,i=1,...,m,\n",
    "\\end{align}\n",
    "where $P_0\\in\\mathbb{S}^{n}_{++}$, $P_{i}\\in\\mathbb{S}^{n}_{+}$ for all $i=1,...,m$, $q_i\\in\\mathbb{R}^{n}$ for all $i=0,1,...,m$, and $r_i\\in\\mathbb{R}$ for all $i=0,1,...,m$.\n",
    "\n",
    "(a) Denote the Lagrange multiplier associated with the primal inequality constraints as $\\lambda\\in\\mathbb{R}^{m}_{\\geq 0}$. Let\n",
    "\\begin{align}\n",
    "P(\\lambda) := P_{0} + \\sum_{i=1}^{m}\\lambda_i P_i \\succ 0,\\quad q(\\lambda) &:= q_0 + \\sum_{i=1}^{m}\\lambda_i q_i,\\quad r(\\lambda) := r_0 + \\sum_{i=1}^{m}\\lambda_i r_i.\n",
    "\\end{align}\n",
    "**Prove that** the Lagrange dual problem associated with the primal problem is \n",
    "$$d^{*} = \\underset{\\lambda\\in\\mathbb{R}^{m}_{\\geq 0}}{\\min}\\:\\frac{1}{2}\\left(q(\\lambda)\\right)^{\\top}\\left(P(\\lambda)\\right)^{-1}q(\\lambda) - r(\\lambda).$$\n",
    "\n",
    "(b) Rewrite the Lagrange dual problem derived in part (a) in one of the standard forms: LP, QP, QCQP, SOCP, SDP. **Show all your calculations**.\n",
    "\n",
    "(c) Specialize Slater's condition for this primal problem to **state a sufficient condition for strong duality** ($p^{*}=d^{*}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a\n",
    "1. Derive the **Lagrangian**\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\underline{x}, \\underline{\\lambda}) & = \\frac{1}{2}x^{\\top}P_{0}x + \\langle q_0, x\\rangle + r_0 + \\langle \\lambda, \\frac{1}{2}x^{\\top}P_{i}x + \\langle q_i, x\\rangle + r_i \\rangle \\\\\n",
    "& = \\frac{1}{2}x^{\\top}P_{0}x + \\langle q_0, x\\rangle + r_0 + \\sum_{i=1}^{m}\\lambda_i \\frac{1}{2}x^{\\top}P_{i}x + \\sum_{i=1}^{m}\\lambda_i \\langle q_i, x\\rangle + \\sum_{i=1}^{m}\\lambda_i r_i \\\\\n",
    "& = \\frac{1}{2}x^{\\top}P_{0} x + \\frac{1}{2} \\sum_{i=1}^{m} x^{\\top}\\lambda_i P_{i} x + \\langle q_0, x\\rangle + \\sum_{i=1}^{m} \\langle \\lambda_i q_i, x\\rangle + r_0 + \\sum_{i=1}^{m}\\lambda_i r_i \\\\\n",
    "& = \\frac{1}{2}x^{\\top}P_{0} x + \\frac{1}{2} \\sum_{i=1}^{m} x^{\\top}\\lambda_i P_{i} x + \\langle q_0, x\\rangle + \\sum_{i=1}^{m} \\langle \\lambda_i q_i, x\\rangle + r(\\lambda) \\\\\n",
    "& = \\frac{1}{2}x^{\\top} P(\\lambda) x + \\langle q(\\lambda), x\\rangle + r(\\lambda) \\\\\n",
    "\\end{align}\n",
    "2. Then for the **dual function**, we take the derivative and equate to 0 to derive $\\underline{x}_{opt}$\n",
    "\\begin{align}\n",
    "g(\\lambda) & = \\underset{\\underline{x} \\in \\mathbb{R}^{n}}\\inf{L(\\underline{x}, \\underline{\\lambda})}\\\\\n",
    "\\frac{\\partial}{\\partial{x}} L(\\underline{x}, \\underline{\\lambda}) & = P(\\lambda)\\underline{x} + q(\\lambda) \\\\\n",
    "\\Rightarrow \\underline{x}_{opt} & = -(P(\\lambda))^{-1} q(\\lambda) \\\\\n",
    "\\Rightarrow g(\\lambda) & = \\frac{1}{2} (-P^{-1} q)^{\\top} P (- P^{-1} q) + q^{\\top} (-P^{-1} q) + r \\\\\n",
    "& = \\frac{1}{2} (P^{-1} q)^{\\top} q - q^{\\top} P^{-1} q + r \\\\\n",
    "& = \\frac{1}{2} q^{\\top} (P^{-1})^{\\top} q - q^{\\top} P^{-1} q + r \\\\\n",
    "& = \\frac{1}{2} q^{\\top} P^{-1} q - q^{\\top} P^{-1} q + r \\quad \\text{P symmetric}\\\\\n",
    "& = -\\frac{1}{2} q^{\\top} P^{-1} q + r \\\\\n",
    "\\end{align}\n",
    "3. The dual is thus (Lec 14 pg 5 proves the domain of $\\underline{\\lambda} \\in \\mathbb{R}^{m} \\geq 0$)\n",
    "\\begin{align}\n",
    "\\underset{\\underline{\\lambda} \\in \\mathbb{R}^{m} \\geq 0}\\sup{g(\\lambda)} & = \\underset{\\underline{\\lambda} \\in \\mathbb{R}^{m} \\geq 0}\\min{-g(\\lambda)}\\\\\n",
    "& = \\underset{\\underline{\\lambda} \\in \\mathbb{R}^{m} \\geq 0}\\min{\\frac{1}{2} q(\\lambda)^{\\top} (P(\\lambda))^{-1} q(\\lambda) - r(\\lambda)}$\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\n",
    "1. We can use the 'turn objective function to constraint' trick in Lec 13 pg 17 to turn the non-linear objective function into a linear constraint.\n",
    "\\begin{align}\n",
    "d^{*} & = \\underset{\\underline{\\lambda}, t}\\min{t} \\\\\n",
    "&\\text{subject to}\\\\\n",
    "&\\quad \\frac{1}{2}q(\\lambda)^{\\top}P(\\lambda)^{-1}q(\\lambda) - r(\\lambda) \\leq t \\\\\n",
    "& \\Leftrightarrow \\frac{1}{2}q(\\lambda)^{\\top}P(\\lambda)^{-1}q(\\lambda) - r(\\lambda) - t \\leq 0 \\\\\n",
    "& \\Leftrightarrow \\mathcal{S} = t + r(\\lambda) - \\frac{1}{2}q(\\lambda)^{\\top}P(\\lambda)^{-1}q(\\lambda) \\geq 0 \\\\\n",
    "\\end{align}\n",
    "2. We see that $\\mathcal{S}$ is the **Schur complement** of\n",
    "\\begin{align}\n",
    "\\mathcal{X} & = \\begin{bmatrix} 2P(\\lambda) \\succ 0 & q(\\lambda) \\\\ q(\\lambda)^{\\top} & t + r(\\lambda) \\end{bmatrix} \n",
    "\\end{align}\n",
    "3. Then from lec 9 pg 3, since the top left $A = 2P(\\lambda) \\succ 0$, $\\mathcal{S} \\succeq 0 \\Leftrightarrow \\mathcal{X} \\succeq 0$\n",
    "4. So then we see that the dual $d^{*}$ has been rewritten as (lec 11 pg 10) **SDP** standard form:\n",
    "\\begin{align}\n",
    "d^{*} & = \\underset{\\underline{\\lambda}, t}\\min{t} \\\\\n",
    "&\\text{subject to}\\\\\n",
    "& \\mathcal{X} = \\begin{bmatrix} 2P(\\lambda) \\succ 0 & q(\\lambda) \\\\ q(\\lambda)^{\\top} & t + r(\\lambda) \\end{bmatrix} \\succeq 0\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.\n",
    "1. Since the $f_{i}(x)$ constraints are **nonlinear**, Slater's condition for this problem says the sufficient condition is if:\n",
    "\\begin{align}\n",
    "\\exists \\underline{x}^{*} & \\in \\text{relint}(\\text{dom}(p^{*})) \\\\\n",
    "f_{i}(\\underline{x}^{*}) & = \\frac{1}{2} (\\underline{x}^{*})^{\\top}P_{i}\\underline{x}^{*} + \\langle q_i, \\underline{x}^{*}\\rangle + r_i \\lt 0 \\quad \\forall i=1,...m \\\\\n",
    "\\Rightarrow \\langle q_i, \\underline{x}^{*}\\rangle + r_i & \\lt -\\frac{1}{2} (\\underline{x}^{*})^{\\top}P_{i}\\underline{x}^{*} \\leq 0 \\quad \\forall i=1,...m \\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
