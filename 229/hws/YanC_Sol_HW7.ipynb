{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [100 points] Hello Europa\n",
    "\n",
    "Scientists speculate that beneath its thick icy surface, [Jupyter's moon Europa](https://en.wikipedia.org/wiki/Europa_(moon)) has huge salty water ocean that stays in liquid phase due to tidal effects. Some believe that Europa's subsurface ocean is a promising place to harbor extraterrestial life.\n",
    "\n",
    "Consider a robotic space mission that delivers $m$ rovers equipped with biochemical sensors on Europa's icy surface at **known locations** $a_{i}\\in\\mathbb{R}^{n}$ (for our application $n=2$ or $3$, but we will fix the numerical data later). All coordinates are measured w.r.t. a fixed reference frame (which is irrelevant for this problem). Upon landing, all rovers detect that a source located at **unknown location** $x\\in\\mathbb{R}^{n}$ is emitting biochemical signal. Specifically, the sensors onboard the rovers measure the noisy range $r_i$ between the source and the $i$th rover:\n",
    "$$r_i = \\|x-a_i\\|_2 + \\varepsilon_i, \\quad i=1,...,m,$$\n",
    "where $(\\varepsilon_1, ..., \\varepsilon_m)^{\\top}$ denotes the **unknown noise vector**. The purpose of this problem is to estimate the source location $x\\in\\mathbb{R}^{n}$ from the noisy measurements $r_i>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [15 + 5 = 20 points] Range Error Formulation\n",
    "\n",
    "(i) A natural way to formulate this problem is to solve\n",
    "$$\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\sum_{i=1}^{m}\\left(r_i - \\|x-a_i\\|_2\\right)^{2},$$\n",
    "or equivalently,\n",
    "\\begin{align*}\n",
    "&\\underset{x\\in\\mathbb{R}^{n},g\\in\\mathbb{R}^{m}}{\\min}\\sum_{i=1}^{m}\\left(r_i - g_i\\right)^{2}\\\\\n",
    "&\\text{subject to} \\quad g_i^2 = \\|x-a_i\\|_2^{2} \\quad \\forall i=1,...,m.\n",
    "\\end{align*}\n",
    "Use the change-of-variables \n",
    "$$G := \\begin{pmatrix}g\\\\\n",
    "1\\end{pmatrix}\\left(g^{\\top} \\quad 1\\right), \\qquad X := \\begin{pmatrix}x\\\\\n",
    "1\\end{pmatrix}\\left(x^{\\top} \\quad 1\\right),$$\n",
    "to **re-write** the above as an optimization problem over the matrix decision variable pair $(X,G)$.\n",
    "\n",
    "(ii) **Mathematically argue** whether the optimization problem derived in part (a)(i) is convex or nonconvex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.\n",
    "1. We see that defined as an outer product, $G$ is **positive semi-definite** and **rank(1)**, $G \\in \\mathbb{S}^{m+1}_{+}$\n",
    "$$G = \\begin{bmatrix}\n",
    "g_{1}^2 & g_{1} g_{2} & ... & g_{1} \\\\\n",
    "g_{2} g_{1} & g_{2}^{2} & ... & g_{2} \\\\\n",
    "... & ...   & ... & ... \\\\\n",
    "g_{1} & g_{2} & ... & 1 \\\\\n",
    "\\end{bmatrix}$$\n",
    "and likewise, $X \\in \\mathbb{S}^{n+1}_{+}$\n",
    "$$X = \\begin{bmatrix}\n",
    "x x^{\\top} & x \\\\\n",
    "x^{\\top} & 1\n",
    "\\end{bmatrix}$$\n",
    "2. We rewrite the expressions in the equivalent expression in terms of operations on these matrices, starting with the objective function:\n",
    "\\begin{align*}\n",
    "&\\underset{x\\in\\mathbb{R}^{n},g\\in\\mathbb{R}^{m}}{\\min}\\sum_{i=1}^{m}\\left(r_i - g_i\\right)^{2}\\\\\n",
    "&\\underset{x\\in\\mathbb{R}^{n},g\\in\\mathbb{R}^{m}}{\\min} \\sum_{i=1}^{m} r_i^{2} - 2 r_i g_i + g_i^{2}\\\\\n",
    "&\\underset{X\\in\\mathbb{S}^{n+1}_{+},G\\in\\mathbb{S}^{m+1}_{+}}{\\min} \\sum_{i=1}^{m} r_i^{2} - 2 r_i G_{m+1, i} + G_{ii}\\\\\n",
    "\\end{align*}\n",
    "3. Same for the constraint\n",
    "\\begin{align}\n",
    "g_i^{2} & = \\lVert x - a_i \\rVert_{2}^{2} \\\\\n",
    "G_{ii} & = \\lVert x \\rVert_{2}^{2} + \\lVert a_i \\rVert_{2}^{2} - 2 x^{\\top} a_i \\\\\n",
    "G_{ii} & = x^{\\top} x + a_i^{\\top} a_i - 2 x^{\\top} a_i \\\\\n",
    "G_{ii} & = \\langle\n",
    "\\begin{bmatrix} x x^{\\top} & x \\\\ x^{\\top} & 1 \\end{bmatrix} ,\n",
    "\\begin{bmatrix} I_{n\\times n} & -a_i \\\\ -a_i & a_i^{\\top} a_i \\end{bmatrix}\n",
    "\\rangle \\\\\n",
    "G_{ii} & = \\langle X , A_i \\rangle \\\\\n",
    "\\end{align}\n",
    "4. We also need to specify that since $G$ and $X$ are **outer products**, $rank(G) = 1, rank(X) = 1$\n",
    "5. Then the original problem is finally rewritten as:\n",
    "\\begin{align}\n",
    "&\\underset{X\\in\\mathbb{S}^{n+1}_{+},G\\in\\mathbb{S}^{m+1}_{+}}{\\min} \\sum_{i=1}^{m} r_i^{2} - 2 r_i G_{m+1, i} + G_{ii}\\\\\n",
    "&\\text{subject to} \\quad \\rm{rank}(X) = \\rm{rank}(G) = 1 \\\\\n",
    "&\\text{and} \\quad G_{ii} = \\langle X , A_i \\rangle \\quad \\forall i=1,...,m. \\\\\n",
    "&\\text{where} \\quad A_i = \\begin{bmatrix} I_{n\\times n} & -a_i \\\\ -a_i & a_i^{\\top} a_i \\end{bmatrix} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.\n",
    "1. let $n = 1$\n",
    "\\begin{align}\n",
    "x_1 & = \\begin{bmatrix} 1 \\end{bmatrix}, \\rm{rank}(x_1) = 1 \\\\\n",
    "x_2 & =  \\begin{bmatrix} -1 \\end{bmatrix}, \\rm{rank}(x_2) = 1 \\\\\n",
    "\\lambda & = 0.5\n",
    "y = \\lambda x_1 + (1-\\lambda) x_2 = 0.5 * 1 - 0.5 * 1 = 0, \\rm{rank}(y) = 0 \\\\\n",
    "\\end{align}\n",
    "2. Then we see that the set $X s.t. \\rm{rank}(X) = 1$ is **nonconvex**, and since such a set is in the problem constraint, the problem itself is **nonconvex** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [15 + 5 = 20 points] Squared Range Error Formulation\n",
    "\n",
    "(i) A different formulation is to solve\n",
    "$$\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\underbrace{\\sum_{i=1}^{m}\\left(\\|x-a_i\\|_2^2 - r_i^2\\right)^{2}}_{f_0(x)},$$\n",
    "or equivalently,\n",
    "\\begin{align*}\n",
    "&\\underset{x\\in\\mathbb{R}^{n},\\alpha\\in\\mathbb{R}}{\\min}\\sum_{i=1}^{m}\\left(\\alpha - 2a_i^{\\top}x + \\|a_i\\|_2^2 - r_i^2\\right)^{2}\\\\\n",
    "&\\text{subject to} \\quad x^{\\top}x=\\alpha.\n",
    "\\end{align*}\n",
    "Introducing $y:=\\begin{pmatrix}\n",
    "x\\\\\n",
    "\\alpha\n",
    "\\end{pmatrix}\\in\\mathbb{R}^{n+1}$, re-write the above problem as \n",
    "\\begin{align*}\n",
    "&\\underset{y\\in\\mathbb{R}^{n+1}}{\\min}\\|Ay-b\\|_2^{2}\\\\\n",
    "&\\text{subject to} \\quad y^{\\top}Cy + 2d^{\\top}y = 0.\n",
    "\\end{align*}\n",
    "**In other words, derive** $A,b,C,d$ as function of the problem data: $r_1^2,..., r_m^2$, and $a_1,..., a_m$.\n",
    "\n",
    "(ii) **Mathematically argue** whether the optimization problem derived in part (b)(i) is convex or nonconvex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.\n",
    "1. w.r.t. the new variable $y$, $a_i$ and $r_i$ are constants.\n",
    "2. Then\n",
    "\\begin{align}\n",
    "\\|Ay-b\\|_2^{2} & = \\sum{(A_i y_i - b_i)^{2}} = (\\alpha - 2a_i^{\\top}x + \\|a_i\\|_2^2 - r_i^2)^{2} \\\\\n",
    "& \\Rightarrow  A_i y_i - b_i = \\alpha - 2a_i^{\\top}x + \\|a_i\\|_2^2 - r_i^2 \\\\\n",
    "& \\Rightarrow  A_i y_i - b_i = \n",
    "\\begin{bmatrix} -2a_i^{\\top} & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ \\alpha \\end{bmatrix} - b_i\n",
    "\\quad , b_i = r_i^2 - \\|a_i\\|_2^2 \\\\\n",
    "& \\Rightarrow A = \\begin{bmatrix} -2a_1^{\\top} & 1 \\\\ -2a_2^{\\top} & 1 \\\\ ... & ... \\\\ -2a_m^{\\top} & 1 \\end{bmatrix} \\quad b = \\begin{bmatrix} r_1^2 - \\|a_1\\|_2^2 \\\\ ... \\\\ r_m^2 - \\|a_m\\|_2^2 \\end{bmatrix}\n",
    "\\end{align}\n",
    "3. For the constraints:\n",
    "\\begin{align}\n",
    "x^{\\top}x - \\alpha & = 0 = \\begin{bmatrix} x & \\alpha \\end{bmatrix} \\begin{bmatrix} I_{n \\times n} & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ \\alpha \\end{bmatrix} + 2 \\begin{bmatrix} 0_{n} & -\\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} x \\\\ \\alpha \\end{bmatrix} \\\\\n",
    "& \\Rightarrow C = \\begin{bmatrix} I_{n \\times n} & 0 \\\\ 0 & 0 \\end{bmatrix} \\quad d = \\begin{bmatrix} 0_{n} & -\\frac{1}{2} \\end{bmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.\n",
    "1. From (i), we see that the objective function is **quadratic**,\n",
    "2. The constraint is also a quadratic LHS, but since it is an **equality**, it is **nonconvex**\n",
    "3. Then, this optimization problem is also **nonconvex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [25 + 35 = 60 points] Numerical Solution\n",
    "\n",
    "(i) Let us call the problem derived in part (b)(i) as the primal problem. It can be theoretically proved that this primal problem has zero duality gap. This theoretical knowledge suggests the following strategy: derive the Lagrange dual problem for the primal in part (b)(i), then numerically solve that Lagrange dual problem via cvx/cvxpy/Convex.jl, then invoke strong duality. \n",
    "\n",
    "**Using this strategy, compute the optimal estimate of the source location** $x^{\\rm{opt}}\\in\\mathbb{R}^{2}$ for $m=5$ rovers located at\n",
    "$$a_1=\\begin{pmatrix}\n",
    "1.8\\\\\n",
    "2.5\n",
    "\\end{pmatrix},\\quad a_2=\\begin{pmatrix}\n",
    "2.0\\\\\n",
    "1.7\n",
    "\\end{pmatrix},\\quad a_3=\\begin{pmatrix}\n",
    "1.5\\\\\n",
    "1.5\n",
    "\\end{pmatrix},\\quad a_4=\\begin{pmatrix}\n",
    "1.5\\\\\n",
    "2.0\n",
    "\\end{pmatrix},\\quad a_5=\\begin{pmatrix}\n",
    "2.5\\\\\n",
    "1.5\n",
    "\\end{pmatrix},$$\n",
    "and $r=(2.00, 1.24, 0.59, 1.31, 1.44)$. **Please submit your code**.\n",
    "\n",
    "For the above data, see figure file $\\texttt{f0Contour.png}$ in CANVAS file section, folder: Homework Problems and Solutions, showing the contour lines of the objective $f_0(x)$ in part (b) with rover locations $a_i$ denoted as circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.\n",
    "1. First, we derive the **dual** for (b)(i): (note that since there is 1 equality constraint, v is a **scalar**.\n",
    "\\begin{align}\n",
    "L(\\underline{y}, \\underline{v}) & = \\|Ay-b\\|_2^{2} + \\langle v, y^{\\top}Cy + 2d^{\\top}y \\rangle \\\\\n",
    "& = \\|Ay\\|_2^{2} + \\|b\\|_2^{2} - 2(Ay)^{\\top} b + v y^{\\top}Cy + 2vd^{\\top}y \\\\ \n",
    "& = y^{\\top}A^{\\top}Ay + y^{\\top}vCy - 2 y^{\\top} A^{\\top} b + 2vd^{\\top}y + b^{\\top}b \\\\\n",
    "& = y^{\\top}(A^{\\top}A + vC)y - 2 y^{\\top} A^{\\top} b + 2vd^{\\top}y + b^{\\top}b \\\\ \n",
    "\\end{align}\n",
    "2. For the case where $A^{\\top}A + vC \\succeq 0$ we find $y_{opt}$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{y}} & = (A^{\\top}A + vC + (A^{\\top}A + vC)^{\\top}) y - 2A^{\\top}b + 2vd \\\\\n",
    "& = (A^{\\top}A + vC + A^{\\top}A + vC) y - 2A^{\\top}b + 2vd \\\\\n",
    "& = 2(A^{\\top}A + vC) y - 2A^{\\top}b + 2vd \\\\\n",
    "& \\Rightarrow \\frac{\\partial{L}}{\\partial{y}} = 0 \\\\\n",
    "& \\Rightarrow 2(A^{\\top}A + vC) y_{opt} - 2A^{\\top}b + 2vd = 0 \\\\\n",
    "& \\Rightarrow 2(A^{\\top}A + vC) y_{opt} = 2A^{\\top}b - 2vd \\\\\n",
    "& \\Rightarrow (A^{\\top}A + vC) y_{opt} = A^{\\top}b - vd \\\\\n",
    "& \\Rightarrow y_{opt} = (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) \\\\\n",
    "& \\Rightarrow y_{opt}^{\\top} = (A^{\\top}b - vd)^{\\top} (A^{\\top}A + vC)^{\\dagger} \\\\\n",
    "& \\Rightarrow g(v) = L(y_{opt}, v) \\\\\n",
    "& \\Rightarrow g(v) = y^{\\top}(A^{\\top}A + vC)y - 2 y^{\\top} A^{\\top} b + 2vd^{\\top}y + b^{\\top}b \\\\\n",
    "& \\Rightarrow g(v) = (A^{\\top}b - vd)^{\\top} (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) - 2 (A^{\\top} b - v d^{\\top}) (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) + b^{\\top}b \\\\\n",
    "& \\Rightarrow g(v) = -(A^{\\top}b - vd)^{\\top} (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) + b^{\\top}b \\\\\n",
    "\\end{align}\n",
    "Then the full expression:\n",
    "$$ g(v) =\n",
    "\\begin{cases}\n",
    "-(A^{\\top}b - vd)^{\\top} (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) + b^{\\top}b & \\quad A^{\\top}A + vC \\succeq 0 \\\\\n",
    "-\\infty, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "3. Then we aim to solve for $$\\max{g(v)} = \\min{-g(v)} = \\underset{v \\in \\mathbb{R}}\\min{(A^{\\top}b - vd)^{\\top} (A^{\\top}A + vC)^{\\dagger} (A^{\\top}b - vd) - b^{\\top}b} \\\\ \\text{s.t.} \\quad A^{\\top}A + vC \\succeq 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 0.02667975425720215\n",
      "v_opt 0.5896188648713906\n",
      "x [1.327 0.645]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy\n",
    "import time\n",
    "\n",
    "a_i = np.array([\n",
    "    [1.8, 2.5],\n",
    "    [2.0, 1.7],\n",
    "    [1.5, 1.5],\n",
    "    [1.5, 2.0],\n",
    "    [2.5, 1.5]\n",
    "])\n",
    "r = np.array([2.00, 1.24, 0.59, 1.31, 1.44])\n",
    "\n",
    "m, n = a_i.shape\n",
    "\n",
    "##############\n",
    "\n",
    "A = np.zeros((m, n+1))\n",
    "b = np.zeros(m)\n",
    "for i in range(m):\n",
    "    A[i, :n] = -2*a_i[i]\n",
    "    A[i, -1] = 1\n",
    "    b[i] = r[i]**2 - np.linalg.norm(a_i[i], ord=2)**2\n",
    "\n",
    "C = np.zeros((n+1, n+1))\n",
    "C[:n, :n] = np.eye(n)\n",
    "\n",
    "d = np.array([0.0, 0.0, -0.5])\n",
    "\n",
    "Atb = np.dot(A.T, b)\n",
    "AtA = np.dot(A.T, A)\n",
    "btb = np.dot(b.T, b)\n",
    "\n",
    "##############\n",
    "\n",
    "v = cvxpy.Variable()\n",
    "\n",
    "prob = cvxpy.Problem(\n",
    "    # https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#matrix-frac\n",
    "    cvxpy.Minimize(cvxpy.atoms.matrix_frac(Atb - v*d, AtA + v*C) - btb),\n",
    "    [AtA + v*C >> 0]\n",
    ")\n",
    "\n",
    "s = time.time()\n",
    "result = prob.solve()\n",
    "e = time.time()\n",
    "\n",
    "print(\"dt\", e - s)\n",
    "\n",
    "print(\"v_opt\", v.value)\n",
    "\n",
    "y_opt = np.dot(np.linalg.inv(AtA + v.value*C), Atb - v.value*d) # [x, \\alpha]\n",
    "\n",
    "print(\"x\", y_opt[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Let us now execute an alternative strategy to solve the primal problem in part (b) as follows. Use the KKT condition to write the primal argmin $y^{\\rm{opt}}$ as an explicit function of the optimal Lagrange multiplier $\\nu^{\\rm{opt}}$. Then derive a nonlinear algebraic equation of the form $\\phi(\\nu^{\\rm{opt}})=0$ and solve the same using bisection method for the numerical data given in part (c)(i). Finally, **compute and compare** the $x^{\\rm{opt}}$ obtained from part c(ii) and c(i)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii\n",
    "1. From lec 16 pg 6/7, we know the 4 KKT conditions:\n",
    "  *  Stationarity of Lagrangian: $$\\frac{\\partial{L}}{\\partial{y}} = 0 \\Rightarrow y_{opt} = (A^{\\top}A + v_{opt} C)^{\\dagger} (A^{\\top}b - v_{opt} d)$$\n",
    "  * Complimentary slackness **does not apply**\n",
    "  * Primal feasibility: $$y_{opt}^{\\top}C y_{opt} + 2d^{\\top}y_{opt} = 0$$\n",
    "  * Dual feasibility: $$A^{\\top}A + v_{opt} C \\succeq 0$$\n",
    "2. From Stationarity alone we get the expression of $y_{opt} = f(v_{opt})$:\n",
    "$$y_{opt} = (A^{\\top}A + v_{opt} C)^{\\dagger} (A^{\\top}b - v_{opt} d)$$\n",
    "3. We can substitute in $y_{opt}$ into primal feasibility to get $\\phi(v_{opt}) = 0$:\n",
    "\\begin{align}\n",
    "\\phi(v_{opt}) & = (A^{\\top}b - v_{opt} d) (A^{\\top}A + v_{opt} C)^{\\dagger} C (A^{\\top}A + v_{opt} C)^{\\dagger} (A^{\\top}b - v_{opt} d) + 2d^{\\top} (A^{\\top}A + v_{opt} C)^{\\dagger} (A^{\\top}b - v_{opt} d) \\\\\n",
    "& = 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 0.0019981861114501953\n",
      "0.589609183371067\n",
      "[1.327 0.645 2.176]\n",
      "x [1.327 0.645]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_i = np.array([\n",
    "    [1.8, 2.5],\n",
    "    [2.0, 1.7],\n",
    "    [1.5, 1.5],\n",
    "    [1.5, 2.0],\n",
    "    [2.5, 1.5]\n",
    "])\n",
    "r = np.array([2.00, 1.24, 0.59, 1.31, 1.44])\n",
    "\n",
    "m, n = a_i.shape\n",
    "\n",
    "##############\n",
    "\n",
    "A = np.zeros((m, n+1))\n",
    "b = np.zeros(m)\n",
    "for i in range(m):\n",
    "    A[i, :n] = -2*a_i[i]\n",
    "    A[i, -1] = 1\n",
    "    b[i] = r[i]**2 - np.linalg.norm(a_i[i], ord=2)**2\n",
    "\n",
    "C = np.zeros((n+1, n+1))\n",
    "C[:n, :n] = np.eye(n)\n",
    "\n",
    "d = np.array([0.0, 0.0, -0.5])\n",
    "\n",
    "Atb = np.dot(A.T, b)\n",
    "AtA = np.dot(A.T, A)\n",
    "btb = np.dot(b.T, b)\n",
    "\n",
    "def y_opt(v):\n",
    "    return np.dot(np.linalg.inv(AtA + v*C), Atb - v*d)\n",
    "\n",
    "def phi(v):\n",
    "    y = y_opt(v)\n",
    "#     print(y)\n",
    "    return np.linalg.multi_dot([y.T, C, y]) + 2*np.dot(d, y)\n",
    "\n",
    "# bisection method from pg 146 in the book, kind of like binary search\n",
    "\n",
    "def bisection(func, e, l, u):\n",
    "    # 'roll' l and u towards 0\n",
    "    t_val = np.inf\n",
    "    while np.abs(t_val) >= e:\n",
    "        \n",
    "        t = (l + u) / 2.0\n",
    "        \n",
    "        t_val = func(t)\n",
    "        l_val = func(l)\n",
    "        u_val = func(u)\n",
    "        \n",
    "#         print(u, l, t)\n",
    "#         print(l_val, t_val, u_val)\n",
    "        \n",
    "        if l_val < 0 and u_val > 0:\n",
    "            # line slopes 'right up'\n",
    "            if t_val < 0:\n",
    "                # l < t_val < 0 < u\n",
    "                # move l up\n",
    "                l = t\n",
    "            else:\n",
    "                # l < 0 < t_val < u\n",
    "                # move u down\n",
    "                u = t\n",
    "        elif l_val > 0 and u_val < 0:\n",
    "            # line slopes 'right down'\n",
    "            if t_val  < 0:\n",
    "                # l < t_val < 0 < u\n",
    "                # move u up\n",
    "                u = t\n",
    "            else:\n",
    "                # l < 0 < t_val < u\n",
    "                # move l to t\n",
    "                l = t\n",
    "        else:\n",
    "            # not feasible?\n",
    "            return -np.inf\n",
    "    \n",
    "    return l\n",
    "\n",
    "s = time.time()\n",
    "v_opt = bisection(phi, 1e-8, 0.0, 20.0)\n",
    "e = time.time()\n",
    "\n",
    "print(\"dt\", e - s)\n",
    "print(v_opt)\n",
    "\n",
    "y_opt = y_opt(v_opt)\n",
    "print(y_opt) # again, [x, \\alpha]\n",
    "print(\"x\", y_opt[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We see that the bisection method solving this equation solving for the 3 KKT conditions to be **simultaneouly true** in $\\phi(v) = 0$ yields the same optimal $v_{opt}$ and $y_{opt}$ as the convex optimiation problem from c.i. !!!\n",
    "\n",
    "6. Also, the KKT approach / bisection solves in 3e-3s, whereas the dual approach solves in 0.025s which is a bit slower"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
